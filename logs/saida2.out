compute-1-1.localdomain
compute-1-1.localdomain

GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-fd7e14c3-91ce-6c4b-e736-393c0d0537ef)
  MIG: Field "uuid" is not a valid field to query.
  MIG: 
  MIG 4g.40gb     Device  0: (UUID: MIG-0dd7cc8d-bef1-51bb-8790-19fb03bacf66)
  MIG: Field "uuid" is not a valid field to query.
  MIG: 
  MIG 3g.40gb     Device  1: (UUID: MIG-67a1ee11-1ab0-511a-ac0f-c81bdbd05f7e)
  No MIG instances found.
GPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-49723f5b-3680-6d21-0357-4b7bf88ad0e7)
  No MIG instances found.
  MIG 2g.20gb     Device  0: (UUID: MIG-f184e443-af81-5f32-bad0-527cd20eb031)
  No MIG instances found.
  MIG 1g.20gb     Device  1: (UUID: MIG-8721230f-e004-50bd-b720-915f56b60dc6)
  No MIG instances found.
  MIG 1g.10gb     Device  2: (UUID: MIG-a444fcc0-f725-530b-9ffb-97805cefb734)
  No MIG instances found.
  MIG 1g.10gb     Device  3: (UUID: MIG-10685134-19fb-5361-83da-7bdc9b8242ba)
  No MIG instances found.
  MIG 1g.10gb     Device  4: (UUID: MIG-a5ff4856-76ba-5d4a-bc36-d6c908a95b14)
  No MIG instances found.
  MIG 1g.10gb     Device  5: (UUID: MIG-d65b56b1-2519-5354-96ae-aec5f0e41128)
  No MIG instances found.
Mon May 20 04:17:39 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe          On  | 00000000:21:00.0 Off |                  Off |
| N/A   35C    P0              43W / 300W |     87MiB / 81920MiB |     N/A      Default |
|                                         |                      |              Enabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  | 00000000:81:00.0 Off |                  Off |
| N/A   32C    P0              47W / 300W |     87MiB / 81920MiB |     N/A      Default |
|                                         |                      |              Enabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| MIG devices:                                                                          |
+------------------+--------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |
|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |
|                  |                                |        ECC|                       |
|==================+================================+===========+=======================|
|  0    1   0   0  |              49MiB / 40192MiB  | 56    N/A |  4   0    2    0    0 |
|                  |               0MiB / 65535MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  0    2   0   1  |              37MiB / 40192MiB  | 42    N/A |  3   0    2    0    0 |
|                  |               0MiB / 65535MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    5   0   0  |              24MiB / 19968MiB  | 28    N/A |  2   0    1    0    0 |
|                  |               0MiB / 32767MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    6   0   1  |              12MiB / 19968MiB  | 14    N/A |  1   0    1    0    0 |
|                  |               0MiB / 32767MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    7   0   2  |              12MiB /  9728MiB  | 14    N/A |  1   0    0    0    0 |
|                  |               0MiB / 16383MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    8   0   3  |              12MiB /  9728MiB  | 14    N/A |  1   0    0    0    0 |
|                  |               0MiB / 16383MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    9   0   4  |              12MiB /  9728MiB  | 14    N/A |  1   0    0    0    0 |
|                  |               0MiB / 16383MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1   10   0   5  |              12MiB /  9728MiB  | 14    N/A |  1   0    0    0    0 |
|                  |               0MiB / 16383MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
0
2
1
2
[1716189467.582311] [compute-1-1:661775:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.582334] [compute-1-1:661775:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.582311] [compute-1-1:661776:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.582334] [compute-1-1:661776:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.605254] [compute-1-1:661775:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.605277] [compute-1-1:661775:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.607009] [compute-1-1:661776:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.607028] [compute-1-1:661776:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.648218] [compute-1-1:661775:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.648233] [compute-1-1:661775:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.648217] [compute-1-1:661776:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.648233] [compute-1-1:661776:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.666030] [compute-1-1:661775:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.666050] [compute-1-1:661775:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.666336] [compute-1-1:661776:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.666353] [compute-1-1:661776:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.703454] [compute-1-1:661775:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.703472] [compute-1-1:661775:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.708570] [compute-1-1:661776:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.708585] [compute-1-1:661776:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.723355] [compute-1-1:661775:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.723371] [compute-1-1:661775:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
[1716189467.729285] [compute-1-1:661776:0]           ib_md.c:1232 UCX  WARN  IB: ibv_fork_init() was disabled or failed, yet a fork() has been issued.
[1716189467.729303] [compute-1-1:661776:0]           ib_md.c:1233 UCX  WARN  IB: data corruption might occur when using registered memory.
cuda:0
cuda:0
Folder already there
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): SiLU()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.3119, losses: [1.164306402206421, 1.147613525390625], learning rate: 0.0100000000
Iteration 0: total loss 2.3177, losses: [1.168155312538147, 1.1495178937911987], learning rate: 0.0100000000
Val loss  tensor(1.1820)
Val loss  tensor(1.6824)
Val loss  tensor(1.1804)
Val loss  tensor(1.6747)
Iteration 5000: total loss 0.1042, losses: [0.05188760906457901, 0.05229896306991577], learning rate: 0.0100000000
Iteration 5000: total loss 0.1673, losses: [0.08304541558027267, 0.08426177501678467], learning rate: 0.0100000000
Iteration 10000: total loss 0.0716, losses: [0.03597385436296463, 0.0356488935649395], learning rate: 0.0100000000
Val loss  tensor(1.1289)
Val loss  tensor(1.1689)
Iteration 10000: total loss 0.1082, losses: [0.053132880479097366, 0.05507795885205269], learning rate: 0.0100000000
Val loss  tensor(1.1494)
Val loss  tensor(1.2199)
Iteration 15000: total loss 0.0477, losses: [0.023572033271193504, 0.024155383929610252], learning rate: 0.0100000000
Iteration 15000: total loss 0.1001, losses: [0.049112822860479355, 0.050943803042173386], learning rate: 0.0100000000
Iteration 20000: total loss 0.0569, losses: [0.02834818698465824, 0.028520239517092705], learning rate: 0.0100000000
Val loss  tensor(1.0554)
Val loss  tensor(1.1254)
Iteration 20000: total loss 0.0922, losses: [0.045194875448942184, 0.04699845239520073], learning rate: 0.0100000000
Val loss  tensor(1.1209)
Val loss  tensor(1.2044)
Iteration 25000: total loss 0.0318, losses: [0.015587466768920422, 0.016216104850172997], learning rate: 0.0100000000
Iteration 25000: total loss 0.0762, losses: [0.03704407066106796, 0.0391240231692791], learning rate: 0.0100000000
Iteration 30000: total loss 0.0378, losses: [0.01886855810880661, 0.018952518701553345], learning rate: 0.0100000000
Val loss  tensor(1.0447)
Val loss  tensor(1.1273)
Iteration 30000: total loss 0.1236, losses: [0.060691285878419876, 0.06294484436511993], learning rate: 0.0100000000
Val loss  tensor(1.1226)
Val loss  tensor(1.2057)
Iteration 35000: total loss 0.0451, losses: [0.02269577607512474, 0.022437673062086105], learning rate: 0.0100000000
Iteration 35000: total loss 0.0694, losses: [0.03447132557630539, 0.034977179020643234], learning rate: 0.0100000000
Iteration 40000: total loss 0.0281, losses: [0.013872168026864529, 0.01419597864151001], learning rate: 0.0100000000
Val loss  tensor(0.9311)
Val loss  tensor(1.0905)
Iteration 40000: total loss 0.0812, losses: [0.039088040590286255, 0.04213361814618111], learning rate: 0.0100000000
Val loss  tensor(1.1393)
Val loss  tensor(1.2147)
Iteration 45000: total loss 0.0343, losses: [0.016704076901078224, 0.017622822895646095], learning rate: 0.0100000000
Iteration 45000: total loss 0.0825, losses: [0.039945680648088455, 0.0425577238202095], learning rate: 0.0100000000
Iteration 50000: total loss 0.0398, losses: [0.020075039938092232, 0.01970578171312809], learning rate: 0.0100000000
Val loss  tensor(1.0949)
Val loss  tensor(1.1750)
Iteration 50000: total loss 0.0762, losses: [0.03711250051856041, 0.03908313065767288], learning rate: 0.0100000000
Val loss  tensor(1.1328)
Val loss  tensor(1.2249)
Iteration 55000: total loss 0.0293, losses: [0.014487874694168568, 0.014852999709546566], learning rate: 0.0100000000
Iteration 55000: total loss 0.0689, losses: [0.03322167694568634, 0.03567902743816376], learning rate: 0.0100000000
Iteration 60000: total loss 0.0372, losses: [0.01860913261771202, 0.018597474321722984], learning rate: 0.0100000000
Val loss  tensor(0.8999)
Val loss  tensor(1.1072)
Iteration 60000: total loss 0.0581, losses: [0.02905857563018799, 0.02903146855533123], learning rate: 0.0100000000
Val loss  tensor(1.1221)
Val loss  tensor(1.2299)
Iteration 65000: total loss 0.0216, losses: [0.010688870213925838, 0.010861787013709545], learning rate: 0.0100000000
Iteration 70000: total loss 0.0275, losses: [0.013637934811413288, 0.013839960098266602], learning rate: 0.0100000000
Val loss  tensor(1.0247)
Iteration 65000: total loss 0.0675, losses: [0.0320296473801136, 0.035439230501651764], learning rate: 0.0100000000
Val loss  tensor(1.2324)
Iteration 70000: total loss 0.0645, losses: [0.03111180104315281, 0.03342394903302193], learning rate: 0.0100000000
Val loss  tensor(1.1066)
Iteration 75000: total loss 0.0256, losses: [0.012990899384021759, 0.012648659758269787], learning rate: 0.0100000000
Val loss  tensor(1.2139)
Iteration 80000: total loss 0.0320, losses: [0.015600253827869892, 0.016433898359537125], learning rate: 0.0100000000
Val loss  tensor(0.8343)
Val loss  tensor(1.1319)
Iteration 75000: total loss 0.0585, losses: [0.028355538845062256, 0.030178386718034744], learning rate: 0.0100000000
Iteration 85000: total loss 0.0181, losses: [0.009046974591910839, 0.009036200121045113], learning rate: 0.0100000000
Iteration 80000: total loss 0.0588, losses: [0.02858869545161724, 0.030207067728042603], learning rate: 0.0100000000
Val loss  tensor(1.0968)
Val loss  tensor(1.1950)
Iteration 90000: total loss 0.0353, losses: [0.017925841733813286, 0.017415296286344528], learning rate: 0.0100000000
Val loss  tensor(0.9174)
Val loss  tensor(1.1920)
Iteration 85000: total loss 0.0618, losses: [0.029723381623625755, 0.0320938304066658], learning rate: 0.0100000000
Iteration 95000: total loss 0.0251, losses: [0.012743216939270496, 0.01239750999957323], learning rate: 0.0100000000
Iteration 90000: total loss 0.0560, losses: [0.02769295684993267, 0.028299391269683838], learning rate: 0.0100000000
Val loss  tensor(1.0921)
Val loss  tensor(1.2001)
Iteration 100000: total loss 0.0211, losses: [0.010608925484120846, 0.010450978763401508], learning rate: 0.0100000000
Val loss  tensor(0.9276)
Val loss  tensor(1.1426)
Iteration 95000: total loss 0.0582, losses: [0.028835995122790337, 0.029331061989068985], learning rate: 0.0100000000
Iteration 105000: total loss 0.0383, losses: [0.01944390870630741, 0.018868735060095787], learning rate: 0.0100000000
Iteration 100000: total loss 0.0893, losses: [0.044445350766181946, 0.0448547787964344], learning rate: 0.0100000000
Val loss  tensor(1.0834)
Val loss  tensor(1.1561)
Iteration 110000: total loss 0.0182, losses: [0.009031392633914948, 0.009187759831547737], learning rate: 0.0100000000
Val loss  tensor(0.8662)
Val loss  tensor(1.1470)
Iteration 105000: total loss 0.0527, losses: [0.025469748303294182, 0.027257487177848816], learning rate: 0.0100000000
Iteration 115000: total loss 0.0178, losses: [0.008948213420808315, 0.00887432973831892], learning rate: 0.0100000000
Iteration 110000: total loss 0.0582, losses: [0.02789630927145481, 0.030327027663588524], learning rate: 0.0100000000
Val loss  tensor(1.0804)
Val loss  tensor(1.2053)
Iteration 120000: total loss 0.0186, losses: [0.009374306537210941, 0.009204862639307976], learning rate: 0.0100000000
Val loss  tensor(0.9637)
Val loss  tensor(1.1177)
Iteration 115000: total loss 0.0851, losses: [0.04207421839237213, 0.043052781373262405], learning rate: 0.0100000000
Iteration 125000: total loss 0.0258, losses: [0.01291425246745348, 0.012862357310950756], learning rate: 0.0100000000
Iteration 120000: total loss 0.0586, losses: [0.02855633571743965, 0.030018383637070656], learning rate: 0.0100000000
Val loss  tensor(1.0856)
Val loss  tensor(1.2246)
Iteration 130000: total loss 0.0223, losses: [0.011086199432611465, 0.011193457059562206], learning rate: 0.0100000000
Val loss  tensor(1.1000)
Val loss  tensor(1.2082)
Iteration 125000: total loss 0.0603, losses: [0.02969062142074108, 0.03061789646744728], learning rate: 0.0100000000
Iteration 135000: total loss 0.0192, losses: [0.009765023365616798, 0.00943498220294714], learning rate: 0.0099000000
Iteration 130000: total loss 0.0535, losses: [0.02579978108406067, 0.027686813846230507], learning rate: 0.0100000000
Val loss  tensor(1.0878)
Iteration 140000: total loss 0.0243, losses: [0.011977151036262512, 0.012284968979656696], learning rate: 0.0099000000
Val loss  tensor(0.7678)
Val loss  tensor(1.2295)
Val loss  tensor(1.2102)
Iteration 145000: total loss 0.0309, losses: [0.015341191552579403, 0.015600609593093395], learning rate: 0.0099000000
Iteration 135000: total loss 0.0751, losses: [0.036325860768556595, 0.03878502547740936], learning rate: 0.0100000000
Iteration 150000: total loss 0.0161, losses: [0.007893741130828857, 0.008201670832931995], learning rate: 0.0099000000
Val loss  tensor(1.0726)
Iteration 140000: total loss 0.0558, losses: [0.027470726519823074, 0.028354991227388382], learning rate: 0.0100000000
Val loss  tensor(1.0970)
Val loss  tensor(1.2011)
Val loss  tensor(1.2329)
Iteration 155000: total loss 0.0208, losses: [0.010452530346810818, 0.01039369311183691], learning rate: 0.0099000000
Iteration 145000: total loss 0.0512, losses: [0.02393171191215515, 0.02727837674319744], learning rate: 0.0100000000
Iteration 160000: total loss 0.0278, losses: [0.013675323687493801, 0.014154060743749142], learning rate: 0.0099000000
Val loss  tensor(0.7632)
Val loss  tensor(1.2267)
Iteration 150000: total loss 0.0517, losses: [0.025589538738131523, 0.02614804543554783], learning rate: 0.0100000000
Val loss  tensor(1.0943)
Val loss  tensor(1.2169)
Iteration 165000: total loss 0.0193, losses: [0.009160416200757027, 0.010167686268687248], learning rate: 0.0099000000
Iteration 155000: total loss 0.0490, losses: [0.02398787997663021, 0.025019003078341484], learning rate: 0.0100000000
Iteration 170000: total loss 0.0157, losses: [0.007863420061767101, 0.007825668901205063], learning rate: 0.0099000000
Val loss  tensor(1.2136)
Val loss  tensor(1.2249)
Iteration 160000: total loss 0.0486, losses: [0.023571519181132317, 0.02502770535647869], learning rate: 0.0100000000
Val loss  tensor(1.1005)
Val loss  tensor(1.1887)
Iteration 175000: total loss 0.0294, losses: [0.014855289831757545, 0.014547936618328094], learning rate: 0.0099000000
Iteration 165000: total loss 0.0481, losses: [0.023468798026442528, 0.02468099258840084], learning rate: 0.0099000000
Iteration 180000: total loss 0.0239, losses: [0.011831454932689667, 0.012032129801809788], learning rate: 0.0099000000
Val loss  tensor(1.2040)
Val loss  tensor(1.2445)
Iteration 170000: total loss 0.0542, losses: [0.026531416922807693, 0.027678433805704117], learning rate: 0.0099000000
Val loss  tensor(1.1038)
Val loss  tensor(1.1910)
Iteration 185000: total loss 0.0176, losses: [0.008765795268118382, 0.00883976835757494], learning rate: 0.0099000000
Iteration 175000: total loss 0.0466, losses: [0.023157386109232903, 0.023412903770804405], learning rate: 0.0099000000
Iteration 190000: total loss 0.0195, losses: [0.009581608697772026, 0.009887064807116985], learning rate: 0.0099000000
Val loss  tensor(1.1349)
Val loss  tensor(1.1876)
Iteration 180000: total loss 0.0445, losses: [0.021297622472047806, 0.02319212071597576], learning rate: 0.0099000000
Val loss  tensor(1.1035)
Val loss  tensor(1.1809)
Iteration 195000: total loss 0.0192, losses: [0.009412631392478943, 0.009756582789123058], learning rate: 0.0099000000
Iteration 185000: total loss 0.0559, losses: [0.026659954339265823, 0.02923331968486309], learning rate: 0.0099000000
Iteration 200000: total loss 0.0241, losses: [0.011963065713644028, 0.012110608629882336], learning rate: 0.0099000000
Val loss  tensor(0.9136)
Val loss  tensor(1.2248)
Iteration 190000: total loss 0.0543, losses: [0.026183536276221275, 0.028070533648133278], learning rate: 0.0099000000
Val loss  tensor(1.1029)
Val loss  tensor(1.1824)
Iteration 205000: total loss 0.0274, losses: [0.013499177061021328, 0.01392293255776167], learning rate: 0.0099000000
Iteration 195000: total loss 0.0458, losses: [0.021751143038272858, 0.024030564352869987], learning rate: 0.0099000000
Iteration 210000: total loss 0.0216, losses: [0.010769304819405079, 0.01080014742910862], learning rate: 0.0099000000
Val loss  tensor(0.7634)
Val loss  tensor(1.0908)
Iteration 200000: total loss 0.0456, losses: [0.02199888788163662, 0.023614436388015747], learning rate: 0.0099000000
Val loss  tensor(1.0979)
Iteration 215000: total loss 0.0117, losses: [0.005878143012523651, 0.005860531236976385], learning rate: 0.0098010000
Val loss  tensor(1.1691)
Iteration 220000: total loss 0.0232, losses: [0.011228942312300205, 0.011964837089180946], learning rate: 0.0098010000
Val loss  tensor(0.9587)
Iteration 205000: total loss 0.0525, losses: [0.02527810074388981, 0.027202745899558067], learning rate: 0.0099000000
Val loss  tensor(1.2019)
Iteration 225000: total loss 0.0125, losses: [0.006172557361423969, 0.006345530040562153], learning rate: 0.0098010000
Iteration 210000: total loss 0.0439, losses: [0.020985517650842667, 0.022866500541567802], learning rate: 0.0099000000
Val loss  tensor(1.1084)
Val loss  tensor(1.1676)
Iteration 230000: total loss 0.0170, losses: [0.008271652273833752, 0.00868566520512104], learning rate: 0.0098010000
Val loss  tensor(0.8657)
Val loss  tensor(1.0967)
Iteration 215000: total loss 0.0493, losses: [0.023607932031154633, 0.02568959631025791], learning rate: 0.0099000000
Iteration 235000: total loss 0.0217, losses: [0.010720662772655487, 0.010962880216538906], learning rate: 0.0098010000
Iteration 220000: total loss 0.0511, losses: [0.025169402360916138, 0.025963788852095604], learning rate: 0.0099000000
Val loss  tensor(1.1208)
Val loss  tensor(1.1698)
Iteration 240000: total loss 0.0280, losses: [0.014096438884735107, 0.01390204019844532], learning rate: 0.0098010000
Val loss  tensor(0.9908)
Val loss  tensor(1.2295)
Iteration 225000: total loss 0.0429, losses: [0.020780866965651512, 0.022153379395604134], learning rate: 0.0098010000
Iteration 245000: total loss 0.0231, losses: [0.011367696337401867, 0.011762075126171112], learning rate: 0.0098010000
Iteration 230000: total loss 0.0617, losses: [0.030458513647317886, 0.031245553866028786], learning rate: 0.0098010000
Val loss  tensor(1.1036)
Val loss  tensor(1.1696)
Iteration 250000: total loss 0.0159, losses: [0.007992609404027462, 0.007947670295834541], learning rate: 0.0098010000
Val loss  tensor(0.8013)
Val loss  tensor(1.2237)
Iteration 235000: total loss 0.0496, losses: [0.024393772706389427, 0.02524724043905735], learning rate: 0.0098010000
Iteration 255000: total loss 0.0208, losses: [0.010603219270706177, 0.010185427032411098], learning rate: 0.0098010000
Iteration 240000: total loss 0.0548, losses: [0.02620384655892849, 0.028625918552279472], learning rate: 0.0098010000
Val loss  tensor(1.0748)
Val loss  tensor(1.1718)
Iteration 260000: total loss 0.0251, losses: [0.012616864405572414, 0.012498801574110985], learning rate: 0.0098010000
Val loss  tensor(1.0939)
Val loss  tensor(1.1320)
Iteration 245000: total loss 0.0477, losses: [0.02321665547788143, 0.02444787137210369], learning rate: 0.0098010000
Iteration 265000: total loss 0.0237, losses: [0.011882568709552288, 0.011830195784568787], learning rate: 0.0098010000
Iteration 250000: total loss 0.0818, losses: [0.04026445746421814, 0.041529472917318344], learning rate: 0.0098010000
Val loss  tensor(1.0558)
Val loss  tensor(1.1590)
Iteration 270000: total loss 0.0153, losses: [0.007582598831504583, 0.007708021905273199], learning rate: 0.0098010000
Val loss  tensor(0.8599)
Val loss  tensor(1.1446)
Iteration 255000: total loss 0.0456, losses: [0.02152404561638832, 0.024118775501847267], learning rate: 0.0098010000
Iteration 275000: total loss 0.0227, losses: [0.01138356328010559, 0.01128931250423193], learning rate: 0.0097029900
Iteration 260000: total loss 0.0437, losses: [0.021842969581484795, 0.021866390481591225], learning rate: 0.0098010000
Val loss  tensor(1.0742)
Iteration 280000: total loss 0.0211, losses: [0.010422262363135815, 0.010665870271623135], learning rate: 0.0097029900
Val loss  tensor(1.1388)
Val loss  tensor(1.1630)
Val loss  tensor(1.2282)
Iteration 265000: total loss 0.0451, losses: [0.022016936913132668, 0.0230668094009161], learning rate: 0.0098010000
Iteration 285000: total loss 0.0128, losses: [0.006194162182509899, 0.006609252188354731], learning rate: 0.0097029900
Iteration 290000: total loss 0.0142, losses: [0.007055205292999744, 0.00716061657294631], learning rate: 0.0097029900
Val loss  tensor(0.9443)
Iteration 270000: total loss 0.0415, losses: [0.020585251972079277, 0.02094004489481449], learning rate: 0.0098010000
Val loss  tensor(1.2144)
Val loss  tensor(1.1718)
Val loss  tensor(1.2230)
Iteration 295000: total loss 0.0209, losses: [0.010386339388787746, 0.010512666776776314], learning rate: 0.0097029900
Iteration 275000: total loss 0.0454, losses: [0.021828124299645424, 0.023598043248057365], learning rate: 0.0098010000
Iteration 300000: total loss 0.0188, losses: [0.009170997887849808, 0.009663164615631104], learning rate: 0.0097029900
Val loss  tensor(0.6106)
Val loss  tensor(1.0828)
Iteration 280000: total loss 0.0522, losses: [0.025878174230456352, 0.026345903053879738], learning rate: 0.0098010000
Val loss  tensor(1.0792)
Val loss  tensor(1.1738)
Iteration 305000: total loss 0.0221, losses: [0.010982765816152096, 0.011067873798310757], learning rate: 0.0097029900
Iteration 285000: total loss 0.0581, losses: [0.028424590826034546, 0.02968924678862095], learning rate: 0.0098010000
Iteration 310000: total loss 0.0187, losses: [0.009333677589893341, 0.009334655478596687], learning rate: 0.0097029900
Val loss  tensor(0.9187)
Val loss  tensor(1.0987)
Iteration 290000: total loss 0.0586, losses: [0.028763486072421074, 0.029825741425156593], learning rate: 0.0098010000
Val loss  tensor(1.0666)
Val loss  tensor(1.1685)
Iteration 315000: total loss 0.0143, losses: [0.0069220587611198425, 0.007395423948764801], learning rate: 0.0097029900
Iteration 295000: total loss 0.0662, losses: [0.032347965985536575, 0.033855654299259186], learning rate: 0.0098010000
Iteration 320000: total loss 0.0173, losses: [0.008453589864075184, 0.008837754838168621], learning rate: 0.0097029900
Val loss  tensor(1.2219)
Val loss  tensor(1.2250)
Iteration 300000: total loss 0.0363, losses: [0.01729070581495762, 0.019003430381417274], learning rate: 0.0098010000
Val loss  tensor(1.1817)
Val loss  tensor(1.2199)
Iteration 325000: total loss 0.0153, losses: [0.007433568127453327, 0.007849281653761864], learning rate: 0.0097029900
Iteration 305000: total loss 0.0368, losses: [0.017586255446076393, 0.019245978444814682], learning rate: 0.0097029900
Iteration 330000: total loss 0.0233, losses: [0.011585329659283161, 0.011673280969262123], learning rate: 0.0097029900
Val loss  tensor(0.8727)
Val loss  tensor(1.1218)
Iteration 310000: total loss 0.0349, losses: [0.017087586224079132, 0.017811469733715057], learning rate: 0.0097029900
Val loss  tensor(1.1107)
Val loss  tensor(1.1819)
Iteration 335000: total loss 0.0205, losses: [0.009982969611883163, 0.010471857152879238], learning rate: 0.0097029900
Iteration 315000: total loss 0.0330, losses: [0.01630319654941559, 0.016717365011572838], learning rate: 0.0097029900
Iteration 340000: total loss 0.0184, losses: [0.00910455547273159, 0.009319865144789219], learning rate: 0.0097029900
Val loss  tensor(1.0355)
Val loss  tensor(1.2249)
Iteration 320000: total loss 0.0619, losses: [0.029993025586009026, 0.03189282491803169], learning rate: 0.0097029900
Val loss  tensor(1.1754)
Val loss  tensor(1.1941)
Iteration 345000: total loss 0.0167, losses: [0.008410854265093803, 0.008330806158483028], learning rate: 0.0097029900
Iteration 325000: total loss 0.0332, losses: [0.01614239625632763, 0.017049310728907585], learning rate: 0.0097029900
Iteration 350000: total loss 0.0157, losses: [0.007998311892151833, 0.007693200837820768], learning rate: 0.0097029900
Val loss  tensor(0.8674)
Val loss  tensor(1.1104)
Iteration 330000: total loss 0.0325, losses: [0.015777193009853363, 0.016723264008760452], learning rate: 0.0097029900
Val loss  tensor(1.2084)
Iteration 355000: total loss 0.0131, losses: [0.006786671467125416, 0.006306909024715424], learning rate: 0.0096059601
Val loss  tensor(1.2311)
Iteration 360000: total loss 0.0181, losses: [0.008966969326138496, 0.009122999384999275], learning rate: 0.0096059601
Val loss  tensor(1.1582)
Iteration 335000: total loss 0.0391, losses: [0.019039956852793694, 0.020029490813612938], learning rate: 0.0097029900
Val loss  tensor(1.2143)
Iteration 365000: total loss 0.0207, losses: [0.010126535780727863, 0.010538660921156406], learning rate: 0.0096059601
Iteration 340000: total loss 0.0324, losses: [0.015568332746624947, 0.016805103048682213], learning rate: 0.0097029900
Val loss  tensor(1.0505)
Val loss  tensor(1.1950)
Iteration 370000: total loss 0.0238, losses: [0.012050650082528591, 0.011752239428460598], learning rate: 0.0096059601
Val loss  tensor(1.1134)
Val loss  tensor(1.1971)
Iteration 345000: total loss 0.0376, losses: [0.017998969182372093, 0.01958468183875084], learning rate: 0.0097029900
Iteration 375000: total loss 0.0253, losses: [0.012964384630322456, 0.01237576175481081], learning rate: 0.0096059601
Iteration 350000: total loss 0.0565, losses: [0.02809734083712101, 0.028408804908394814], learning rate: 0.0097029900
Val loss  tensor(1.2037)
Val loss  tensor(1.2080)
Iteration 380000: total loss 0.0105, losses: [0.005205111112445593, 0.00525389751419425], learning rate: 0.0096059601
Val loss  tensor(1.1726)
Val loss  tensor(1.2322)
Iteration 355000: total loss 0.0327, losses: [0.015764590352773666, 0.016957363113760948], learning rate: 0.0097029900
Iteration 385000: total loss 0.0157, losses: [0.007695201318711042, 0.007997455075383186], learning rate: 0.0096059601
Iteration 360000: total loss 0.0517, losses: [0.02540038339793682, 0.026323506608605385], learning rate: 0.0097029900
Val loss  tensor(1.1924)
Val loss  tensor(1.2187)
Iteration 390000: total loss 0.0140, losses: [0.006834525149315596, 0.007151311729103327], learning rate: 0.0096059601
Val loss  tensor(1.1443)
Val loss  tensor(1.1751)
Iteration 365000: total loss 0.0398, losses: [0.01913643814623356, 0.020697183907032013], learning rate: 0.0097029900
Iteration 395000: total loss 0.0134, losses: [0.006398031022399664, 0.006970655173063278], learning rate: 0.0096059601
Iteration 370000: total loss 0.0304, losses: [0.015022553503513336, 0.01533882599323988], learning rate: 0.0097029900
Val loss  tensor(1.1956)
Val loss  tensor(1.2182)
Iteration 400000: total loss 0.0145, losses: [0.00734821567311883, 0.007150331977754831], learning rate: 0.0096059601
Val loss  tensor(0.7882)
Val loss  tensor(1.1808)
Iteration 375000: total loss 0.0317, losses: [0.015230215154588223, 0.016476592049002647], learning rate: 0.0097029900
Iteration 405000: total loss 0.0153, losses: [0.007586566265672445, 0.0076870485208928585], learning rate: 0.0096059601
Iteration 380000: total loss 0.0315, losses: [0.015951115638017654, 0.015548096969723701], learning rate: 0.0097029900
Val loss  tensor(1.2158)
Val loss  tensor(1.2483)
Iteration 410000: total loss 0.0120, losses: [0.006096852011978626, 0.005944949109107256], learning rate: 0.0096059601
Val loss  tensor(0.8495)
Val loss  tensor(1.1281)
Iteration 385000: total loss 0.0453, losses: [0.02236970141530037, 0.02289011888206005], learning rate: 0.0097029900
Iteration 415000: total loss 0.0128, losses: [0.006347134243696928, 0.006471616216003895], learning rate: 0.0095099005
Iteration 390000: total loss 0.0416, losses: [0.020230883732438087, 0.021413685753941536], learning rate: 0.0097029900
Val loss  tensor(1.0365)
Val loss  tensor(1.2278)
Iteration 420000: total loss 0.0191, losses: [0.009425808675587177, 0.00965657364577055], learning rate: 0.0095099005
Val loss  tensor(1.0230)
Val loss  tensor(1.3346)
Iteration 395000: total loss 0.0301, losses: [0.01486990787088871, 0.015212828293442726], learning rate: 0.0097029900
Iteration 425000: total loss 0.0170, losses: [0.008774294517934322, 0.0082634212449193], learning rate: 0.0095099005
Iteration 400000: total loss 0.0559, losses: [0.027293736115098, 0.028625015169382095], learning rate: 0.0097029900
Iteration 430000: total loss 0.0164, losses: [0.008390103466808796, 0.007985273376107216], learning rate: 0.0095099005
Val loss  tensor(1.1275)
Val loss  tensor(0.8314)
Val loss  tensor(1.1369)
Val loss  tensor(1.2013)
Iteration 435000: total loss 0.0159, losses: [0.008049062453210354, 0.00789159256964922], learning rate: 0.0095099005
Iteration 405000: total loss 0.0336, losses: [0.01638795994222164, 0.01723610796034336], learning rate: 0.0097029900
Iteration 440000: total loss 0.0142, losses: [0.00701814005151391, 0.007204446475952864], learning rate: 0.0095099005
Val loss  tensor(1.1136)
Val loss  tensor(1.1621)
Iteration 410000: total loss 0.0342, losses: [0.016556132584810257, 0.017672821879386902], learning rate: 0.0097029900
Val loss  tensor(1.0976)
Val loss  tensor(1.2106)
Iteration 445000: total loss 0.0162, losses: [0.007864455692470074, 0.008317048661410809], learning rate: 0.0095099005
Iteration 415000: total loss 0.0467, losses: [0.023068208247423172, 0.02361278049647808], learning rate: 0.0097029900
Iteration 450000: total loss 0.0109, losses: [0.0049765221774578094, 0.005916960071772337], learning rate: 0.0095099005
Val loss  tensor(0.8720)
Val loss  tensor(1.2908)
Iteration 420000: total loss 0.0507, losses: [0.02494988404214382, 0.02570589818060398], learning rate: 0.0097029900
Val loss  tensor(1.0379)
Val loss  tensor(1.2169)
Iteration 455000: total loss 0.0173, losses: [0.008547874167561531, 0.008711837232112885], learning rate: 0.0095099005
Iteration 425000: total loss 0.0493, losses: [0.024452732875943184, 0.024876369163393974], learning rate: 0.0097029900
Iteration 460000: total loss 0.0107, losses: [0.005380635615438223, 0.005340594332665205], learning rate: 0.0095099005
Val loss  tensor(1.0439)
Val loss  tensor(1.3333)
Iteration 430000: total loss 0.0342, losses: [0.016213474795222282, 0.017981840297579765], learning rate: 0.0097029900
Val loss  tensor(1.2000)
Val loss  tensor(1.2251)
Iteration 465000: total loss 0.0186, losses: [0.009288852103054523, 0.009321284480392933], learning rate: 0.0094148015
Iteration 435000: total loss 0.0547, losses: [0.026448488235473633, 0.02827158011496067], learning rate: 0.0097029900
Iteration 470000: total loss 0.0204, losses: [0.010040041990578175, 0.010381238535046577], learning rate: 0.0094148015
Val loss  tensor(1.1381)
Val loss  tensor(1.1926)
Iteration 440000: total loss 0.0339, losses: [0.016188886016607285, 0.017704257741570473], learning rate: 0.0097029900
Val loss  tensor(1.0449)
Val loss  tensor(1.2125)
Iteration 475000: total loss 0.0155, losses: [0.007733254227787256, 0.007730843033641577], learning rate: 0.0094148015
Iteration 445000: total loss 0.1030, losses: [0.05126918479800224, 0.051764458417892456], learning rate: 0.0096059601
Iteration 480000: total loss 0.0129, losses: [0.006365950684994459, 0.006486610975116491], learning rate: 0.0094148015
Val loss  tensor(0.8191)
Val loss  tensor(1.1290)
Iteration 450000: total loss 0.0369, losses: [0.01871144399046898, 0.018199587240815163], learning rate: 0.0096059601
Val loss  tensor(1.0560)
Val loss  tensor(1.2061)
Iteration 485000: total loss 0.0206, losses: [0.01042979396879673, 0.010152987204492092], learning rate: 0.0094148015
Iteration 455000: total loss 0.0265, losses: [0.012971964664757252, 0.013570012524724007], learning rate: 0.0096059601
Iteration 490000: total loss 0.0148, losses: [0.007366524077951908, 0.007461043540388346], learning rate: 0.0094148015
Val loss  tensor(0.8795)
Val loss  tensor(1.2634)
Iteration 460000: total loss 0.0414, losses: [0.02024139277637005, 0.021187646314501762], learning rate: 0.0096059601
Val loss  tensor(1.0442)
Val loss  tensor(1.2267)
Iteration 495000: total loss 0.0133, losses: [0.006650579161942005, 0.006685009691864252], learning rate: 0.0094148015
Iteration 465000: total loss 0.0469, losses: [0.02338915877044201, 0.023470038548111916], learning rate: 0.0096059601
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): SiLU()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 470000: total loss 0.0294, losses: [0.014705977402627468, 0.014701509848237038], learning rate: 0.0096059601
Val loss  tensor(1.2004)
Val loss  tensor(1.2111)
Iteration 475000: total loss 0.0291, losses: [0.014590069651603699, 0.014474347233772278], learning rate: 0.0096059601
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5055, losses: [1.2554726600646973, 1.2500108480453491], learning rate: 0.0100000000
Val loss  tensor(1.2341)
Val loss  tensor(1.1437)
Iteration 480000: total loss 0.0340, losses: [0.016322722658514977, 0.01767251454293728], learning rate: 0.0096059601
Val loss  tensor(1.1956)
Val loss  tensor(1.2089)
Iteration 5000: total loss 0.1123, losses: [0.055791597813367844, 0.05652107670903206], learning rate: 0.0100000000
Iteration 10000: total loss 0.1280, losses: [0.06291013211011887, 0.06504081934690475], learning rate: 0.0100000000
Iteration 485000: total loss 0.0292, losses: [0.013952153734862804, 0.015296428464353085], learning rate: 0.0096059601
Val loss  tensor(1.0425)
Val loss  tensor(1.1945)
Iteration 490000: total loss 0.0364, losses: [0.018000725656747818, 0.018365083262324333], learning rate: 0.0096059601
Val loss  tensor(1.0060)
Iteration 15000: total loss 0.3005, losses: [0.1499192714691162, 0.1506069004535675], learning rate: 0.0100000000
Val loss  tensor(1.2161)
Iteration 20000: total loss 0.2660, losses: [0.13226987421512604, 0.1337113380432129], learning rate: 0.0100000000
Val loss  tensor(1.1570)
Val loss  tensor(1.2038)
Iteration 495000: total loss 0.0295, losses: [0.014176348224282265, 0.015320101752877235], learning rate: 0.0096059601
Iteration 25000: total loss 0.1526, losses: [0.07556777447462082, 0.07702149450778961], learning rate: 0.0100000000
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 30000: total loss 0.2002, losses: [0.10031826794147491, 0.09992451965808868], learning rate: 0.0100000000
Val loss  tensor(1.1297)
Val loss  tensor(1.1561)
Iteration 35000: total loss 0.2051, losses: [0.10272544622421265, 0.1024109348654747], learning rate: 0.0100000000
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.4808, losses: [1.2519454956054688, 1.2288402318954468], learning rate: 0.0100000000
Val loss  tensor(1.2598)
Val loss  tensor(1.1563)
Iteration 40000: total loss 0.1879, losses: [0.09418349713087082, 0.0937557965517044], learning rate: 0.0100000000
Val loss  tensor(0.9375)
Val loss  tensor(1.1753)
Iteration 5000: total loss 0.0800, losses: [0.040170010179281235, 0.03981202468276024], learning rate: 0.0100000000
Iteration 45000: total loss 0.2623, losses: [0.13150039315223694, 0.13076741993427277], learning rate: 0.0100000000
Iteration 10000: total loss 0.0620, losses: [0.030275588855147362, 0.031731877475976944], learning rate: 0.0100000000
Val loss  tensor(1.0076)
Val loss  tensor(1.1929)
Iteration 50000: total loss 0.1894, losses: [0.09424730390310287, 0.09515120834112167], learning rate: 0.0100000000
Val loss  tensor(1.3134)
Val loss  tensor(1.5150)
Iteration 15000: total loss 0.0725, losses: [0.03626025840640068, 0.03628672659397125], learning rate: 0.0100000000
Iteration 55000: total loss 0.1985, losses: [0.09928357601165771, 0.09923849999904633], learning rate: 0.0100000000
Iteration 20000: total loss 0.0613, losses: [0.030501341447234154, 0.030774200335144997], learning rate: 0.0100000000
Val loss  tensor(1.0444)
Val loss  tensor(1.1839)
Iteration 60000: total loss 0.1566, losses: [0.07805529981851578, 0.07857655733823776], learning rate: 0.0100000000
Val loss  tensor(1.0346)
Val loss  tensor(1.2298)
Iteration 25000: total loss 0.0641, losses: [0.03186501935124397, 0.0322611927986145], learning rate: 0.0100000000
Iteration 65000: total loss 0.1440, losses: [0.07202739268541336, 0.07192907482385635], learning rate: 0.0100000000
Iteration 30000: total loss 0.0486, losses: [0.024055559188127518, 0.024571968242526054], learning rate: 0.0100000000
Val loss  tensor(1.0504)
Val loss  tensor(1.1680)
Iteration 70000: total loss 0.1859, losses: [0.09282854944467545, 0.0930432453751564], learning rate: 0.0100000000
Val loss  tensor(1.0613)
Val loss  tensor(1.1694)
Iteration 35000: total loss 0.0660, losses: [0.0331420861184597, 0.032841797918081284], learning rate: 0.0100000000
Iteration 75000: total loss 0.1432, losses: [0.0715651884675026, 0.07158571481704712], learning rate: 0.0100000000
Iteration 40000: total loss 0.0611, losses: [0.030648598447442055, 0.030447138473391533], learning rate: 0.0100000000
Val loss  tensor(1.0636)
Val loss  tensor(1.1400)
Iteration 80000: total loss 0.1869, losses: [0.09316873550415039, 0.09371312707662582], learning rate: 0.0100000000
Val loss  tensor(1.0007)
Val loss  tensor(1.2278)
Iteration 45000: total loss 0.0511, losses: [0.025787783786654472, 0.02531023882329464], learning rate: 0.0100000000
Iteration 85000: total loss 0.1541, losses: [0.07704394310712814, 0.07709293067455292], learning rate: 0.0100000000
Iteration 50000: total loss 0.0631, losses: [0.03202848136425018, 0.03103041648864746], learning rate: 0.0100000000
Val loss  tensor(1.0318)
Val loss  tensor(1.1888)
Iteration 90000: total loss 0.2831, losses: [0.1416139453649521, 0.14148207008838654], learning rate: 0.0100000000
Val loss  tensor(0.8130)
Val loss  tensor(1.2067)
Iteration 55000: total loss 0.0628, losses: [0.03141578659415245, 0.03134232014417648], learning rate: 0.0100000000
Iteration 95000: total loss 0.2017, losses: [0.1012042760848999, 0.10048587620258331], learning rate: 0.0100000000
Iteration 60000: total loss 0.0588, losses: [0.029173558577895164, 0.029643485322594643], learning rate: 0.0100000000
Val loss  tensor(0.9075)
Val loss  tensor(1.1615)
Iteration 100000: total loss 0.2940, losses: [0.14768104255199432, 0.146307110786438], learning rate: 0.0100000000
Val loss  tensor(1.0583)
Val loss  tensor(1.2226)
Iteration 65000: total loss 0.0395, losses: [0.019411778077483177, 0.020090896636247635], learning rate: 0.0100000000
Iteration 105000: total loss 0.1388, losses: [0.06875991076231003, 0.07003521174192429], learning rate: 0.0100000000
Iteration 70000: total loss 0.0449, losses: [0.022143138572573662, 0.02277429960668087], learning rate: 0.0100000000
Val loss  tensor(0.9061)
Val loss  tensor(1.1751)
Iteration 110000: total loss 0.1968, losses: [0.09800481051206589, 0.0988142117857933], learning rate: 0.0100000000
Val loss  tensor(0.7413)
Val loss  tensor(1.1447)
Iteration 75000: total loss 0.0445, losses: [0.022425925359129906, 0.02207058109343052], learning rate: 0.0100000000
Iteration 115000: total loss 0.1862, losses: [0.09364134073257446, 0.09251954406499863], learning rate: 0.0100000000
Iteration 80000: total loss 0.0446, losses: [0.022512977942824364, 0.02207799069583416], learning rate: 0.0100000000
Val loss  tensor(1.0555)
Val loss  tensor(1.1711)
Iteration 120000: total loss 0.1600, losses: [0.0802411213517189, 0.07971727102994919], learning rate: 0.0100000000
Val loss  tensor(0.9753)
Val loss  tensor(1.1421)
Iteration 85000: total loss 0.0302, losses: [0.015031714923679829, 0.015183490701019764], learning rate: 0.0100000000
Iteration 125000: total loss 0.1927, losses: [0.09696698188781738, 0.09568317979574203], learning rate: 0.0100000000
Iteration 90000: total loss 0.0414, losses: [0.020714879035949707, 0.020693300291895866], learning rate: 0.0100000000
Val loss  tensor(1.0792)
Val loss  tensor(1.1539)
Iteration 130000: total loss 0.1976, losses: [0.09857285022735596, 0.09903606027364731], learning rate: 0.0100000000
Val loss  tensor(1.1407)
Val loss  tensor(1.2460)
Iteration 95000: total loss 0.0319, losses: [0.016012713313102722, 0.01585542969405651], learning rate: 0.0100000000
Iteration 135000: total loss 0.1857, losses: [0.09278690069913864, 0.09293456375598907], learning rate: 0.0100000000
Iteration 100000: total loss 0.0488, losses: [0.0240681953728199, 0.024777669459581375], learning rate: 0.0100000000
Val loss  tensor(1.0908)
Val loss  tensor(1.1516)
Iteration 140000: total loss 0.1551, losses: [0.07730843126773834, 0.07775187492370605], learning rate: 0.0100000000
Val loss  tensor(1.0328)
Val loss  tensor(1.1789)
Iteration 105000: total loss 0.0478, losses: [0.023614240810275078, 0.02416248992085457], learning rate: 0.0100000000
Iteration 145000: total loss 0.1986, losses: [0.09913398325443268, 0.09951194375753403], learning rate: 0.0100000000
Iteration 110000: total loss 0.0467, losses: [0.024051571264863014, 0.022611601278185844], learning rate: 0.0100000000
Val loss  tensor(0.9166)
Val loss  tensor(1.1291)
Iteration 150000: total loss 0.1521, losses: [0.0763508528470993, 0.07577484846115112], learning rate: 0.0100000000
Val loss  tensor(0.6885)
Val loss  tensor(1.1999)
Iteration 115000: total loss 0.0350, losses: [0.017505014315247536, 0.01748005487024784], learning rate: 0.0100000000
Iteration 155000: total loss 0.1919, losses: [0.09551174938678741, 0.09642800688743591], learning rate: 0.0100000000
Iteration 120000: total loss 0.0394, losses: [0.019478408619761467, 0.019879451021552086], learning rate: 0.0100000000
Val loss  tensor(0.8501)
Val loss  tensor(1.1498)
Iteration 160000: total loss 0.1542, losses: [0.077168770134449, 0.07702875137329102], learning rate: 0.0100000000
Val loss  tensor(0.8253)
Val loss  tensor(1.1732)
Iteration 125000: total loss 0.0659, losses: [0.03305628523230553, 0.032866574823856354], learning rate: 0.0100000000
Iteration 165000: total loss 0.2215, losses: [0.11029364913702011, 0.11119630187749863], learning rate: 0.0100000000
Iteration 130000: total loss 0.0498, losses: [0.024066898971796036, 0.025749584659934044], learning rate: 0.0100000000
Val loss  tensor(0.8387)
Val loss  tensor(1.1251)
Iteration 170000: total loss 0.1866, losses: [0.09348674863576889, 0.09316151589155197], learning rate: 0.0100000000
Val loss  tensor(0.7586)
Val loss  tensor(1.2267)
Iteration 135000: total loss 0.0376, losses: [0.01879742182791233, 0.018784508109092712], learning rate: 0.0100000000
Iteration 175000: total loss 0.1400, losses: [0.07009965181350708, 0.06993674486875534], learning rate: 0.0100000000
Iteration 140000: total loss 0.0364, losses: [0.017855094745755196, 0.01850794069468975], learning rate: 0.0100000000
Val loss  tensor(0.9589)
Val loss  tensor(1.1208)
Iteration 180000: total loss 0.1871, losses: [0.09326108545064926, 0.09381674975156784], learning rate: 0.0100000000
Val loss  tensor(1.0535)
Val loss  tensor(1.2223)
Iteration 145000: total loss 0.0483, losses: [0.023993877694010735, 0.024262243881821632], learning rate: 0.0100000000
Iteration 185000: total loss 0.1570, losses: [0.07782021909952164, 0.0791848823428154], learning rate: 0.0100000000
Iteration 150000: total loss 0.0352, losses: [0.01753133162856102, 0.01764102280139923], learning rate: 0.0100000000
Val loss  tensor(1.0811)
Val loss  tensor(1.2274)
Iteration 190000: total loss 0.2121, losses: [0.10593850910663605, 0.10618946701288223], learning rate: 0.0100000000
Val loss  tensor(1.0209)
Val loss  tensor(1.2658)
Iteration 155000: total loss 0.0333, losses: [0.016550889238715172, 0.016761539503932], learning rate: 0.0100000000
Iteration 195000: total loss 0.1819, losses: [0.09047875553369522, 0.09145834296941757], learning rate: 0.0100000000
Iteration 160000: total loss 0.0352, losses: [0.017352497205138206, 0.017864519730210304], learning rate: 0.0100000000
Val loss  tensor(1.1701)
Val loss  tensor(1.2568)
Iteration 200000: total loss 0.1596, losses: [0.08014082163572311, 0.07950752973556519], learning rate: 0.0100000000
Val loss  tensor(1.1740)
Val loss  tensor(1.2595)
Iteration 165000: total loss 0.0324, losses: [0.016092926263809204, 0.016313930973410606], learning rate: 0.0100000000
Iteration 205000: total loss 0.1804, losses: [0.09012450277805328, 0.0902988612651825], learning rate: 0.0099000000
Iteration 170000: total loss 0.0421, losses: [0.021166052669286728, 0.020897826179862022], learning rate: 0.0100000000
Val loss  tensor(0.9124)
Val loss  tensor(1.1494)
Iteration 210000: total loss 0.2177, losses: [0.10941789299249649, 0.10830085724592209], learning rate: 0.0099000000
Val loss  tensor(1.0637)
Val loss  tensor(1.1833)
Iteration 175000: total loss 0.0356, losses: [0.01773267425596714, 0.01789594627916813], learning rate: 0.0100000000
Iteration 215000: total loss 0.2004, losses: [0.09978049248456955, 0.10058122873306274], learning rate: 0.0099000000
Iteration 180000: total loss 0.0469, losses: [0.023155245929956436, 0.023735016584396362], learning rate: 0.0100000000
Val loss  tensor(0.8778)
Val loss  tensor(1.1028)
Iteration 220000: total loss 0.1688, losses: [0.08415821939706802, 0.08460714668035507], learning rate: 0.0099000000
Val loss  tensor(0.7709)
Val loss  tensor(1.2026)
Iteration 185000: total loss 0.0471, losses: [0.02367309108376503, 0.02340439334511757], learning rate: 0.0099000000
Iteration 225000: total loss 0.1591, losses: [0.07971879094839096, 0.07935380935668945], learning rate: 0.0099000000
Iteration 190000: total loss 0.0320, losses: [0.015870945528149605, 0.016101635992527008], learning rate: 0.0099000000
Val loss  tensor(0.8821)
Val loss  tensor(1.1393)
Iteration 230000: total loss 0.1459, losses: [0.07271384447813034, 0.07317410409450531], learning rate: 0.0099000000
Val loss  tensor(0.7568)
Val loss  tensor(1.1627)
Iteration 195000: total loss 0.0305, losses: [0.015165015123784542, 0.015288646332919598], learning rate: 0.0099000000
Iteration 235000: total loss 0.2204, losses: [0.11085263639688492, 0.10951413959264755], learning rate: 0.0099000000
Iteration 200000: total loss 0.0414, losses: [0.020458431914448738, 0.020936904475092888], learning rate: 0.0099000000
Val loss  tensor(1.1080)
Val loss  tensor(1.3260)
Iteration 240000: total loss 0.2237, losses: [0.11176516860723495, 0.11190753430128098], learning rate: 0.0099000000
Val loss  tensor(1.0224)
Val loss  tensor(1.2361)
Iteration 205000: total loss 0.0332, losses: [0.016675224527716637, 0.016571739688515663], learning rate: 0.0099000000
Iteration 245000: total loss 0.2286, losses: [0.11428501456975937, 0.11427531391382217], learning rate: 0.0099000000
Iteration 210000: total loss 0.0485, losses: [0.024486104026436806, 0.024054840207099915], learning rate: 0.0099000000
Val loss  tensor(1.1787)
Val loss  tensor(1.2563)
Iteration 250000: total loss 0.1666, losses: [0.08338000625371933, 0.08322804421186447], learning rate: 0.0099000000
Val loss  tensor(0.7562)
Val loss  tensor(1.1649)
Iteration 215000: total loss 0.0408, losses: [0.02047671005129814, 0.020355939865112305], learning rate: 0.0099000000
Iteration 255000: total loss 0.2253, losses: [0.11265820264816284, 0.11260640621185303], learning rate: 0.0099000000
Iteration 220000: total loss 0.0596, losses: [0.029811521992087364, 0.029756814241409302], learning rate: 0.0099000000
Val loss  tensor(0.9928)
Val loss  tensor(1.2637)
Iteration 260000: total loss 0.1515, losses: [0.07615822553634644, 0.07537206262350082], learning rate: 0.0099000000
Val loss  tensor(1.0379)
Val loss  tensor(1.1843)
Iteration 225000: total loss 0.0286, losses: [0.014144864864647388, 0.014453236944973469], learning rate: 0.0099000000
Iteration 265000: total loss 0.1471, losses: [0.07398640364408493, 0.07308981567621231], learning rate: 0.0098010000
Iteration 230000: total loss 0.0293, losses: [0.01442884374409914, 0.014920582063496113], learning rate: 0.0099000000
Val loss  tensor(0.8172)
Val loss  tensor(1.0955)
Iteration 270000: total loss 0.1793, losses: [0.09010201692581177, 0.0892314687371254], learning rate: 0.0098010000
Val loss  tensor(0.7596)
Val loss  tensor(1.1749)
Iteration 235000: total loss 0.0535, losses: [0.026801595464348793, 0.026721540838479996], learning rate: 0.0099000000
Iteration 275000: total loss 0.2294, losses: [0.11444371938705444, 0.11493410915136337], learning rate: 0.0098010000
Iteration 240000: total loss 0.0332, losses: [0.016443071886897087, 0.01677107624709606], learning rate: 0.0099000000
Val loss  tensor(0.9431)
Val loss  tensor(1.0934)
Iteration 280000: total loss 0.1602, losses: [0.0795774981379509, 0.08059088885784149], learning rate: 0.0098010000
Val loss  tensor(1.1643)
Val loss  tensor(1.2270)
Iteration 245000: total loss 0.0394, losses: [0.019838713109493256, 0.019595200195908546], learning rate: 0.0099000000
Iteration 285000: total loss 0.1541, losses: [0.07710828632116318, 0.07699694484472275], learning rate: 0.0098010000
Iteration 250000: total loss 0.0362, losses: [0.01822340488433838, 0.017966995015740395], learning rate: 0.0099000000
Val loss  tensor(0.8573)
Val loss  tensor(1.1733)
Iteration 290000: total loss 0.1558, losses: [0.07809258997440338, 0.07772721350193024], learning rate: 0.0098010000
Val loss  tensor(0.8768)
Val loss  tensor(1.3442)
Iteration 255000: total loss 0.0580, losses: [0.02875511720776558, 0.029210109263658524], learning rate: 0.0099000000
Iteration 295000: total loss 0.1715, losses: [0.08562596887350082, 0.0858285054564476], learning rate: 0.0098010000
Iteration 260000: total loss 0.0571, losses: [0.028441965579986572, 0.028696198016405106], learning rate: 0.0099000000
Val loss  tensor(1.2070)
Val loss  tensor(1.3174)
Iteration 300000: total loss 0.1628, losses: [0.08143364638090134, 0.08133460581302643], learning rate: 0.0098010000
Val loss  tensor(1.0567)
Val loss  tensor(1.2158)
Iteration 265000: total loss 0.0694, losses: [0.03488466888666153, 0.03447118401527405], learning rate: 0.0099000000
Iteration 305000: total loss 0.1366, losses: [0.06810994446277618, 0.06848438829183578], learning rate: 0.0098010000
Iteration 270000: total loss 0.0551, losses: [0.02825302444398403, 0.026891667395830154], learning rate: 0.0099000000
Val loss  tensor(0.8955)
Val loss  tensor(1.2084)
Iteration 310000: total loss 0.1733, losses: [0.08644315600395203, 0.08687105029821396], learning rate: 0.0098010000
Val loss  tensor(1.1515)
Val loss  tensor(1.1806)
Iteration 275000: total loss 0.0605, losses: [0.0294810738414526, 0.031062139198184013], learning rate: 0.0099000000
Iteration 315000: total loss 0.1673, losses: [0.08311527222394943, 0.08417496085166931], learning rate: 0.0097029900
Iteration 280000: total loss 0.0587, losses: [0.029480278491973877, 0.029223868623375893], learning rate: 0.0099000000
Val loss  tensor(1.0734)
Val loss  tensor(1.3602)
Iteration 320000: total loss 0.1756, losses: [0.08801259845495224, 0.08762409538030624], learning rate: 0.0097029900
Val loss  tensor(0.7414)
Val loss  tensor(1.1649)
Iteration 285000: total loss 0.0372, losses: [0.018439503386616707, 0.018721941858530045], learning rate: 0.0098010000
Iteration 325000: total loss 0.1280, losses: [0.06357798725366592, 0.06440453976392746], learning rate: 0.0097029900
Iteration 290000: total loss 0.0264, losses: [0.01307479478418827, 0.013335672207176685], learning rate: 0.0098010000
Val loss  tensor(1.0399)
Val loss  tensor(1.2467)
Iteration 330000: total loss 0.1563, losses: [0.07783829420804977, 0.07847161591053009], learning rate: 0.0097029900
Val loss  tensor(0.9901)
Val loss  tensor(1.1711)
Iteration 295000: total loss 0.0398, losses: [0.019782785326242447, 0.02006378397345543], learning rate: 0.0098010000
Iteration 335000: total loss 0.1432, losses: [0.07138292491436005, 0.07182759046554565], learning rate: 0.0097029900
Iteration 300000: total loss 0.0344, losses: [0.01707567274570465, 0.01729043759405613], learning rate: 0.0098010000
Val loss  tensor(0.7837)
Val loss  tensor(1.1295)
Iteration 340000: total loss 0.2401, losses: [0.12018433958292007, 0.1198868677020073], learning rate: 0.0097029900
Val loss  tensor(0.9912)
Val loss  tensor(1.2506)
Iteration 305000: total loss 0.0421, losses: [0.02117837779223919, 0.020910164341330528], learning rate: 0.0098010000
Iteration 345000: total loss 0.1502, losses: [0.07517360895872116, 0.07499679177999496], learning rate: 0.0097029900
Iteration 310000: total loss 0.0528, losses: [0.026549262925982475, 0.026224246248602867], learning rate: 0.0098010000
Val loss  tensor(0.9476)
Val loss  tensor(1.2530)
Iteration 350000: total loss 0.1737, losses: [0.08707580715417862, 0.08665312826633453], learning rate: 0.0097029900
Val loss  tensor(0.8198)
Val loss  tensor(1.1818)
Iteration 315000: total loss 0.0409, losses: [0.020260725170373917, 0.020618252456188202], learning rate: 0.0098010000
Iteration 355000: total loss 0.1519, losses: [0.07577697932720184, 0.07608123123645782], learning rate: 0.0097029900
Iteration 320000: total loss 0.0349, losses: [0.01731022819876671, 0.017626769840717316], learning rate: 0.0098010000
Val loss  tensor(0.7510)
Val loss  tensor(1.2400)
Iteration 360000: total loss 0.1283, losses: [0.0635591596364975, 0.06474459916353226], learning rate: 0.0097029900
Val loss  tensor(0.9579)
Val loss  tensor(1.1964)
Iteration 325000: total loss 0.0543, losses: [0.02692609839141369, 0.027393808588385582], learning rate: 0.0098010000
Iteration 365000: total loss 0.1646, losses: [0.08257611840963364, 0.08200626820325851], learning rate: 0.0097029900
Iteration 330000: total loss 0.0268, losses: [0.01331160869449377, 0.013482028618454933], learning rate: 0.0098010000
Val loss  tensor(1.2002)
Val loss  tensor(1.4642)
Iteration 370000: total loss 0.1929, losses: [0.09590797126293182, 0.09699306637048721], learning rate: 0.0097029900
Val loss  tensor(0.7595)
Val loss  tensor(1.1527)
Iteration 335000: total loss 0.0325, losses: [0.016232801601290703, 0.016232483088970184], learning rate: 0.0098010000
Iteration 375000: total loss 0.1689, losses: [0.08434392511844635, 0.08457054942846298], learning rate: 0.0096059601
Iteration 340000: total loss 0.0365, losses: [0.018106041476130486, 0.018379738554358482], learning rate: 0.0098010000
Val loss  tensor(0.8950)
Val loss  tensor(1.1480)
Iteration 380000: total loss 0.1926, losses: [0.09656959027051926, 0.09601553529500961], learning rate: 0.0096059601
Val loss  tensor(1.0614)
Val loss  tensor(1.1982)
Iteration 345000: total loss 0.0517, losses: [0.025554591789841652, 0.026160812005400658], learning rate: 0.0098010000
Iteration 385000: total loss 0.1375, losses: [0.06902157515287399, 0.06845849752426147], learning rate: 0.0096059601
Iteration 350000: total loss 0.0366, losses: [0.018561745062470436, 0.018000110983848572], learning rate: 0.0098010000
Val loss  tensor(0.7471)
Val loss  tensor(1.2149)
Iteration 390000: total loss 0.1158, losses: [0.057326387614011765, 0.05847085639834404], learning rate: 0.0096059601
Val loss  tensor(0.8717)
Val loss  tensor(1.2007)
Iteration 355000: total loss 0.0448, losses: [0.0222864281386137, 0.02248627319931984], learning rate: 0.0098010000
Iteration 395000: total loss 0.0932, losses: [0.04663714021444321, 0.046523693948984146], learning rate: 0.0096059601
Iteration 360000: total loss 0.0353, losses: [0.017469191923737526, 0.01778608001768589], learning rate: 0.0098010000
Val loss  tensor(0.7833)
Val loss  tensor(1.0828)
Iteration 400000: total loss 0.2420, losses: [0.12073387950658798, 0.12128441780805588], learning rate: 0.0096059601
Val loss  tensor(0.7437)
Val loss  tensor(1.1753)
Iteration 365000: total loss 0.0630, losses: [0.031423162668943405, 0.031573325395584106], learning rate: 0.0098010000
Iteration 405000: total loss 0.1219, losses: [0.06087115406990051, 0.06104579195380211], learning rate: 0.0096059601
Iteration 370000: total loss 0.0274, losses: [0.013975344598293304, 0.013457456603646278], learning rate: 0.0098010000
Val loss  tensor(1.1864)
Val loss  tensor(1.2961)
Iteration 410000: total loss 0.1393, losses: [0.07006796449422836, 0.069252610206604], learning rate: 0.0096059601
Val loss  tensor(1.0223)
Val loss  tensor(1.2394)
Iteration 375000: total loss 0.0292, losses: [0.01452020462602377, 0.014711263589560986], learning rate: 0.0098010000
Iteration 415000: total loss 0.1385, losses: [0.06941857188940048, 0.06910961121320724], learning rate: 0.0096059601
Iteration 380000: total loss 0.0541, losses: [0.02692396007478237, 0.027197247371077538], learning rate: 0.0098010000
Val loss  tensor(1.0754)
Val loss  tensor(1.2495)
Iteration 420000: total loss 0.1491, losses: [0.07422303408384323, 0.07491035759449005], learning rate: 0.0096059601
Val loss  tensor(0.6814)
Val loss  tensor(1.1938)
Iteration 385000: total loss 0.0390, losses: [0.01935247890651226, 0.019682150334119797], learning rate: 0.0098010000
Iteration 425000: total loss 0.1023, losses: [0.0513693206012249, 0.05091084912419319], learning rate: 0.0096059601
Iteration 390000: total loss 0.0283, losses: [0.014053292572498322, 0.014259072951972485], learning rate: 0.0098010000
Val loss  tensor(0.7078)
Val loss  tensor(1.2354)
Iteration 430000: total loss 0.1943, losses: [0.09702672809362411, 0.09731560945510864], learning rate: 0.0096059601
Val loss  tensor(0.7923)
Val loss  tensor(1.1533)
Iteration 395000: total loss 0.0370, losses: [0.018237516283988953, 0.018748437985777855], learning rate: 0.0098010000
Iteration 435000: total loss 0.1370, losses: [0.06859982758760452, 0.06840638071298599], learning rate: 0.0096059601
Iteration 400000: total loss 0.0289, losses: [0.014212238602340221, 0.014684706926345825], learning rate: 0.0098010000
Val loss  tensor(0.7258)
Val loss  tensor(1.3100)
Iteration 440000: total loss 0.1494, losses: [0.07455796748399734, 0.074795663356781], learning rate: 0.0096059601
Val loss  tensor(1.1040)
Val loss  tensor(1.2368)
Iteration 405000: total loss 0.0930, losses: [0.04573267325758934, 0.04730898141860962], learning rate: 0.0098010000
Iteration 445000: total loss 0.1463, losses: [0.07311500608921051, 0.07314631342887878], learning rate: 0.0096059601
Iteration 410000: total loss 0.0391, losses: [0.019653944298624992, 0.019421685487031937], learning rate: 0.0098010000
Val loss  tensor(0.7933)
Val loss  tensor(1.1880)
Iteration 450000: total loss 0.1565, losses: [0.07847423851490021, 0.07806630432605743], learning rate: 0.0096059601
Val loss  tensor(1.1364)
Val loss  tensor(1.2607)
Iteration 415000: total loss 0.0548, losses: [0.027482515200972557, 0.02727016806602478], learning rate: 0.0098010000
Iteration 455000: total loss 0.1855, losses: [0.09258470684289932, 0.09289120882749557], learning rate: 0.0096059601
Iteration 420000: total loss 0.0363, losses: [0.01803717575967312, 0.018227433785796165], learning rate: 0.0098010000
Val loss  tensor(0.7425)
Val loss  tensor(1.1025)
Iteration 460000: total loss 0.1578, losses: [0.0784987360239029, 0.0793469026684761], learning rate: 0.0096059601
Val loss  tensor(0.8989)
Val loss  tensor(1.2138)
Iteration 425000: total loss 0.0317, losses: [0.016060391440987587, 0.01564052700996399], learning rate: 0.0098010000
Iteration 465000: total loss 0.1396, losses: [0.07003547996282578, 0.0695706158876419], learning rate: 0.0096059601
Iteration 430000: total loss 0.0396, losses: [0.019588826224207878, 0.02005269005894661], learning rate: 0.0098010000
Val loss  tensor(1.2371)
Val loss  tensor(1.2398)
Iteration 470000: total loss 0.2071, losses: [0.1036360040307045, 0.10346249490976334], learning rate: 0.0096059601
Val loss  tensor(1.1654)
Val loss  tensor(1.2679)
Iteration 435000: total loss 0.0325, losses: [0.016204506158828735, 0.016304126009345055], learning rate: 0.0098010000
Iteration 475000: total loss 0.1529, losses: [0.07653182744979858, 0.07641283422708511], learning rate: 0.0095099005
Iteration 440000: total loss 0.0457, losses: [0.022523391991853714, 0.02320956066250801], learning rate: 0.0098010000
Val loss  tensor(1.3662)
Val loss  tensor(1.3863)
Iteration 480000: total loss 0.1782, losses: [0.08912467211484909, 0.08904086798429489], learning rate: 0.0095099005
Val loss  tensor(0.9415)
Val loss  tensor(1.1684)
Iteration 445000: total loss 0.0539, losses: [0.027169963344931602, 0.02670830488204956], learning rate: 0.0097029900
Iteration 485000: total loss 0.1603, losses: [0.08046089857816696, 0.07985631376504898], learning rate: 0.0095099005
Iteration 450000: total loss 0.0283, losses: [0.014010808430612087, 0.014328080229461193], learning rate: 0.0097029900
Val loss  tensor(0.7333)
Val loss  tensor(1.2155)
Iteration 490000: total loss 0.2117, losses: [0.10566055774688721, 0.10603722184896469], learning rate: 0.0095099005
Val loss  tensor(0.8557)
Val loss  tensor(1.2457)
Iteration 455000: total loss 0.0473, losses: [0.02397899143397808, 0.023310458287596703], learning rate: 0.0097029900
Iteration 495000: total loss 0.1301, losses: [0.06523444503545761, 0.06481998413801193], learning rate: 0.0095099005
Iteration 460000: total loss 0.0404, losses: [0.020294498652219772, 0.020144779235124588], learning rate: 0.0097029900
Val loss  tensor(1.2256)
Val loss  tensor(1.2923)
Iteration 465000: total loss 0.0471, losses: [0.023277172818779945, 0.02378421649336815], learning rate: 0.0097029900
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): SiLU()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 470000: total loss 0.0330, losses: [0.016168301925063133, 0.01679227314889431], learning rate: 0.0097029900
Val loss  tensor(0.7532)
Val loss  tensor(1.1408)
Iteration 475000: total loss 0.0377, losses: [0.018974849954247475, 0.018734976649284363], learning rate: 0.0097029900
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.3154, losses: [1.1633856296539307, 1.151986002922058], learning rate: 0.0100000000
Val loss  tensor(1.0930)
Val loss  tensor(1.0918)
Iteration 480000: total loss 0.0304, losses: [0.015023775398731232, 0.015341403894126415], learning rate: 0.0097029900
Val loss  tensor(0.7218)
Val loss  tensor(1.2167)
Iteration 5000: total loss 0.0788, losses: [0.03858919069170952, 0.04021596536040306], learning rate: 0.0100000000
Iteration 485000: total loss 0.0387, losses: [0.019336914643645287, 0.019407598301768303], learning rate: 0.0097029900
Iteration 10000: total loss 0.1003, losses: [0.049900949001312256, 0.05042136460542679], learning rate: 0.0100000000
Val loss  tensor(1.0535)
Val loss  tensor(1.0961)
Iteration 490000: total loss 0.0434, losses: [0.02180265635251999, 0.02157922461628914], learning rate: 0.0097029900
Val loss  tensor(0.6882)
Val loss  tensor(1.1963)
Iteration 15000: total loss 0.0424, losses: [0.020769909024238586, 0.021665310487151146], learning rate: 0.0100000000
Iteration 495000: total loss 0.0314, losses: [0.015567632392048836, 0.015872418880462646], learning rate: 0.0097029900
Iteration 20000: total loss 0.0457, losses: [0.02252940833568573, 0.023152032867074013], learning rate: 0.0100000000
Val loss  tensor(1.0839)
Val loss  tensor(1.1385)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 25000: total loss 0.0454, losses: [0.022504078224301338, 0.022943973541259766], learning rate: 0.0100000000
Iteration 30000: total loss 0.0283, losses: [0.013953975401818752, 0.01432089228183031], learning rate: 0.0100000000
Val loss  tensor(1.1478)
Val loss  tensor(1.1874)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.2719, losses: [1.1354025602340698, 1.136500358581543], learning rate: 0.0100000000
Val loss  tensor(1.7591)
Val loss  tensor(1.6583)
Iteration 35000: total loss 0.0455, losses: [0.023029236122965813, 0.022437337785959244], learning rate: 0.0100000000
Iteration 5000: total loss 0.1352, losses: [0.06630653142929077, 0.06889549642801285], learning rate: 0.0100000000
Iteration 40000: total loss 0.0675, losses: [0.03395616635680199, 0.0335712693631649], learning rate: 0.0100000000
Val loss  tensor(1.0890)
Val loss  tensor(1.1472)
Iteration 10000: total loss 0.1385, losses: [0.06669340282678604, 0.0718412846326828], learning rate: 0.0100000000
Val loss  tensor(1.1191)
Val loss  tensor(1.1678)
Iteration 45000: total loss 0.0344, losses: [0.016887862235307693, 0.017521608620882034], learning rate: 0.0100000000
Iteration 15000: total loss 0.0799, losses: [0.03896864131093025, 0.04092967510223389], learning rate: 0.0100000000
Iteration 50000: total loss 0.0325, losses: [0.01570706255733967, 0.016810711473226547], learning rate: 0.0100000000
Val loss  tensor(1.1301)
Val loss  tensor(1.2159)
Iteration 20000: total loss 0.0961, losses: [0.046871524304151535, 0.049201276153326035], learning rate: 0.0100000000
Val loss  tensor(1.1361)
Val loss  tensor(1.1909)
Iteration 55000: total loss 0.0559, losses: [0.027811599895358086, 0.02806711196899414], learning rate: 0.0100000000
Iteration 25000: total loss 0.0857, losses: [0.041734855622053146, 0.044004589319229126], learning rate: 0.0100000000
Iteration 60000: total loss 0.0305, losses: [0.015044263564050198, 0.01544451154768467], learning rate: 0.0100000000
Val loss  tensor(1.1733)
Val loss  tensor(1.2071)
Iteration 30000: total loss 0.0638, losses: [0.03086363710463047, 0.032917413860559464], learning rate: 0.0100000000
Val loss  tensor(1.0994)
Val loss  tensor(1.1762)
Iteration 65000: total loss 0.0237, losses: [0.01165368314832449, 0.012068274430930614], learning rate: 0.0099000000
Iteration 35000: total loss 0.0702, losses: [0.03452762961387634, 0.03565119579434395], learning rate: 0.0100000000
Iteration 70000: total loss 0.0239, losses: [0.011910744942724705, 0.011983434669673443], learning rate: 0.0099000000
Val loss  tensor(1.1595)
Val loss  tensor(1.2563)
Iteration 40000: total loss 0.0547, losses: [0.027100181207060814, 0.027570290490984917], learning rate: 0.0100000000
Val loss  tensor(1.0933)
Val loss  tensor(1.1679)
Iteration 75000: total loss 0.0256, losses: [0.012464663945138454, 0.013101592659950256], learning rate: 0.0099000000
Iteration 45000: total loss 0.0523, losses: [0.02623547427356243, 0.02605351246893406], learning rate: 0.0100000000
Iteration 80000: total loss 0.0219, losses: [0.010911642573773861, 0.011005702428519726], learning rate: 0.0099000000
Val loss  tensor(1.0959)
Val loss  tensor(1.2491)
Iteration 50000: total loss 0.0556, losses: [0.02737259864807129, 0.02818724513053894], learning rate: 0.0100000000
Val loss  tensor(1.0990)
Val loss  tensor(1.1774)
Iteration 85000: total loss 0.0477, losses: [0.023443395271897316, 0.024296656250953674], learning rate: 0.0099000000
Iteration 55000: total loss 0.1162, losses: [0.05752130225300789, 0.05869637802243233], learning rate: 0.0100000000
Iteration 90000: total loss 0.0245, losses: [0.01224610861390829, 0.01226106658577919], learning rate: 0.0099000000
Val loss  tensor(1.1043)
Val loss  tensor(1.1885)
Iteration 60000: total loss 0.0640, losses: [0.030898502096533775, 0.03308385610580444], learning rate: 0.0100000000
Val loss  tensor(1.1481)
Val loss  tensor(1.2009)
Iteration 95000: total loss 0.0305, losses: [0.015090162865817547, 0.01536186970770359], learning rate: 0.0099000000
Iteration 65000: total loss 0.0555, losses: [0.027271408587694168, 0.02825484238564968], learning rate: 0.0100000000
Iteration 100000: total loss 0.0533, losses: [0.026309877634048462, 0.026998871937394142], learning rate: 0.0099000000
Val loss  tensor(0.9111)
Val loss  tensor(1.1769)
Iteration 70000: total loss 0.0582, losses: [0.028220245614647865, 0.03000137209892273], learning rate: 0.0100000000
Val loss  tensor(1.1140)
Val loss  tensor(1.1842)
Iteration 105000: total loss 0.0250, losses: [0.011625218205153942, 0.013345819897949696], learning rate: 0.0099000000
Iteration 75000: total loss 0.0581, losses: [0.02899106778204441, 0.02913476899266243], learning rate: 0.0100000000
Iteration 110000: total loss 0.0247, losses: [0.01224195584654808, 0.012445543892681599], learning rate: 0.0099000000
Val loss  tensor(1.1935)
Val loss  tensor(1.3114)
Iteration 80000: total loss 0.0464, losses: [0.022188624367117882, 0.02421092800796032], learning rate: 0.0100000000
Val loss  tensor(1.1067)
Val loss  tensor(1.1823)
Iteration 115000: total loss 0.0445, losses: [0.022185999900102615, 0.022312423214316368], learning rate: 0.0099000000
Iteration 85000: total loss 0.0768, losses: [0.037811245769262314, 0.03901653736829758], learning rate: 0.0100000000
Iteration 120000: total loss 0.0229, losses: [0.011314858682453632, 0.011575999669730663], learning rate: 0.0099000000
Val loss  tensor(0.9243)
Val loss  tensor(1.1235)
Iteration 90000: total loss 0.0815, losses: [0.03886289522051811, 0.042646266520023346], learning rate: 0.0100000000
Val loss  tensor(1.1205)
Val loss  tensor(1.1681)
Iteration 125000: total loss 0.0309, losses: [0.01519711222499609, 0.01570538990199566], learning rate: 0.0099000000
Iteration 95000: total loss 0.0701, losses: [0.03402968868613243, 0.03607549890875816], learning rate: 0.0099000000
Iteration 130000: total loss 0.0371, losses: [0.018557565286755562, 0.01855325512588024], learning rate: 0.0099000000
Val loss  tensor(1.1189)
Val loss  tensor(1.1871)
Iteration 100000: total loss 0.0580, losses: [0.02817394770681858, 0.029827622696757317], learning rate: 0.0099000000
Val loss  tensor(1.1001)
Val loss  tensor(1.1986)
Iteration 135000: total loss 0.0203, losses: [0.010054013691842556, 0.010250425897538662], learning rate: 0.0099000000
Iteration 105000: total loss 0.0429, losses: [0.020890377461910248, 0.02205537259578705], learning rate: 0.0099000000
Iteration 140000: total loss 0.0302, losses: [0.01507095992565155, 0.015149031765758991], learning rate: 0.0099000000
Val loss  tensor(1.0019)
Val loss  tensor(1.1593)
Iteration 110000: total loss 0.0840, losses: [0.04136065021157265, 0.04267065227031708], learning rate: 0.0099000000
Val loss  tensor(1.1137)
Val loss  tensor(1.1693)
Iteration 145000: total loss 0.0205, losses: [0.009840716607868671, 0.010631843470036983], learning rate: 0.0099000000
Iteration 115000: total loss 0.0424, losses: [0.020679906010627747, 0.02172461524605751], learning rate: 0.0099000000
Iteration 150000: total loss 0.0387, losses: [0.019126160070300102, 0.019613580778241158], learning rate: 0.0099000000
Val loss  tensor(1.0184)
Val loss  tensor(1.1691)
Iteration 120000: total loss 0.0421, losses: [0.02027258276939392, 0.021793777123093605], learning rate: 0.0099000000
Val loss  tensor(1.0836)
Val loss  tensor(1.1912)
Iteration 155000: total loss 0.0393, losses: [0.019436225295066833, 0.019836056977510452], learning rate: 0.0098010000
Iteration 125000: total loss 0.0743, losses: [0.037078727036714554, 0.03717423602938652], learning rate: 0.0099000000
Iteration 160000: total loss 0.0348, losses: [0.01744030974805355, 0.017332013696432114], learning rate: 0.0098010000
Val loss  tensor(1.2003)
Val loss  tensor(1.2358)
Iteration 130000: total loss 0.0601, losses: [0.02988986112177372, 0.030184024944901466], learning rate: 0.0099000000
Val loss  tensor(1.0939)
Val loss  tensor(1.1988)
Iteration 165000: total loss 0.0401, losses: [0.019806956872344017, 0.020311465486884117], learning rate: 0.0098010000
Iteration 135000: total loss 0.0588, losses: [0.028529977425932884, 0.030243266373872757], learning rate: 0.0099000000
Iteration 170000: total loss 0.0223, losses: [0.01113138161599636, 0.011119508184492588], learning rate: 0.0098010000
Val loss  tensor(0.9447)
Val loss  tensor(1.1707)
Iteration 140000: total loss 0.0967, losses: [0.04863370954990387, 0.04807795211672783], learning rate: 0.0099000000
Val loss  tensor(1.1508)
Val loss  tensor(1.2306)
Iteration 175000: total loss 0.0333, losses: [0.01631181500852108, 0.01696079783141613], learning rate: 0.0098010000
Iteration 145000: total loss 0.0715, losses: [0.03511745482683182, 0.03641920164227486], learning rate: 0.0099000000
Iteration 180000: total loss 0.0199, losses: [0.00970529206097126, 0.010199001058936119], learning rate: 0.0098010000
Val loss  tensor(1.0262)
Iteration 150000: total loss 0.0569, losses: [0.027129901573061943, 0.02977173589169979], learning rate: 0.0099000000
Val loss  tensor(1.0830)
Val loss  tensor(1.1446)
Val loss  tensor(1.1808)
Iteration 185000: total loss 0.0208, losses: [0.009893588721752167, 0.01085752248764038], learning rate: 0.0098010000
Iteration 155000: total loss 0.0414, losses: [0.02030292898416519, 0.021076150238513947], learning rate: 0.0099000000
Iteration 190000: total loss 0.0191, losses: [0.009279212914407253, 0.009867419488728046], learning rate: 0.0098010000
Val loss  tensor(1.1684)
Iteration 160000: total loss 0.0402, losses: [0.019112348556518555, 0.021089166402816772], learning rate: 0.0099000000
Val loss  tensor(1.1103)
Val loss  tensor(1.2059)
Val loss  tensor(1.1794)
Iteration 195000: total loss 0.0226, losses: [0.010882309637963772, 0.011673248372972012], learning rate: 0.0098010000
Iteration 165000: total loss 0.0684, losses: [0.03280628100037575, 0.0356009267270565], learning rate: 0.0099000000
Iteration 200000: total loss 0.0193, losses: [0.009485567919909954, 0.009860852733254433], learning rate: 0.0098010000
Val loss  tensor(1.1432)
Iteration 170000: total loss 0.0457, losses: [0.021917732432484627, 0.023768967017531395], learning rate: 0.0099000000
Val loss  tensor(1.1025)
Val loss  tensor(1.1994)
Val loss  tensor(1.1828)
Iteration 205000: total loss 0.0374, losses: [0.01872555911540985, 0.0187069084495306], learning rate: 0.0098010000
Iteration 175000: total loss 0.0575, losses: [0.02736823819577694, 0.030121609568595886], learning rate: 0.0099000000
Iteration 210000: total loss 0.0232, losses: [0.011374636553227901, 0.011835924349725246], learning rate: 0.0098010000
Val loss  tensor(1.1117)
Iteration 180000: total loss 0.0667, losses: [0.03236840292811394, 0.03429146856069565], learning rate: 0.0099000000
Val loss  tensor(1.1227)
Val loss  tensor(1.1814)
Val loss  tensor(1.1743)
Iteration 215000: total loss 0.0188, losses: [0.009453732520341873, 0.009328004904091358], learning rate: 0.0097029900
Iteration 185000: total loss 0.0397, losses: [0.018743129447102547, 0.021006131544709206], learning rate: 0.0099000000
Iteration 220000: total loss 0.0336, losses: [0.016302112489938736, 0.017318880185484886], learning rate: 0.0097029900
Iteration 190000: total loss 0.0385, losses: [0.018681734800338745, 0.019801190122961998], learning rate: 0.0099000000
Val loss  tensor(0.9862)
Val loss  tensor(1.0957)
Val loss  tensor(1.1576)
Val loss  tensor(1.2042)
Iteration 195000: total loss 0.0606, losses: [0.02896098606288433, 0.031652458012104034], learning rate: 0.0099000000
Iteration 225000: total loss 0.0250, losses: [0.012446572072803974, 0.012595064006745815], learning rate: 0.0097029900
Iteration 200000: total loss 0.1519, losses: [0.07669772952795029, 0.07518631964921951], learning rate: 0.0099000000
Val loss  tensor(1.0942)
Iteration 230000: total loss 0.0217, losses: [0.010725922882556915, 0.0110140610486269], learning rate: 0.0097029900
Val loss  tensor(0.9902)
Val loss  tensor(1.2146)
Val loss  tensor(1.1516)
Iteration 205000: total loss 0.0480, losses: [0.023513568565249443, 0.024509379640221596], learning rate: 0.0098010000
Iteration 235000: total loss 0.0227, losses: [0.011174566112458706, 0.011504413560032845], learning rate: 0.0097029900
Iteration 210000: total loss 0.0584, losses: [0.028656749054789543, 0.02970007061958313], learning rate: 0.0098010000
Val loss  tensor(1.1871)
Iteration 240000: total loss 0.0184, losses: [0.00894246157258749, 0.009483597241342068], learning rate: 0.0097029900
Val loss  tensor(0.9702)
Val loss  tensor(1.2239)
Val loss  tensor(1.1539)
Iteration 215000: total loss 0.0526, losses: [0.02595498040318489, 0.02666737511754036], learning rate: 0.0098010000
Iteration 245000: total loss 0.0378, losses: [0.018976857885718346, 0.018817611038684845], learning rate: 0.0097029900
Iteration 220000: total loss 0.0391, losses: [0.01906026341021061, 0.0200802069157362], learning rate: 0.0098010000
Val loss  tensor(1.1661)
Iteration 250000: total loss 0.0190, losses: [0.009593515656888485, 0.009358551353216171], learning rate: 0.0097029900
Val loss  tensor(0.8954)
Val loss  tensor(1.1882)
Val loss  tensor(1.1851)
Iteration 225000: total loss 0.0435, losses: [0.021125534549355507, 0.022327005863189697], learning rate: 0.0098010000
Iteration 255000: total loss 0.0177, losses: [0.008793110027909279, 0.008859631605446339], learning rate: 0.0097029900
Iteration 230000: total loss 0.0393, losses: [0.019203027710318565, 0.020072434097528458], learning rate: 0.0098010000
Val loss  tensor(1.2228)
Iteration 260000: total loss 0.0156, losses: [0.007354175206273794, 0.008261044509708881], learning rate: 0.0097029900
Val loss  tensor(0.9347)
Val loss  tensor(1.2595)
Val loss  tensor(1.1571)
Iteration 235000: total loss 0.0365, losses: [0.018100323155522346, 0.018378620967268944], learning rate: 0.0098010000
Iteration 265000: total loss 0.0179, losses: [0.008642534725368023, 0.009209091775119305], learning rate: 0.0097029900
Iteration 240000: total loss 0.0376, losses: [0.018576854839920998, 0.019006429240107536], learning rate: 0.0098010000
Val loss  tensor(1.1758)
Iteration 270000: total loss 0.0312, losses: [0.01541262399405241, 0.015829207375645638], learning rate: 0.0097029900
Val loss  tensor(1.1124)
Val loss  tensor(1.1997)
Val loss  tensor(1.2652)
Iteration 245000: total loss 0.0742, losses: [0.036831457167863846, 0.037405479699373245], learning rate: 0.0098010000
Iteration 275000: total loss 0.0167, losses: [0.008334408514201641, 0.00838104635477066], learning rate: 0.0097029900
Iteration 250000: total loss 0.0526, losses: [0.025693625211715698, 0.02694207802414894], learning rate: 0.0098010000
Val loss  tensor(1.1554)
Iteration 280000: total loss 0.0206, losses: [0.010133860632777214, 0.010435161180794239], learning rate: 0.0097029900
Val loss  tensor(1.2220)
Val loss  tensor(1.2197)
Val loss  tensor(1.2346)
Iteration 255000: total loss 0.0583, losses: [0.028751561418175697, 0.029557669535279274], learning rate: 0.0098010000
Iteration 285000: total loss 0.0314, losses: [0.015607274137437344, 0.01576380245387554], learning rate: 0.0097029900
Iteration 260000: total loss 0.0744, losses: [0.037211813032627106, 0.03718810901045799], learning rate: 0.0098010000
Val loss  tensor(1.1713)
Iteration 290000: total loss 0.0228, losses: [0.011009984649717808, 0.011753912083804607], learning rate: 0.0097029900
Val loss  tensor(1.0941)
Val loss  tensor(1.2065)
Val loss  tensor(1.1773)
Iteration 265000: total loss 0.0359, losses: [0.01721426285803318, 0.01863727904856205], learning rate: 0.0097029900
Iteration 295000: total loss 0.0173, losses: [0.008617717772722244, 0.008646751753985882], learning rate: 0.0097029900
Iteration 270000: total loss 0.0459, losses: [0.022287575528025627, 0.023577511310577393], learning rate: 0.0097029900
Val loss  tensor(1.1476)
Iteration 300000: total loss 0.0173, losses: [0.008729705587029457, 0.008527010679244995], learning rate: 0.0097029900
Val loss  tensor(0.9730)
Val loss  tensor(1.2594)
Val loss  tensor(1.1462)
Iteration 275000: total loss 0.0373, losses: [0.017805155366659164, 0.019495032727718353], learning rate: 0.0097029900
Iteration 305000: total loss 0.0182, losses: [0.008703474886715412, 0.009498592466115952], learning rate: 0.0096059601
Iteration 280000: total loss 0.0446, losses: [0.022481951862573624, 0.0221224557608366], learning rate: 0.0097029900
Val loss  tensor(1.1792)
Iteration 310000: total loss 0.0212, losses: [0.010393482632935047, 0.01082321535795927], learning rate: 0.0096059601
Val loss  tensor(1.2166)
Val loss  tensor(1.1655)
Val loss  tensor(1.2108)
Iteration 285000: total loss 0.0460, losses: [0.022230980917811394, 0.023721303790807724], learning rate: 0.0097029900
Iteration 315000: total loss 0.0284, losses: [0.014022516086697578, 0.014395937323570251], learning rate: 0.0096059601
Iteration 290000: total loss 0.0351, losses: [0.01728115789592266, 0.017803369089961052], learning rate: 0.0097029900
Val loss  tensor(1.1756)
Iteration 320000: total loss 0.0286, losses: [0.014208734035491943, 0.014387081377208233], learning rate: 0.0096059601
Val loss  tensor(1.2013)
Val loss  tensor(0.8310)
Val loss  tensor(1.1692)
Iteration 295000: total loss 0.0523, losses: [0.026117999106645584, 0.0261848121881485], learning rate: 0.0097029900
Iteration 325000: total loss 0.0388, losses: [0.019563455134630203, 0.01924879103899002], learning rate: 0.0096059601
Iteration 300000: total loss 0.0421, losses: [0.020706763491034508, 0.021372748538851738], learning rate: 0.0097029900
Val loss  tensor(1.2345)
Val loss  tensor(1.2112)
Iteration 330000: total loss 0.0286, losses: [0.014412184245884418, 0.014212149195373058], learning rate: 0.0096059601
Val loss  tensor(1.1952)
Val loss  tensor(1.2180)
Iteration 305000: total loss 0.0341, losses: [0.01683487556874752, 0.017290184274315834], learning rate: 0.0097029900
Iteration 335000: total loss 0.0318, losses: [0.015658246353268623, 0.01610158197581768], learning rate: 0.0096059601
Iteration 310000: total loss 0.0445, losses: [0.021955175325274467, 0.02254277653992176], learning rate: 0.0097029900
Val loss  tensor(1.1860)
Val loss  tensor(1.2242)
Iteration 340000: total loss 0.0178, losses: [0.008523771539330482, 0.009280095808207989], learning rate: 0.0096059601
Val loss  tensor(0.9831)
Val loss  tensor(1.1760)
Iteration 315000: total loss 0.0303, losses: [0.015013793483376503, 0.015332548879086971], learning rate: 0.0096059601
Iteration 345000: total loss 0.0311, losses: [0.015593054704368114, 0.015496784821152687], learning rate: 0.0096059601
Iteration 320000: total loss 0.0291, losses: [0.014382312074303627, 0.01472410000860691], learning rate: 0.0096059601
Val loss  tensor(1.1972)
Val loss  tensor(1.2313)
Iteration 350000: total loss 0.0173, losses: [0.008482011966407299, 0.00881773978471756], learning rate: 0.0096059601
Val loss  tensor(1.0020)
Val loss  tensor(1.1813)
Iteration 325000: total loss 0.1140, losses: [0.05583114176988602, 0.058188438415527344], learning rate: 0.0096059601
Iteration 355000: total loss 0.0173, losses: [0.008519617840647697, 0.008764609694480896], learning rate: 0.0096059601
Iteration 330000: total loss 0.0402, losses: [0.020129812881350517, 0.0200890451669693], learning rate: 0.0096059601
Val loss  tensor(1.2282)
Val loss  tensor(1.2244)
Iteration 360000: total loss 0.0190, losses: [0.009078753180801868, 0.009880808182060719], learning rate: 0.0096059601
Val loss  tensor(1.1254)
Val loss  tensor(1.1755)
Iteration 335000: total loss 0.0409, losses: [0.020007634535431862, 0.02093014493584633], learning rate: 0.0096059601
Iteration 365000: total loss 0.0178, losses: [0.008817262016236782, 0.008932976052165031], learning rate: 0.0096059601
Iteration 340000: total loss 0.1152, losses: [0.05758189037442207, 0.05760129168629646], learning rate: 0.0096059601
Val loss  tensor(1.1348)
Val loss  tensor(1.2016)
Iteration 370000: total loss 0.0338, losses: [0.01654486171901226, 0.017294947057962418], learning rate: 0.0096059601
Val loss  tensor(1.1332)
Val loss  tensor(1.2197)
Iteration 345000: total loss 0.0467, losses: [0.022880086675286293, 0.023798100650310516], learning rate: 0.0096059601
Iteration 375000: total loss 0.0185, losses: [0.009249239228665829, 0.009283780120313168], learning rate: 0.0095099005
Iteration 350000: total loss 0.0531, losses: [0.026310591027140617, 0.02682938612997532], learning rate: 0.0096059601
Val loss  tensor(1.1647)
Val loss  tensor(1.2212)
Iteration 380000: total loss 0.0188, losses: [0.009366552345454693, 0.00942596048116684], learning rate: 0.0095099005
Val loss  tensor(0.7815)
Val loss  tensor(1.1493)
Iteration 355000: total loss 0.0256, losses: [0.012382224202156067, 0.013241231441497803], learning rate: 0.0096059601
Iteration 385000: total loss 0.0182, losses: [0.008922717534005642, 0.009238864295184612], learning rate: 0.0095099005
Iteration 360000: total loss 0.0331, losses: [0.016268810257315636, 0.016786077991127968], learning rate: 0.0096059601
Val loss  tensor(1.2334)
Val loss  tensor(1.2180)
Iteration 390000: total loss 0.0314, losses: [0.015434634871780872, 0.015920257195830345], learning rate: 0.0095099005
Val loss  tensor(0.9784)
Val loss  tensor(1.2251)
Iteration 365000: total loss 0.0260, losses: [0.012484133243560791, 0.013496661558747292], learning rate: 0.0096059601
Iteration 395000: total loss 0.0181, losses: [0.008707410655915737, 0.009389066137373447], learning rate: 0.0095099005
Iteration 370000: total loss 0.0431, losses: [0.021394958719611168, 0.021722285076975822], learning rate: 0.0096059601
Val loss  tensor(1.1397)
Val loss  tensor(1.2252)
Iteration 400000: total loss 0.0221, losses: [0.01073196530342102, 0.011413355357944965], learning rate: 0.0095099005
Val loss  tensor(1.2200)
Val loss  tensor(1.2905)
Iteration 375000: total loss 0.0468, losses: [0.022832514718174934, 0.023943090811371803], learning rate: 0.0095099005
Iteration 405000: total loss 0.0193, losses: [0.009212756529450417, 0.01009969413280487], learning rate: 0.0095099005
Iteration 380000: total loss 0.0311, losses: [0.015334575437009335, 0.01572832092642784], learning rate: 0.0095099005
Val loss  tensor(1.1754)
Val loss  tensor(1.1964)
Iteration 410000: total loss 0.0282, losses: [0.013730265200138092, 0.014431296847760677], learning rate: 0.0095099005
Val loss  tensor(1.2009)
Val loss  tensor(1.2345)
Iteration 385000: total loss 0.0450, losses: [0.021992607042193413, 0.02299521304666996], learning rate: 0.0095099005
Iteration 415000: total loss 0.0190, losses: [0.009439955465495586, 0.009563248604536057], learning rate: 0.0095099005
Iteration 390000: total loss 0.0480, losses: [0.023384440690279007, 0.024591660127043724], learning rate: 0.0095099005
Val loss  tensor(1.2349)
Val loss  tensor(1.2252)
Iteration 420000: total loss 0.0382, losses: [0.018963061273097992, 0.019210085272789], learning rate: 0.0095099005
Val loss  tensor(1.1508)
Val loss  tensor(1.2356)
Iteration 395000: total loss 0.0272, losses: [0.013126934878528118, 0.014088178053498268], learning rate: 0.0095099005
Iteration 425000: total loss 0.0185, losses: [0.009180773980915546, 0.009350891225039959], learning rate: 0.0095099005
Iteration 400000: total loss 0.0553, losses: [0.027310563251376152, 0.028037581592798233], learning rate: 0.0095099005
Val loss  tensor(1.1699)
Val loss  tensor(1.2367)
Iteration 430000: total loss 0.0201, losses: [0.010004484094679356, 0.010081457905471325], learning rate: 0.0095099005
Val loss  tensor(1.1197)
Val loss  tensor(1.1809)
Iteration 405000: total loss 0.0599, losses: [0.029148375615477562, 0.03073769249022007], learning rate: 0.0095099005
Iteration 435000: total loss 0.0157, losses: [0.0077428328804671764, 0.007971790619194508], learning rate: 0.0094148015
Iteration 410000: total loss 0.0382, losses: [0.019235366955399513, 0.018988406285643578], learning rate: 0.0095099005
Val loss  tensor(1.1503)
Val loss  tensor(1.2089)
Iteration 440000: total loss 0.0165, losses: [0.008171877823770046, 0.008293600752949715], learning rate: 0.0094148015
Val loss  tensor(0.9952)
Val loss  tensor(1.1994)
Iteration 415000: total loss 0.0274, losses: [0.013412738218903542, 0.01396297849714756], learning rate: 0.0095099005
Iteration 445000: total loss 0.0175, losses: [0.008752441965043545, 0.008752420544624329], learning rate: 0.0094148015
Iteration 420000: total loss 0.0275, losses: [0.013561898842453957, 0.01390570867806673], learning rate: 0.0095099005
Val loss  tensor(1.1426)
Val loss  tensor(1.2144)
Iteration 450000: total loss 0.0153, losses: [0.007430905010551214, 0.00786843802779913], learning rate: 0.0094148015
Val loss  tensor(1.0002)
Val loss  tensor(1.1805)
Iteration 425000: total loss 0.0284, losses: [0.01401269156485796, 0.014423376880586147], learning rate: 0.0094148015
Iteration 455000: total loss 0.0161, losses: [0.007816314697265625, 0.008325747214257717], learning rate: 0.0094148015
Iteration 430000: total loss 0.0287, losses: [0.013943946920335293, 0.01474923174828291], learning rate: 0.0094148015
Val loss  tensor(1.2220)
Val loss  tensor(1.2246)
Iteration 460000: total loss 0.0207, losses: [0.010304528288543224, 0.010444867424666882], learning rate: 0.0094148015
Val loss  tensor(1.1296)
Val loss  tensor(1.1961)
Iteration 435000: total loss 0.0432, losses: [0.020813794806599617, 0.022338727489113808], learning rate: 0.0094148015
Iteration 465000: total loss 0.0178, losses: [0.008707350119948387, 0.009088399820029736], learning rate: 0.0094148015
Iteration 440000: total loss 0.0422, losses: [0.020571041852235794, 0.021657027304172516], learning rate: 0.0094148015
Val loss  tensor(1.2558)
Val loss  tensor(1.2669)
Iteration 470000: total loss 0.0310, losses: [0.015788424760103226, 0.015195660293102264], learning rate: 0.0094148015
Val loss  tensor(1.0134)
Val loss  tensor(1.1743)
Iteration 445000: total loss 0.0398, losses: [0.02019992284476757, 0.019650069996714592], learning rate: 0.0094148015
Iteration 475000: total loss 0.0224, losses: [0.011165720410645008, 0.011244994588196278], learning rate: 0.0094148015
Iteration 450000: total loss 0.0443, losses: [0.021694082766771317, 0.02263428270816803], learning rate: 0.0094148015
Val loss  tensor(1.2224)
Val loss  tensor(1.2308)
Iteration 480000: total loss 0.0164, losses: [0.008067701943218708, 0.008376444689929485], learning rate: 0.0094148015
Val loss  tensor(1.2069)
Val loss  tensor(1.2331)
Iteration 455000: total loss 0.0365, losses: [0.01745022088289261, 0.019083529710769653], learning rate: 0.0094148015
Iteration 485000: total loss 0.0154, losses: [0.007368089165538549, 0.008060569874942303], learning rate: 0.0094148015
Iteration 460000: total loss 0.0459, losses: [0.022784395143389702, 0.023156484588980675], learning rate: 0.0094148015
Val loss  tensor(1.1716)
Val loss  tensor(1.2027)
Iteration 490000: total loss 0.0157, losses: [0.007635340094566345, 0.008072946220636368], learning rate: 0.0094148015
Val loss  tensor(1.1205)
Val loss  tensor(1.1957)
Iteration 465000: total loss 0.0409, losses: [0.020507285371422768, 0.020423848181962967], learning rate: 0.0094148015
Iteration 495000: total loss 0.0204, losses: [0.009965001605451107, 0.010469682514667511], learning rate: 0.0093206535
Iteration 470000: total loss 0.0382, losses: [0.018393168225884438, 0.019811255857348442], learning rate: 0.0094148015
Val loss  tensor(1.2268)
Val loss  tensor(1.2418)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): SiLU()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=256, bias=True)
    (7): ReLU()
    (8): Linear(in_features=256, out_features=2, bias=True)
  )
)
Iteration 475000: total loss 0.0394, losses: [0.018990784883499146, 0.020407123491168022], learning rate: 0.0094148015
Iteration 480000: total loss 0.0355, losses: [0.01740158535540104, 0.018129942938685417], learning rate: 0.0094148015
Val loss  tensor(1.2183)
Val loss  tensor(1.2230)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.4805, losses: [1.2510936260223389, 1.2293704748153687], learning rate: 0.0100000000
Val loss  tensor(2.2546)
Val loss  tensor(2.2402)
Iteration 485000: total loss 0.0458, losses: [0.02240566723048687, 0.02343788556754589], learning rate: 0.0093206535
Iteration 5000: total loss 0.0875, losses: [0.04424983263015747, 0.0432831235229969], learning rate: 0.0100000000
Iteration 490000: total loss 0.0559, losses: [0.02786659635603428, 0.028080081567168236], learning rate: 0.0093206535
Val loss  tensor(1.2044)
Val loss  tensor(1.2191)
Iteration 495000: total loss 0.0387, losses: [0.019001513719558716, 0.019743984565138817], learning rate: 0.0093206535
Iteration 10000: total loss 0.0815, losses: [0.041364192962646484, 0.04013478383421898], learning rate: 0.0100000000
Val loss  tensor(1.0267)
Val loss  tensor(1.1856)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): SiLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): Tanh()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 15000: total loss 0.0703, losses: [0.034678198397159576, 0.03559131175279617], learning rate: 0.0100000000
Iteration 20000: total loss 0.0538, losses: [0.02711939625442028, 0.026717010885477066], learning rate: 0.0100000000
Val loss  tensor(1.1290)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.6002, losses: [1.3113892078399658, 1.28876531124115], learning rate: 0.0100000000
Val loss  tensor(1.1321)
Val loss  tensor(1.2476)
Val loss  tensor(1.0477)
Iteration 5000: total loss 0.1374, losses: [0.06758173555135727, 0.06985051929950714], learning rate: 0.0100000000
Iteration 25000: total loss 0.0504, losses: [0.02473534271121025, 0.025622118264436722], learning rate: 0.0100000000
Iteration 10000: total loss 0.1204, losses: [0.06042350456118584, 0.06000996753573418], learning rate: 0.0100000000
Val loss  tensor(1.1683)
Val loss  tensor(1.2180)
Iteration 30000: total loss 0.0590, losses: [0.028932129964232445, 0.030117113143205643], learning rate: 0.0100000000
Val loss  tensor(1.1858)
Val loss  tensor(1.1853)
Iteration 15000: total loss 0.0723, losses: [0.03698074817657471, 0.0353233776986599], learning rate: 0.0100000000
Iteration 20000: total loss 0.0641, losses: [0.032066620886325836, 0.03207787498831749], learning rate: 0.0100000000
Val loss  tensor(1.0368)
Val loss  tensor(1.1205)
Iteration 35000: total loss 0.0478, losses: [0.023552656173706055, 0.024222172796726227], learning rate: 0.0100000000
Iteration 25000: total loss 0.0983, losses: [0.049557145684957504, 0.04872788116335869], learning rate: 0.0100000000
Iteration 40000: total loss 0.0592, losses: [0.029564648866653442, 0.029648849740624428], learning rate: 0.0100000000
Val loss  tensor(1.2111)
Val loss  tensor(1.2272)
Iteration 30000: total loss 0.1034, losses: [0.05186627432703972, 0.051503755152225494], learning rate: 0.0100000000
Val loss  tensor(0.9346)
Val loss  tensor(1.1290)
Iteration 35000: total loss 0.0780, losses: [0.038974713534116745, 0.039075274020433426], learning rate: 0.0100000000
Iteration 45000: total loss 0.0586, losses: [0.02894159033894539, 0.02969648875296116], learning rate: 0.0100000000
Iteration 40000: total loss 0.0930, losses: [0.04645882919430733, 0.04652681574225426], learning rate: 0.0100000000
Val loss  tensor(1.2273)
Val loss  tensor(1.2508)
Iteration 50000: total loss 0.0365, losses: [0.017703568562865257, 0.018765566870570183], learning rate: 0.0100000000
Val loss  tensor(1.0307)
Val loss  tensor(1.2096)
Iteration 45000: total loss 0.0741, losses: [0.03670918196439743, 0.03741547092795372], learning rate: 0.0100000000
Iteration 50000: total loss 0.0727, losses: [0.036710213869810104, 0.036011528223752975], learning rate: 0.0100000000
Val loss  tensor(1.0375)
Val loss  tensor(1.1706)
Iteration 55000: total loss 0.0502, losses: [0.024584446102380753, 0.02562432922422886], learning rate: 0.0100000000
Iteration 55000: total loss 0.0770, losses: [0.03861505165696144, 0.038399528712034225], learning rate: 0.0100000000
Iteration 60000: total loss 0.0267, losses: [0.012872576713562012, 0.013784139417111874], learning rate: 0.0100000000
Val loss  tensor(1.1319)
Val loss  tensor(1.3160)
Iteration 60000: total loss 0.0863, losses: [0.042914628982543945, 0.043375007808208466], learning rate: 0.0100000000
Val loss  tensor(1.1035)
Val loss  tensor(1.2253)
Iteration 65000: total loss 0.0985, losses: [0.048955004662275314, 0.04958387836813927], learning rate: 0.0100000000
Iteration 65000: total loss 0.0283, losses: [0.013914915733039379, 0.014428130351006985], learning rate: 0.0099000000
Iteration 70000: total loss 0.0811, losses: [0.040863942354917526, 0.040196578949689865], learning rate: 0.0100000000
Val loss  tensor(0.8555)
Val loss  tensor(1.1588)
Iteration 70000: total loss 0.0379, losses: [0.01880441978573799, 0.019105184823274612], learning rate: 0.0099000000
Val loss  tensor(1.1955)
Val loss  tensor(1.2531)
Iteration 75000: total loss 0.0894, losses: [0.04504585638642311, 0.04437420889735222], learning rate: 0.0100000000
Iteration 80000: total loss 0.0787, losses: [0.03963708505034447, 0.03902527317404747], learning rate: 0.0100000000
Val loss  tensor(1.2365)
Val loss  tensor(1.2489)
Iteration 75000: total loss 0.0446, losses: [0.022156057879328728, 0.022411635145545006], learning rate: 0.0099000000
Iteration 85000: total loss 0.0815, losses: [0.04110616073012352, 0.04034725949168205], learning rate: 0.0100000000
Iteration 80000: total loss 0.0331, losses: [0.016400812193751335, 0.016719767823815346], learning rate: 0.0099000000
Val loss  tensor(1.2307)
Val loss  tensor(1.2203)
Iteration 90000: total loss 0.0690, losses: [0.03466011956334114, 0.034351807087659836], learning rate: 0.0100000000
Val loss  tensor(0.7651)
Val loss  tensor(1.2348)
Iteration 95000: total loss 0.0781, losses: [0.03916437551379204, 0.038939304649829865], learning rate: 0.0100000000
Iteration 85000: total loss 0.0381, losses: [0.018731122836470604, 0.019339296966791153], learning rate: 0.0099000000
Iteration 100000: total loss 0.0747, losses: [0.03713401034474373, 0.03760136291384697], learning rate: 0.0100000000
Val loss  tensor(1.1843)
Val loss  tensor(1.2103)
Iteration 90000: total loss 0.0567, losses: [0.027895886451005936, 0.028760625049471855], learning rate: 0.0099000000
Val loss  tensor(1.2355)
Val loss  tensor(1.2386)
Iteration 105000: total loss 0.0932, losses: [0.046122957020998, 0.047047775238752365], learning rate: 0.0100000000
Iteration 110000: total loss 0.0846, losses: [0.04236416146159172, 0.04221109673380852], learning rate: 0.0100000000
Val loss  tensor(1.1128)
Iteration 95000: total loss 0.0462, losses: [0.022521747276186943, 0.02370232716202736], learning rate: 0.0099000000
Val loss  tensor(1.2092)
Iteration 115000: total loss 0.0688, losses: [0.0341474823653698, 0.0346943698823452], learning rate: 0.0100000000
Iteration 100000: total loss 0.0364, losses: [0.017958879470825195, 0.018459534272551537], learning rate: 0.0099000000
Val loss  tensor(1.2322)
Val loss  tensor(1.2213)
Iteration 120000: total loss 0.0829, losses: [0.04118536040186882, 0.04170556366443634], learning rate: 0.0100000000
Val loss  tensor(0.9499)
Val loss  tensor(1.2119)
Iteration 105000: total loss 0.0460, losses: [0.02307271771132946, 0.02289767935872078], learning rate: 0.0099000000
Iteration 125000: total loss 0.0675, losses: [0.03342170640826225, 0.034116219729185104], learning rate: 0.0100000000
Iteration 130000: total loss 0.0612, losses: [0.03043784759938717, 0.030786752700805664], learning rate: 0.0100000000
Val loss  tensor(1.2143)
Val loss  tensor(1.2292)
Iteration 110000: total loss 0.0364, losses: [0.01847034879028797, 0.01797105371952057], learning rate: 0.0099000000
Val loss  tensor(1.2693)
Val loss  tensor(1.2527)
Iteration 135000: total loss 0.0764, losses: [0.038048770278692245, 0.03837447986006737], learning rate: 0.0100000000
Iteration 115000: total loss 0.0469, losses: [0.023000022396445274, 0.023857492953538895], learning rate: 0.0099000000
Iteration 140000: total loss 0.0870, losses: [0.0431278795003891, 0.043910104781389236], learning rate: 0.0100000000
Val loss  tensor(1.2059)
Val loss  tensor(1.2343)
Iteration 145000: total loss 0.0391, losses: [0.019659768790006638, 0.019410327076911926], learning rate: 0.0099000000
Iteration 120000: total loss 0.0441, losses: [0.02223273366689682, 0.021861353889107704], learning rate: 0.0099000000
Val loss  tensor(1.1624)
Val loss  tensor(1.2261)
Iteration 150000: total loss 0.0710, losses: [0.035545047372579575, 0.03549833968281746], learning rate: 0.0099000000
Val loss  tensor(1.0983)
Val loss  tensor(1.2008)
Iteration 125000: total loss 0.0254, losses: [0.0126529885455966, 0.012723276391625404], learning rate: 0.0098010000
Iteration 155000: total loss 0.0691, losses: [0.03440377116203308, 0.034743454307317734], learning rate: 0.0099000000
Iteration 160000: total loss 0.0716, losses: [0.03640099614858627, 0.03523203358054161], learning rate: 0.0099000000
Val loss  tensor(1.2423)
Val loss  tensor(1.2706)
Iteration 130000: total loss 0.0400, losses: [0.019759781658649445, 0.020252767950296402], learning rate: 0.0098010000
Val loss  tensor(1.2000)
Val loss  tensor(1.2266)
Iteration 165000: total loss 0.0900, losses: [0.04519277438521385, 0.0447893850505352], learning rate: 0.0099000000
Iteration 135000: total loss 0.0274, losses: [0.013443908654153347, 0.013956439681351185], learning rate: 0.0098010000
Iteration 170000: total loss 0.0615, losses: [0.030134577304124832, 0.031339120119810104], learning rate: 0.0099000000
Val loss  tensor(1.2329)
Val loss  tensor(1.2514)
Iteration 175000: total loss 0.0777, losses: [0.038428254425525665, 0.039252955466508865], learning rate: 0.0099000000
Iteration 140000: total loss 0.0415, losses: [0.020099790766835213, 0.021406499668955803], learning rate: 0.0098010000
Val loss  tensor(1.1970)
Val loss  tensor(1.1993)
Iteration 180000: total loss 0.0374, losses: [0.018789837136864662, 0.018594548106193542], learning rate: 0.0099000000
Val loss  tensor(1.2695)
Val loss  tensor(1.2614)
Iteration 145000: total loss 0.0363, losses: [0.01743978261947632, 0.01889771781861782], learning rate: 0.0098010000
Iteration 185000: total loss 0.0851, losses: [0.04274015873670578, 0.042362794280052185], learning rate: 0.0099000000
Iteration 190000: total loss 0.0630, losses: [0.03148350864648819, 0.03148285299539566], learning rate: 0.0099000000
Val loss  tensor(1.1757)
Val loss  tensor(1.2056)
Iteration 150000: total loss 0.0365, losses: [0.017962416633963585, 0.018550192937254906], learning rate: 0.0098010000
Val loss  tensor(1.2354)
Val loss  tensor(1.3002)
Iteration 195000: total loss 0.0725, losses: [0.03639347478747368, 0.03614620491862297], learning rate: 0.0099000000
Iteration 155000: total loss 0.0374, losses: [0.01817743293941021, 0.019213814288377762], learning rate: 0.0098010000
Iteration 200000: total loss 0.0830, losses: [0.0413479208946228, 0.0417015515267849], learning rate: 0.0099000000
Val loss  tensor(0.9080)
Val loss  tensor(1.2152)
Iteration 205000: total loss 0.0694, losses: [0.03462798520922661, 0.03475247696042061], learning rate: 0.0098010000
Iteration 160000: total loss 0.0321, losses: [0.015942860394716263, 0.016110137104988098], learning rate: 0.0098010000
Val loss  tensor(1.1289)
Val loss  tensor(1.3243)
Iteration 210000: total loss 0.0862, losses: [0.04322180896997452, 0.043009281158447266], learning rate: 0.0098010000
Val loss  tensor(1.1970)
Val loss  tensor(1.2192)
Iteration 165000: total loss 0.0344, losses: [0.016731761395931244, 0.01771441474556923], learning rate: 0.0098010000
Iteration 215000: total loss 0.0960, losses: [0.04792007431387901, 0.04810647293925285], learning rate: 0.0098010000
Iteration 220000: total loss 0.0800, losses: [0.04024263098835945, 0.0397525429725647], learning rate: 0.0098010000
Val loss  tensor(0.9760)
Val loss  tensor(1.1982)
Iteration 170000: total loss 0.0318, losses: [0.01587032340466976, 0.01597544364631176], learning rate: 0.0098010000
Val loss  tensor(1.2288)
Val loss  tensor(1.2427)
Iteration 225000: total loss 0.0761, losses: [0.03789178654551506, 0.03819727525115013], learning rate: 0.0098010000
Iteration 175000: total loss 0.0352, losses: [0.017494507133960724, 0.017680227756500244], learning rate: 0.0097029900
Iteration 230000: total loss 0.0705, losses: [0.03552373871207237, 0.034983474761247635], learning rate: 0.0098010000
Val loss  tensor(0.8217)
Val loss  tensor(1.2256)
Iteration 235000: total loss 0.0680, losses: [0.03406607732176781, 0.0339706726372242], learning rate: 0.0098010000
Iteration 180000: total loss 0.0392, losses: [0.019457701593637466, 0.019701236858963966], learning rate: 0.0097029900
Val loss  tensor(1.1997)
Val loss  tensor(1.2317)
Iteration 240000: total loss 0.0621, losses: [0.03143689036369324, 0.0306742824614048], learning rate: 0.0098010000
Val loss  tensor(0.7787)
Val loss  tensor(1.1902)
Iteration 185000: total loss 0.0302, losses: [0.014886128716170788, 0.015295848250389099], learning rate: 0.0097029900
Iteration 245000: total loss 0.0601, losses: [0.03003748133778572, 0.030046040192246437], learning rate: 0.0098010000
Iteration 250000: total loss 0.0659, losses: [0.03292488679289818, 0.03297147899866104], learning rate: 0.0098010000
Val loss  tensor(1.2342)
Iteration 190000: total loss 0.0321, losses: [0.01573713682591915, 0.016376784071326256], learning rate: 0.0097029900
Val loss  tensor(1.1733)
Val loss  tensor(1.2557)
Val loss  tensor(1.2209)
Iteration 255000: total loss 0.0839, losses: [0.0419066920876503, 0.04198512062430382], learning rate: 0.0097029900
Iteration 195000: total loss 0.0315, losses: [0.015327155590057373, 0.01620534062385559], learning rate: 0.0097029900
Iteration 260000: total loss 0.0405, losses: [0.020400920882821083, 0.020128754898905754], learning rate: 0.0097029900
Val loss  tensor(1.0936)
Val loss  tensor(1.1929)
Iteration 200000: total loss 0.0349, losses: [0.01739473082125187, 0.017487941309809685], learning rate: 0.0097029900
Val loss  tensor(1.1366)
Iteration 265000: total loss 0.0727, losses: [0.03583504632115364, 0.03682772070169449], learning rate: 0.0097029900
Val loss  tensor(1.2153)
Iteration 270000: total loss 0.0638, losses: [0.03172014281153679, 0.03207217529416084], learning rate: 0.0097029900
Val loss  tensor(0.7255)
Val loss  tensor(1.1828)
Iteration 205000: total loss 0.0396, losses: [0.01906512677669525, 0.020520994439721107], learning rate: 0.0097029900
Iteration 275000: total loss 0.0930, losses: [0.046411722898483276, 0.0466262549161911], learning rate: 0.0097029900
Iteration 210000: total loss 0.0326, losses: [0.01617378555238247, 0.01645154319703579], learning rate: 0.0097029900
Val loss  tensor(1.2197)
Iteration 280000: total loss 0.0722, losses: [0.035896748304367065, 0.036296091973781586], learning rate: 0.0097029900
Val loss  tensor(1.1698)
Val loss  tensor(1.2444)
Val loss  tensor(1.2160)
Iteration 285000: total loss 0.0610, losses: [0.030665123835206032, 0.030315328389406204], learning rate: 0.0097029900
Iteration 215000: total loss 0.0401, losses: [0.019443750381469727, 0.020645854994654655], learning rate: 0.0097029900
Iteration 290000: total loss 0.0719, losses: [0.03599667549133301, 0.03593096882104874], learning rate: 0.0097029900
Val loss  tensor(0.7394)
Val loss  tensor(1.1836)
Iteration 220000: total loss 0.0328, losses: [0.015608824789524078, 0.017167171463370323], learning rate: 0.0097029900
Val loss  tensor(1.1932)
Val loss  tensor(1.2339)
Iteration 295000: total loss 0.0783, losses: [0.03910471126437187, 0.03918598219752312], learning rate: 0.0097029900
Iteration 300000: total loss 0.0895, losses: [0.04457264766097069, 0.04494474455714226], learning rate: 0.0097029900
Val loss  tensor(0.6573)
Val loss  tensor(1.1676)
Iteration 225000: total loss 0.0367, losses: [0.018231932073831558, 0.01845904439687729], learning rate: 0.0097029900
Iteration 305000: total loss 0.0650, losses: [0.032568611204624176, 0.03240831568837166], learning rate: 0.0097029900
Iteration 230000: total loss 0.0281, losses: [0.013757050037384033, 0.014318208210170269], learning rate: 0.0097029900
Val loss  tensor(1.2240)
Iteration 310000: total loss 0.0702, losses: [0.035665031522512436, 0.0345427468419075], learning rate: 0.0097029900
Val loss  tensor(1.2404)
Val loss  tensor(1.1237)
Val loss  tensor(1.2250)
Iteration 315000: total loss 0.0909, losses: [0.04532657191157341, 0.045570384711027145], learning rate: 0.0097029900
Iteration 235000: total loss 0.0211, losses: [0.010215545073151588, 0.010861650109291077], learning rate: 0.0096059601
Iteration 320000: total loss 0.0582, losses: [0.028878534212708473, 0.029370782896876335], learning rate: 0.0097029900
Val loss  tensor(1.2130)
Val loss  tensor(1.2461)
Iteration 240000: total loss 0.0435, losses: [0.021939482539892197, 0.021568384021520615], learning rate: 0.0096059601
Val loss  tensor(1.1137)
Val loss  tensor(1.2464)
Iteration 325000: total loss 0.0624, losses: [0.03100460208952427, 0.03137914091348648], learning rate: 0.0097029900
Iteration 330000: total loss 0.0931, losses: [0.04632336646318436, 0.04681336134672165], learning rate: 0.0097029900
Val loss  tensor(0.9339)
Val loss  tensor(1.2149)
Iteration 245000: total loss 0.0294, losses: [0.014359665103256702, 0.015043267980217934], learning rate: 0.0096059601
Iteration 335000: total loss 0.0593, losses: [0.029442045837640762, 0.029833389446139336], learning rate: 0.0097029900
Iteration 250000: total loss 0.0317, losses: [0.015672706067562103, 0.01603236049413681], learning rate: 0.0096059601
Val loss  tensor(1.2349)
Val loss  tensor(1.2326)
Iteration 340000: total loss 0.0742, losses: [0.0371674969792366, 0.0370633490383625], learning rate: 0.0097029900
Val loss  tensor(1.1680)
Val loss  tensor(1.2155)
Iteration 345000: total loss 0.0926, losses: [0.0465078167617321, 0.04611200839281082], learning rate: 0.0097029900
Iteration 255000: total loss 0.0351, losses: [0.017165925353765488, 0.017904039472341537], learning rate: 0.0096059601
Iteration 350000: total loss 0.0504, losses: [0.025431588292121887, 0.02493094839155674], learning rate: 0.0097029900
Val loss  tensor(1.1462)
Val loss  tensor(1.2199)
Iteration 260000: total loss 0.0372, losses: [0.01845630630850792, 0.018762890249490738], learning rate: 0.0096059601
Val loss  tensor(1.1681)
Val loss  tensor(1.1937)
Iteration 355000: total loss 0.0741, losses: [0.036452945321798325, 0.03766154497861862], learning rate: 0.0096059601
Iteration 360000: total loss 0.0754, losses: [0.037748780101537704, 0.03767499327659607], learning rate: 0.0096059601
Val loss  tensor(1.0387)
Val loss  tensor(1.2303)
Iteration 265000: total loss 0.0339, losses: [0.01658536307513714, 0.017345862463116646], learning rate: 0.0096059601
Iteration 365000: total loss 0.0685, losses: [0.034521158784627914, 0.03401091694831848], learning rate: 0.0096059601
Iteration 270000: total loss 0.0271, losses: [0.013491409830749035, 0.013630655594170094], learning rate: 0.0096059601
Val loss  tensor(1.1292)
Val loss  tensor(1.2181)
Iteration 370000: total loss 0.0510, losses: [0.025440638884902, 0.025596613064408302], learning rate: 0.0096059601
Val loss  tensor(1.0721)
Val loss  tensor(1.2118)
Iteration 375000: total loss 0.0786, losses: [0.039352770894765854, 0.03928104788064957], learning rate: 0.0096059601
Iteration 275000: total loss 0.0360, losses: [0.017967570573091507, 0.018050460144877434], learning rate: 0.0096059601
Iteration 380000: total loss 0.0781, losses: [0.03919743373990059, 0.03892011567950249], learning rate: 0.0096059601
Val loss  tensor(1.1580)
Val loss  tensor(1.2207)
Iteration 280000: total loss 0.0319, losses: [0.01518889982253313, 0.016687117516994476], learning rate: 0.0096059601
Val loss  tensor(0.9776)
Val loss  tensor(1.2169)
Iteration 385000: total loss 0.0696, losses: [0.03474433720111847, 0.034874673932790756], learning rate: 0.0096059601
Iteration 390000: total loss 0.0683, losses: [0.033974409103393555, 0.034315310418605804], learning rate: 0.0096059601
Val loss  tensor(0.6112)
Iteration 285000: total loss 0.0273, losses: [0.01299124676734209, 0.014339596033096313], learning rate: 0.0096059601
Val loss  tensor(1.1877)
Iteration 395000: total loss 0.0667, losses: [0.03325078636407852, 0.033438146114349365], learning rate: 0.0096059601
Iteration 290000: total loss 0.0313, losses: [0.015516367740929127, 0.01582571677863598], learning rate: 0.0096059601
Val loss  tensor(1.1991)
Val loss  tensor(1.2170)
Iteration 400000: total loss 0.0891, losses: [0.04432898387312889, 0.044740524142980576], learning rate: 0.0096059601
Val loss  tensor(0.8940)
Val loss  tensor(1.2251)
Iteration 295000: total loss 0.0311, losses: [0.01558022852987051, 0.015549344010651112], learning rate: 0.0096059601
Iteration 405000: total loss 0.0441, losses: [0.02171231620013714, 0.022403409704566002], learning rate: 0.0096059601
Iteration 410000: total loss 0.0771, losses: [0.03873108699917793, 0.03833829239010811], learning rate: 0.0096059601
Val loss  tensor(1.2070)
Val loss  tensor(1.2358)
Iteration 300000: total loss 0.0240, losses: [0.011520572006702423, 0.012432470917701721], learning rate: 0.0096059601
Val loss  tensor(1.4065)
Val loss  tensor(1.4456)
Iteration 415000: total loss 0.0823, losses: [0.04103005677461624, 0.041223324835300446], learning rate: 0.0096059601
Iteration 420000: total loss 0.0657, losses: [0.03242959827184677, 0.03329562768340111], learning rate: 0.0096059601
Iteration 305000: total loss 0.0274, losses: [0.013496831059455872, 0.013868219219148159], learning rate: 0.0096059601
Val loss  tensor(1.2183)
Val loss  tensor(1.2296)
Iteration 425000: total loss 0.0785, losses: [0.03918107971549034, 0.039336975663900375], learning rate: 0.0096059601
Iteration 310000: total loss 0.0257, losses: [0.01231800951063633, 0.013353998772799969], learning rate: 0.0096059601
Val loss  tensor(1.2011)
Val loss  tensor(1.2203)
Iteration 430000: total loss 0.0827, losses: [0.040987975895404816, 0.04167085886001587], learning rate: 0.0096059601
Val loss  tensor(1.2334)
Val loss  tensor(1.2437)
Iteration 315000: total loss 0.0289, losses: [0.014189657755196095, 0.014749342575669289], learning rate: 0.0096059601
Iteration 435000: total loss 0.0703, losses: [0.035234130918979645, 0.03510678932070732], learning rate: 0.0096059601
Iteration 440000: total loss 0.0670, losses: [0.03347044065594673, 0.03356991335749626], learning rate: 0.0096059601
Val loss  tensor(1.1761)
Val loss  tensor(1.2303)
Iteration 320000: total loss 0.0399, losses: [0.019679777324199677, 0.020252475515007973], learning rate: 0.0096059601
Val loss  tensor(1.2811)
Val loss  tensor(1.2215)
Iteration 445000: total loss 0.0816, losses: [0.040982503443956375, 0.04058944806456566], learning rate: 0.0095099005
Iteration 325000: total loss 0.0380, losses: [0.019485661759972572, 0.018519287928938866], learning rate: 0.0096059601
Iteration 450000: total loss 0.0610, losses: [0.030268287286162376, 0.0307037141174078], learning rate: 0.0095099005
Val loss  tensor(1.0071)
Val loss  tensor(1.1843)
Iteration 455000: total loss 0.0682, losses: [0.03438242897391319, 0.03378110006451607], learning rate: 0.0095099005
Iteration 330000: total loss 0.0306, losses: [0.014975464902818203, 0.01564323529601097], learning rate: 0.0096059601
Val loss  tensor(1.2243)
Val loss  tensor(1.2290)
Iteration 460000: total loss 0.0710, losses: [0.03543242812156677, 0.035540785640478134], learning rate: 0.0095099005
Val loss  tensor(0.6960)
Val loss  tensor(1.2146)
Iteration 335000: total loss 0.0309, losses: [0.01501991506665945, 0.01588018797338009], learning rate: 0.0095099005
Iteration 465000: total loss 0.0660, losses: [0.03295751288533211, 0.033060308545827866], learning rate: 0.0095099005
Iteration 470000: total loss 0.0699, losses: [0.035215966403484344, 0.034700941294431686], learning rate: 0.0095099005
Val loss  tensor(0.8700)
Val loss  tensor(1.2232)
Iteration 340000: total loss 0.0308, losses: [0.015357588417828083, 0.015410690568387508], learning rate: 0.0095099005
Val loss  tensor(1.1169)
Val loss  tensor(1.2018)
Iteration 475000: total loss 0.0703, losses: [0.0349370501935482, 0.035385049879550934], learning rate: 0.0095099005
Iteration 345000: total loss 0.0355, losses: [0.017648011445999146, 0.017889713868498802], learning rate: 0.0095099005
Iteration 480000: total loss 0.0781, losses: [0.03870806097984314, 0.03940659388899803], learning rate: 0.0095099005
Val loss  tensor(0.9353)
Val loss  tensor(1.2176)
Iteration 485000: total loss 0.0800, losses: [0.040169958025217056, 0.03980128839612007], learning rate: 0.0095099005
Iteration 350000: total loss 0.0264, losses: [0.012915394268929958, 0.013446961529552937], learning rate: 0.0095099005
Val loss  tensor(1.2629)
Val loss  tensor(1.2800)
Iteration 490000: total loss 0.0683, losses: [0.033790480345487595, 0.03449168801307678], learning rate: 0.0095099005
Val loss  tensor(1.2380)
Val loss  tensor(1.2417)
Iteration 355000: total loss 0.0209, losses: [0.01020618062466383, 0.01072615385055542], learning rate: 0.0095099005
Iteration 495000: total loss 0.0786, losses: [0.03911914676427841, 0.03944781422615051], learning rate: 0.0095099005
Iteration 360000: total loss 0.0345, losses: [0.01702437363564968, 0.017478544265031815], learning rate: 0.0095099005
Val loss  tensor(1.1360)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): SiLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): Tanh()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Val loss  tensor(1.2277)
Iteration 365000: total loss 0.0277, losses: [0.013800161890685558, 0.013873395510017872], learning rate: 0.0095099005
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5520, losses: [1.2866524457931519, 1.2653130292892456], learning rate: 0.0100000000
Val loss  tensor(1.2614)
Val loss  tensor(1.1628)
Iteration 370000: total loss 0.0406, losses: [0.019643032923340797, 0.020930534228682518], learning rate: 0.0095099005
Val loss  tensor(1.1753)
Val loss  tensor(1.2041)
Iteration 5000: total loss 0.1240, losses: [0.06087540462613106, 0.06309410184621811], learning rate: 0.0100000000
Iteration 10000: total loss 0.1318, losses: [0.06492061913013458, 0.06690933555364609], learning rate: 0.0100000000
Val loss  tensor(1.0551)
Val loss  tensor(1.1224)
Iteration 375000: total loss 0.0336, losses: [0.016644645482301712, 0.01698957197368145], learning rate: 0.0095099005
Iteration 15000: total loss 0.0822, losses: [0.04022374376654625, 0.04199739173054695], learning rate: 0.0100000000
Iteration 380000: total loss 0.0289, losses: [0.014461280778050423, 0.014404750429093838], learning rate: 0.0095099005
Val loss  tensor(1.1409)
Val loss  tensor(1.2163)
Iteration 20000: total loss 0.0727, losses: [0.03622952103614807, 0.0364302434027195], learning rate: 0.0100000000
Val loss  tensor(0.8768)
Val loss  tensor(1.1165)
Iteration 25000: total loss 0.0760, losses: [0.037388723343610764, 0.0385650172829628], learning rate: 0.0100000000
Iteration 385000: total loss 0.0252, losses: [0.012598144821822643, 0.012586873956024647], learning rate: 0.0095099005
Iteration 30000: total loss 0.3376, losses: [0.16695836186408997, 0.170685276389122], learning rate: 0.0100000000
Val loss  tensor(1.0691)
Val loss  tensor(1.2290)
Iteration 390000: total loss 0.0316, losses: [0.015507245436310768, 0.01608954183757305], learning rate: 0.0095099005
Val loss  tensor(1.1998)
Val loss  tensor(1.2355)
Iteration 35000: total loss 0.1335, losses: [0.06659203767776489, 0.06685839593410492], learning rate: 0.0100000000
Iteration 40000: total loss 0.1549, losses: [0.0774676725268364, 0.07745999842882156], learning rate: 0.0100000000
Val loss  tensor(0.9821)
Val loss  tensor(1.1045)
Iteration 395000: total loss 0.0213, losses: [0.010515093803405762, 0.010788778774440289], learning rate: 0.0094148015
Iteration 45000: total loss 0.1744, losses: [0.08711803704500198, 0.08732930570840836], learning rate: 0.0100000000
Iteration 400000: total loss 0.0250, losses: [0.01221710629761219, 0.012809696607291698], learning rate: 0.0094148015
Val loss  tensor(1.3931)
Val loss  tensor(1.6179)
Iteration 50000: total loss 0.1809, losses: [0.08972854912281036, 0.09119420498609543], learning rate: 0.0100000000
Val loss  tensor(0.8260)
Val loss  tensor(1.0343)
Iteration 55000: total loss 0.1692, losses: [0.08449011296033859, 0.08466003835201263], learning rate: 0.0100000000
Iteration 405000: total loss 0.0235, losses: [0.01129436306655407, 0.01222566980868578], learning rate: 0.0094148015
Iteration 60000: total loss 0.1721, losses: [0.08596773445606232, 0.08612234890460968], learning rate: 0.0100000000
Val loss  tensor(1.2529)
Val loss  tensor(1.2713)
Iteration 410000: total loss 0.0219, losses: [0.010600163601338863, 0.011303559876978397], learning rate: 0.0094148015
Val loss  tensor(1.1973)
Val loss  tensor(1.2238)
Iteration 65000: total loss 0.1668, losses: [0.08370407670736313, 0.08314266800880432], learning rate: 0.0100000000
Iteration 70000: total loss 0.1964, losses: [0.097661592066288, 0.09870628267526627], learning rate: 0.0100000000
Val loss  tensor(1.0588)
Iteration 415000: total loss 0.0289, losses: [0.014187646098434925, 0.0146945184096694], learning rate: 0.0094148015
Val loss  tensor(1.1531)
Iteration 75000: total loss 0.1863, losses: [0.09290673583745956, 0.09342417865991592], learning rate: 0.0100000000
Iteration 420000: total loss 0.0320, losses: [0.015522658824920654, 0.016462668776512146], learning rate: 0.0094148015
Val loss  tensor(1.2072)
Val loss  tensor(1.2201)
Iteration 80000: total loss 0.1661, losses: [0.08303766697645187, 0.08303386718034744], learning rate: 0.0100000000
Val loss  tensor(1.2813)
Val loss  tensor(1.3822)
Iteration 425000: total loss 0.0407, losses: [0.020380085334181786, 0.020288562402129173], learning rate: 0.0094148015
Iteration 85000: total loss 0.1700, losses: [0.0847877785563469, 0.08516968786716461], learning rate: 0.0100000000
Iteration 90000: total loss 0.2004, losses: [0.0998687669634819, 0.10049831122159958], learning rate: 0.0100000000
Val loss  tensor(1.2209)
Val loss  tensor(1.2012)
Iteration 430000: total loss 0.0219, losses: [0.011125016026198864, 0.01076497882604599], learning rate: 0.0094148015
Val loss  tensor(1.2454)
Val loss  tensor(1.7930)
Iteration 95000: total loss 0.1501, losses: [0.07513364404439926, 0.07497008889913559], learning rate: 0.0100000000
Iteration 435000: total loss 0.0375, losses: [0.018437910825014114, 0.019021200016140938], learning rate: 0.0094148015
Iteration 100000: total loss 0.1892, losses: [0.09425316005945206, 0.0948999673128128], learning rate: 0.0100000000
Val loss  tensor(1.3166)
Val loss  tensor(1.3805)
Iteration 105000: total loss 0.1650, losses: [0.082427479326725, 0.08259189128875732], learning rate: 0.0099000000
Iteration 440000: total loss 0.0235, losses: [0.01186400093138218, 0.011669471859931946], learning rate: 0.0094148015
Val loss  tensor(1.1514)
Val loss  tensor(1.4349)
Iteration 110000: total loss 0.1371, losses: [0.06837731599807739, 0.06873009353876114], learning rate: 0.0099000000
Val loss  tensor(1.0197)
Val loss  tensor(1.1950)
Iteration 445000: total loss 0.0360, losses: [0.017670851200819016, 0.018285050988197327], learning rate: 0.0093206535
Iteration 115000: total loss 0.1862, losses: [0.09319913387298584, 0.09304221719503403], learning rate: 0.0099000000
Iteration 120000: total loss 0.2069, losses: [0.1030261293053627, 0.10386641323566437], learning rate: 0.0099000000
Val loss  tensor(1.1884)
Val loss  tensor(1.2779)
Iteration 450000: total loss 0.0317, losses: [0.015275368466973305, 0.01639694534242153], learning rate: 0.0093206535
Val loss  tensor(1.4074)
Val loss  tensor(1.2398)
Iteration 125000: total loss 0.1703, losses: [0.08491691201925278, 0.08537875860929489], learning rate: 0.0099000000
Iteration 455000: total loss 0.0225, losses: [0.011044708080589771, 0.01145942322909832], learning rate: 0.0093206535
Iteration 130000: total loss 0.1687, losses: [0.08401843160390854, 0.08466779440641403], learning rate: 0.0099000000
Val loss  tensor(1.0206)
Val loss  tensor(1.0899)
Iteration 135000: total loss 0.1508, losses: [0.07508020848035812, 0.07576817274093628], learning rate: 0.0099000000
Iteration 460000: total loss 0.0396, losses: [0.019115569069981575, 0.020480040460824966], learning rate: 0.0093206535
Val loss  tensor(1.2190)
Val loss  tensor(1.4792)
Iteration 140000: total loss 0.2155, losses: [0.10706229507923126, 0.10846979916095734], learning rate: 0.0099000000
Val loss  tensor(0.8490)
Val loss  tensor(1.1908)
Iteration 465000: total loss 0.0273, losses: [0.012973886914551258, 0.014283408410847187], learning rate: 0.0093206535
Iteration 145000: total loss 0.2578, losses: [0.1289866417646408, 0.1288178712129593], learning rate: 0.0099000000
Iteration 150000: total loss 0.1602, losses: [0.07985371351242065, 0.08032285422086716], learning rate: 0.0099000000
Val loss  tensor(0.8399)
Val loss  tensor(1.2537)
Iteration 470000: total loss 0.0330, losses: [0.015947164967656136, 0.01710090972483158], learning rate: 0.0093206535
Val loss  tensor(1.2255)
Val loss  tensor(1.2378)
Iteration 155000: total loss 0.1960, losses: [0.09804200381040573, 0.09796904772520065], learning rate: 0.0099000000
Iteration 475000: total loss 0.0242, losses: [0.011351858265697956, 0.012894879095256329], learning rate: 0.0093206535
Iteration 160000: total loss 0.2018, losses: [0.10055728256702423, 0.10127055644989014], learning rate: 0.0099000000
Val loss  tensor(1.1670)
Val loss  tensor(1.2577)
Iteration 165000: total loss 0.1670, losses: [0.08285444229841232, 0.08411756157875061], learning rate: 0.0098010000
Iteration 480000: total loss 0.0283, losses: [0.013968109153211117, 0.01432668138295412], learning rate: 0.0093206535
Val loss  tensor(1.2256)
Val loss  tensor(1.2261)
Iteration 170000: total loss 0.1281, losses: [0.06408267468214035, 0.06406670808792114], learning rate: 0.0098010000
Val loss  tensor(0.8041)
Val loss  tensor(1.2090)
Iteration 485000: total loss 0.0244, losses: [0.011290766298770905, 0.01315563078969717], learning rate: 0.0093206535
Iteration 175000: total loss 0.1459, losses: [0.07321581989526749, 0.07267053425312042], learning rate: 0.0098010000
Iteration 180000: total loss 0.1750, losses: [0.08768418431282043, 0.08728282898664474], learning rate: 0.0098010000
Val loss  tensor(0.8422)
Iteration 490000: total loss 0.0332, losses: [0.016108950600028038, 0.017113033682107925], learning rate: 0.0093206535
Val loss  tensor(1.2181)
Val loss  tensor(1.1138)
Val loss  tensor(1.3606)
Iteration 185000: total loss 0.2104, losses: [0.10519502311944962, 0.1051785945892334], learning rate: 0.0098010000
Iteration 495000: total loss 0.0396, losses: [0.019826287403702736, 0.019820561632514], learning rate: 0.0093206535
Iteration 190000: total loss 0.1892, losses: [0.09433459490537643, 0.09489449858665466], learning rate: 0.0098010000
Val loss  tensor(0.8585)
Val loss  tensor(1.1730)
Iteration 195000: total loss 0.1962, losses: [0.09796897321939468, 0.09824205935001373], learning rate: 0.0098010000
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=256, bias=True)
    (7): ReLU()
    (8): Linear(in_features=256, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 200000: total loss 0.1422, losses: [0.07064439356327057, 0.07150749117136002], learning rate: 0.0098010000
Val loss  tensor(0.7021)
Val loss  tensor(1.1092)
Iteration 205000: total loss 0.2349, losses: [0.11775829643011093, 0.11715608090162277], learning rate: 0.0098010000
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5313, losses: [1.2747642993927002, 1.256524920463562], learning rate: 0.0100000000
Val loss  tensor(1.2527)
Val loss  tensor(1.1928)
Iteration 210000: total loss 0.1702, losses: [0.0851319208741188, 0.08507601171731949], learning rate: 0.0098010000
Val loss  tensor(1.0411)
Val loss  tensor(1.2693)
Iteration 5000: total loss 0.1529, losses: [0.07406573742628098, 0.07882440090179443], learning rate: 0.0100000000
Iteration 215000: total loss 0.1778, losses: [0.088710255920887, 0.08910860121250153], learning rate: 0.0098010000
Iteration 10000: total loss 0.0925, losses: [0.04524075612425804, 0.04729905724525452], learning rate: 0.0100000000
Val loss  tensor(1.0237)
Val loss  tensor(1.1506)
Iteration 220000: total loss 0.1826, losses: [0.09184607118368149, 0.09080389142036438], learning rate: 0.0098010000
Val loss  tensor(0.6152)
Val loss  tensor(1.0753)
Iteration 15000: total loss 0.1090, losses: [0.05431722477078438, 0.054709043353796005], learning rate: 0.0100000000
Iteration 225000: total loss 0.1920, losses: [0.09530096501111984, 0.09671839326620102], learning rate: 0.0098010000
Iteration 20000: total loss 0.0914, losses: [0.04508325085043907, 0.04632138833403587], learning rate: 0.0100000000
Val loss  tensor(0.9112)
Val loss  tensor(1.1076)
Iteration 230000: total loss 0.1130, losses: [0.0564693920314312, 0.05652322992682457], learning rate: 0.0098010000
Val loss  tensor(1.2353)
Val loss  tensor(1.3027)
Iteration 25000: total loss 0.0523, losses: [0.025917483493685722, 0.02641146443784237], learning rate: 0.0100000000
Iteration 235000: total loss 0.1798, losses: [0.08975639194250107, 0.08999644964933395], learning rate: 0.0098010000
Iteration 30000: total loss 0.0362, losses: [0.018106846138834953, 0.01812000386416912], learning rate: 0.0100000000
Val loss  tensor(0.9113)
Val loss  tensor(1.1058)
Iteration 240000: total loss 0.1514, losses: [0.07579334825277328, 0.07560878247022629], learning rate: 0.0098010000
Val loss  tensor(0.8150)
Val loss  tensor(1.1666)
Iteration 35000: total loss 0.0558, losses: [0.027730150148272514, 0.028026679530739784], learning rate: 0.0100000000
Iteration 245000: total loss 0.1734, losses: [0.08625207096338272, 0.0871487408876419], learning rate: 0.0098010000
Iteration 40000: total loss 0.0540, losses: [0.026324881240725517, 0.02771363966166973], learning rate: 0.0100000000
Val loss  tensor(1.0720)
Val loss  tensor(1.1356)
Iteration 250000: total loss 0.1824, losses: [0.0907471776008606, 0.09162058681249619], learning rate: 0.0098010000
Val loss  tensor(0.7042)
Val loss  tensor(1.2019)
Iteration 45000: total loss 0.0656, losses: [0.03229386731982231, 0.03329331427812576], learning rate: 0.0100000000
Iteration 255000: total loss 0.2069, losses: [0.10370726883411407, 0.10321678966283798], learning rate: 0.0098010000
Iteration 50000: total loss 0.0423, losses: [0.020878495648503304, 0.021452678367495537], learning rate: 0.0100000000
Val loss  tensor(0.8878)
Val loss  tensor(1.0926)
Iteration 260000: total loss 0.1887, losses: [0.09359484910964966, 0.09511669725179672], learning rate: 0.0098010000
Val loss  tensor(0.9173)
Val loss  tensor(1.2080)
Iteration 55000: total loss 0.0834, losses: [0.041309576481580734, 0.04206141456961632], learning rate: 0.0100000000
Iteration 265000: total loss 0.1344, losses: [0.06663232296705246, 0.0677189826965332], learning rate: 0.0098010000
Iteration 60000: total loss 0.0500, losses: [0.02435755729675293, 0.025593137368559837], learning rate: 0.0100000000
Val loss  tensor(0.9674)
Val loss  tensor(1.1451)
Iteration 270000: total loss 0.2109, losses: [0.10538341850042343, 0.10550475120544434], learning rate: 0.0098010000
Val loss  tensor(1.1691)
Val loss  tensor(1.2257)
Iteration 65000: total loss 0.0332, losses: [0.016398726031184196, 0.01679774932563305], learning rate: 0.0100000000
Iteration 275000: total loss 0.1755, losses: [0.0875658318400383, 0.08790155500173569], learning rate: 0.0097029900
Iteration 70000: total loss 0.0446, losses: [0.02226327173411846, 0.02231515757739544], learning rate: 0.0100000000
Val loss  tensor(1.0928)
Val loss  tensor(1.2332)
Iteration 280000: total loss 0.1604, losses: [0.07989205420017242, 0.08047715574502945], learning rate: 0.0097029900
Val loss  tensor(0.7805)
Val loss  tensor(1.1348)
Iteration 75000: total loss 0.0456, losses: [0.022356931120157242, 0.023206789046525955], learning rate: 0.0100000000
Iteration 285000: total loss 0.1977, losses: [0.09879529476165771, 0.09894513338804245], learning rate: 0.0097029900
Iteration 80000: total loss 0.0548, losses: [0.027405789121985435, 0.0273689366877079], learning rate: 0.0100000000
Val loss  tensor(1.2007)
Val loss  tensor(1.2773)
Iteration 290000: total loss 0.1781, losses: [0.08868779987096786, 0.08940010517835617], learning rate: 0.0097029900
Val loss  tensor(1.1022)
Val loss  tensor(1.1841)
Iteration 85000: total loss 0.0599, losses: [0.03029794432222843, 0.029563898220658302], learning rate: 0.0100000000
Iteration 295000: total loss 0.1461, losses: [0.07244084030389786, 0.07368769496679306], learning rate: 0.0097029900
Iteration 90000: total loss 0.0410, losses: [0.020548447966575623, 0.020471004769206047], learning rate: 0.0100000000
Val loss  tensor(1.1266)
Val loss  tensor(1.1832)
Iteration 300000: total loss 0.1516, losses: [0.07547776401042938, 0.07608956098556519], learning rate: 0.0097029900
Val loss  tensor(1.1566)
Val loss  tensor(1.2559)
Iteration 95000: total loss 0.0463, losses: [0.0227858517318964, 0.023485302925109863], learning rate: 0.0100000000
Iteration 305000: total loss 0.2024, losses: [0.10140581429004669, 0.10103674232959747], learning rate: 0.0097029900
Iteration 100000: total loss 0.0553, losses: [0.027367359027266502, 0.027883848175406456], learning rate: 0.0100000000
Val loss  tensor(1.1560)
Val loss  tensor(1.2054)
Iteration 310000: total loss 0.1989, losses: [0.09952356666326523, 0.099420927464962], learning rate: 0.0097029900
Val loss  tensor(1.1428)
Val loss  tensor(1.2524)
Iteration 105000: total loss 0.0370, losses: [0.01832241751253605, 0.01864553987979889], learning rate: 0.0099000000
Iteration 315000: total loss 0.1192, losses: [0.0596926212310791, 0.05947863683104515], learning rate: 0.0097029900
Iteration 110000: total loss 0.0366, losses: [0.01817169226706028, 0.01844252087175846], learning rate: 0.0099000000
Val loss  tensor(1.0412)
Val loss  tensor(1.2286)
Iteration 320000: total loss 0.1979, losses: [0.09889533370733261, 0.09898363798856735], learning rate: 0.0097029900
Val loss  tensor(1.0902)
Val loss  tensor(1.2415)
Iteration 115000: total loss 0.0409, losses: [0.020221946761012077, 0.020710229873657227], learning rate: 0.0099000000
Iteration 325000: total loss 0.2141, losses: [0.10642222315073013, 0.10764491558074951], learning rate: 0.0097029900
Iteration 120000: total loss 0.0324, losses: [0.016242580488324165, 0.01612207293510437], learning rate: 0.0099000000
Val loss  tensor(0.9985)
Val loss  tensor(1.2283)
Iteration 330000: total loss 0.1568, losses: [0.07873404771089554, 0.07806779444217682], learning rate: 0.0097029900
Val loss  tensor(1.1332)
Val loss  tensor(1.1988)
Iteration 125000: total loss 0.0303, losses: [0.014799192547798157, 0.015509918332099915], learning rate: 0.0099000000
Iteration 335000: total loss 0.1373, losses: [0.06988202780485153, 0.06746571511030197], learning rate: 0.0096059601
Iteration 130000: total loss 0.0373, losses: [0.018316682428121567, 0.0189852062612772], learning rate: 0.0099000000
Val loss  tensor(0.8375)
Val loss  tensor(1.2047)
Iteration 340000: total loss 0.1522, losses: [0.07557439804077148, 0.07664309442043304], learning rate: 0.0096059601
Val loss  tensor(0.7765)
Val loss  tensor(1.1079)
Iteration 135000: total loss 0.0418, losses: [0.020712044090032578, 0.02111278660595417], learning rate: 0.0099000000
Iteration 345000: total loss 0.0911, losses: [0.04533318802714348, 0.0458061657845974], learning rate: 0.0096059601
Iteration 140000: total loss 0.0331, losses: [0.016310468316078186, 0.016808055341243744], learning rate: 0.0099000000
Val loss  tensor(1.0014)
Val loss  tensor(1.1973)
Iteration 350000: total loss 0.1852, losses: [0.09247733652591705, 0.09274637699127197], learning rate: 0.0096059601
Val loss  tensor(0.8977)
Val loss  tensor(1.1530)
Iteration 145000: total loss 0.0303, losses: [0.01491019781678915, 0.015376100316643715], learning rate: 0.0099000000
Iteration 355000: total loss 0.1602, losses: [0.08000332117080688, 0.08015520870685577], learning rate: 0.0096059601
Iteration 150000: total loss 0.0303, losses: [0.015096220187842846, 0.01521170325577259], learning rate: 0.0099000000
Val loss  tensor(1.0538)
Val loss  tensor(1.2137)
Iteration 360000: total loss 0.1522, losses: [0.07564348727464676, 0.07651765644550323], learning rate: 0.0096059601
Val loss  tensor(1.1011)
Val loss  tensor(1.2533)
Iteration 155000: total loss 0.0634, losses: [0.030904704704880714, 0.03247292712330818], learning rate: 0.0099000000
Iteration 365000: total loss 0.1386, losses: [0.06963249295949936, 0.06900615990161896], learning rate: 0.0096059601
Iteration 160000: total loss 0.0279, losses: [0.013512206263840199, 0.014437081292271614], learning rate: 0.0099000000
Val loss  tensor(1.0777)
Val loss  tensor(1.1958)
Iteration 370000: total loss 0.1370, losses: [0.06858458369970322, 0.06839017570018768], learning rate: 0.0096059601
Val loss  tensor(0.7812)
Val loss  tensor(1.1595)
Iteration 165000: total loss 0.0303, losses: [0.014935138635337353, 0.015379488468170166], learning rate: 0.0099000000
Iteration 375000: total loss 0.1583, losses: [0.07891819626092911, 0.07937677204608917], learning rate: 0.0096059601
Iteration 170000: total loss 0.0375, losses: [0.01834731176495552, 0.019139930605888367], learning rate: 0.0099000000
Val loss  tensor(1.2297)
Val loss  tensor(1.2774)
Iteration 380000: total loss 0.1724, losses: [0.0859554335474968, 0.08649333566427231], learning rate: 0.0096059601
Val loss  tensor(1.0736)
Val loss  tensor(1.2075)
Iteration 175000: total loss 0.0258, losses: [0.012601259164512157, 0.013210137374699116], learning rate: 0.0099000000
Iteration 385000: total loss 0.1959, losses: [0.09764224290847778, 0.09828585386276245], learning rate: 0.0095099005
Iteration 180000: total loss 0.0493, losses: [0.024402229115366936, 0.024924403056502342], learning rate: 0.0099000000
Val loss  tensor(1.1759)
Val loss  tensor(1.2185)
Iteration 390000: total loss 0.1660, losses: [0.08341913670301437, 0.08255165070295334], learning rate: 0.0095099005
Val loss  tensor(0.6972)
Val loss  tensor(1.1127)
Iteration 185000: total loss 0.0433, losses: [0.0211733877658844, 0.022078903391957283], learning rate: 0.0098010000
Iteration 395000: total loss 0.1799, losses: [0.0892346128821373, 0.0906975194811821], learning rate: 0.0095099005
Iteration 190000: total loss 0.0451, losses: [0.022529898211359978, 0.022604025900363922], learning rate: 0.0098010000
Val loss  tensor(0.8765)
Val loss  tensor(1.2507)
Iteration 400000: total loss 0.1892, losses: [0.09388556331396103, 0.09532129019498825], learning rate: 0.0095099005
Val loss  tensor(0.8150)
Val loss  tensor(1.1272)
Iteration 195000: total loss 0.0257, losses: [0.012821397744119167, 0.012926846742630005], learning rate: 0.0098010000
Iteration 405000: total loss 0.1508, losses: [0.07465240359306335, 0.07614725828170776], learning rate: 0.0095099005
Iteration 200000: total loss 0.0331, losses: [0.016425825655460358, 0.016664067283272743], learning rate: 0.0098010000
Val loss  tensor(1.0933)
Val loss  tensor(1.2284)
Iteration 410000: total loss 0.2059, losses: [0.10280592739582062, 0.10308774560689926], learning rate: 0.0095099005
Val loss  tensor(0.7876)
Val loss  tensor(1.1062)
Iteration 205000: total loss 0.0351, losses: [0.017280057072639465, 0.017791856080293655], learning rate: 0.0098010000
Iteration 415000: total loss 0.1695, losses: [0.08421148359775543, 0.08530043810606003], learning rate: 0.0095099005
Iteration 210000: total loss 0.0278, losses: [0.013583174906671047, 0.014205122366547585], learning rate: 0.0098010000
Val loss  tensor(0.9989)
Val loss  tensor(1.2300)
Iteration 420000: total loss 0.1230, losses: [0.06140708923339844, 0.06158144399523735], learning rate: 0.0095099005
Val loss  tensor(1.0330)
Val loss  tensor(1.1097)
Iteration 215000: total loss 0.0399, losses: [0.01978645659983158, 0.020132455974817276], learning rate: 0.0098010000
Iteration 425000: total loss 0.1854, losses: [0.09297456592321396, 0.09247054904699326], learning rate: 0.0095099005
Iteration 220000: total loss 0.0265, losses: [0.01311612781137228, 0.013402066193521023], learning rate: 0.0098010000
Val loss  tensor(0.8539)
Val loss  tensor(1.2258)
Iteration 430000: total loss 0.1310, losses: [0.06569141894578934, 0.06529831141233444], learning rate: 0.0095099005
Val loss  tensor(0.7602)
Val loss  tensor(1.1512)
Iteration 225000: total loss 0.0976, losses: [0.04811655357480049, 0.049434345215559006], learning rate: 0.0098010000
Iteration 435000: total loss 0.2152, losses: [0.10645301640033722, 0.10871846973896027], learning rate: 0.0095099005
Iteration 230000: total loss 0.1162, losses: [0.057406097650527954, 0.05882035568356514], learning rate: 0.0098010000
Val loss  tensor(1.2018)
Val loss  tensor(1.2411)
Iteration 440000: total loss 0.1961, losses: [0.09768402576446533, 0.0983695313334465], learning rate: 0.0095099005
Val loss  tensor(0.8445)
Val loss  tensor(1.1427)
Iteration 235000: total loss 0.0517, losses: [0.025870895013213158, 0.025838984176516533], learning rate: 0.0098010000
Iteration 445000: total loss 0.1930, losses: [0.09632573276758194, 0.09666511416435242], learning rate: 0.0094148015
Iteration 240000: total loss 0.0268, losses: [0.013491982594132423, 0.013262069784104824], learning rate: 0.0098010000
Val loss  tensor(1.1081)
Val loss  tensor(1.2427)
Iteration 450000: total loss 0.2215, losses: [0.11063804477453232, 0.1108926311135292], learning rate: 0.0094148015
Val loss  tensor(0.6991)
Val loss  tensor(0.9770)
Iteration 245000: total loss 0.0267, losses: [0.01315688993781805, 0.013586530461907387], learning rate: 0.0097029900
Iteration 455000: total loss 0.1647, losses: [0.08234359323978424, 0.0823151022195816], learning rate: 0.0094148015
Iteration 250000: total loss 0.0446, losses: [0.022023886442184448, 0.022554298862814903], learning rate: 0.0097029900
Val loss  tensor(1.1258)
Val loss  tensor(1.2256)
Iteration 460000: total loss 0.1632, losses: [0.08156850188970566, 0.08166736364364624], learning rate: 0.0094148015
Val loss  tensor(1.2230)
Val loss  tensor(1.3499)
Iteration 255000: total loss 0.0260, losses: [0.012645666487514973, 0.013398303650319576], learning rate: 0.0097029900
Iteration 465000: total loss 0.1975, losses: [0.098854199051857, 0.09860705584287643], learning rate: 0.0094148015
Iteration 260000: total loss 0.0462, losses: [0.02306216023862362, 0.023132851347327232], learning rate: 0.0097029900
Val loss  tensor(1.1656)
Val loss  tensor(1.2312)
Iteration 470000: total loss 0.1784, losses: [0.08890766650438309, 0.0894603282213211], learning rate: 0.0094148015
Val loss  tensor(0.7355)
Val loss  tensor(1.1344)
Iteration 265000: total loss 0.0480, losses: [0.02372947335243225, 0.024316245689988136], learning rate: 0.0097029900
Iteration 475000: total loss 0.1841, losses: [0.0925881490111351, 0.09147907048463821], learning rate: 0.0094148015
Iteration 270000: total loss 0.0316, losses: [0.015404465608298779, 0.016169818118214607], learning rate: 0.0097029900
Val loss  tensor(1.2114)
Val loss  tensor(1.2122)
Iteration 480000: total loss 0.1156, losses: [0.057806529104709625, 0.05775050073862076], learning rate: 0.0094148015
Val loss  tensor(0.6672)
Val loss  tensor(1.1099)
Iteration 275000: total loss 0.0270, losses: [0.012991736643016338, 0.014039789326488972], learning rate: 0.0097029900
Iteration 485000: total loss 0.1684, losses: [0.08354189991950989, 0.08485087007284164], learning rate: 0.0094148015
Iteration 280000: total loss 0.0276, losses: [0.013671464286744595, 0.013917334377765656], learning rate: 0.0097029900
Val loss  tensor(1.0062)
Val loss  tensor(1.1188)
Iteration 490000: total loss 0.1636, losses: [0.08186646550893784, 0.0817665085196495], learning rate: 0.0094148015
Val loss  tensor(0.7376)
Val loss  tensor(1.0834)
Iteration 285000: total loss 0.1460, losses: [0.07163796573877335, 0.07440074533224106], learning rate: 0.0097029900
Iteration 495000: total loss 0.1620, losses: [0.08035431057214737, 0.08162989467382431], learning rate: 0.0093206535
Iteration 290000: total loss 0.3804, losses: [0.19072790443897247, 0.18963883817195892], learning rate: 0.0097029900
Val loss  tensor(1.3546)
Val loss  tensor(1.5635)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 295000: total loss 0.0461, losses: [0.022797003388404846, 0.023328127339482307], learning rate: 0.0096059601
Iteration 300000: total loss 0.0491, losses: [0.024603238329291344, 0.0245328601449728], learning rate: 0.0096059601
Val loss  tensor(0.8084)
Val loss  tensor(1.2332)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.3890, losses: [1.196755051612854, 1.1922630071640015], learning rate: 0.0100000000
Val loss  tensor(1.3727)
Iteration 305000: total loss 0.0481, losses: [0.023963626474142075, 0.024109546095132828], learning rate: 0.0096059601
Val loss  tensor(1.2717)
Iteration 310000: total loss 0.0268, losses: [0.013191414065659046, 0.013595925644040108], learning rate: 0.0096059601
Val loss  tensor(1.0563)
Iteration 5000: total loss 0.1112, losses: [0.055387403815984726, 0.05585962533950806], learning rate: 0.0100000000
Val loss  tensor(1.1877)
Iteration 10000: total loss 0.0929, losses: [0.045963969081640244, 0.04697119817137718], learning rate: 0.0100000000
Val loss  tensor(0.9253)
Iteration 315000: total loss 0.0386, losses: [0.019298413768410683, 0.019330356270074844], learning rate: 0.0096059601
Val loss  tensor(1.1119)
Iteration 320000: total loss 0.0256, losses: [0.012778425589203835, 0.012776863761246204], learning rate: 0.0096059601
Val loss  tensor(0.9680)
Iteration 15000: total loss 0.0852, losses: [0.042076099663972855, 0.04317304119467735], learning rate: 0.0100000000
Val loss  tensor(1.1701)
Iteration 20000: total loss 0.1085, losses: [0.05515912175178528, 0.05331066995859146], learning rate: 0.0100000000
Val loss  tensor(1.1243)
Iteration 325000: total loss 0.0284, losses: [0.014098373241722584, 0.014296493493020535], learning rate: 0.0096059601
Val loss  tensor(1.2446)
Iteration 330000: total loss 0.0517, losses: [0.025648949667811394, 0.026003802195191383], learning rate: 0.0096059601
Val loss  tensor(1.0312)
Iteration 25000: total loss 0.0903, losses: [0.04407760128378868, 0.04623417183756828], learning rate: 0.0100000000
Val loss  tensor(1.2916)
Iteration 30000: total loss 0.1084, losses: [0.05447053909301758, 0.053904276341199875], learning rate: 0.0100000000
Val loss  tensor(0.9439)
Iteration 335000: total loss 0.0243, losses: [0.012097042985260487, 0.012164875864982605], learning rate: 0.0096059601
Val loss  tensor(1.1141)
Iteration 340000: total loss 0.0469, losses: [0.023394951596856117, 0.0234820693731308], learning rate: 0.0096059601
Val loss  tensor(1.1919)
Iteration 35000: total loss 0.1185, losses: [0.05870840698480606, 0.059761352837085724], learning rate: 0.0100000000
Val loss  tensor(1.2341)
Iteration 40000: total loss 0.1133, losses: [0.057646602392196655, 0.055699121206998825], learning rate: 0.0100000000
Val loss  tensor(0.9600)
Iteration 345000: total loss 0.0291, losses: [0.014446171931922436, 0.014660543762147427], learning rate: 0.0096059601
Val loss  tensor(1.5872)
Iteration 350000: total loss 0.0402, losses: [0.01970767416059971, 0.020473292097449303], learning rate: 0.0096059601
Val loss  tensor(1.0102)
Iteration 45000: total loss 0.0866, losses: [0.042741529643535614, 0.043863505125045776], learning rate: 0.0100000000
Val loss  tensor(1.2142)
Iteration 50000: total loss 0.0943, losses: [0.04702255129814148, 0.04726319760084152], learning rate: 0.0100000000
Val loss  tensor(1.1178)
Iteration 355000: total loss 0.0303, losses: [0.015005484223365784, 0.015342965722084045], learning rate: 0.0095099005
Val loss  tensor(1.1200)
Iteration 360000: total loss 0.0266, losses: [0.01321006752550602, 0.013421944342553616], learning rate: 0.0095099005
Val loss  tensor(1.1342)
Iteration 55000: total loss 0.1143, losses: [0.05681570991873741, 0.05747904255986214], learning rate: 0.0100000000
Val loss  tensor(1.1732)
Iteration 60000: total loss 0.1089, losses: [0.05497397854924202, 0.05392782762646675], learning rate: 0.0100000000
Val loss  tensor(1.1466)
Iteration 365000: total loss 0.0287, losses: [0.01413789577782154, 0.014605539850890636], learning rate: 0.0095099005
Val loss  tensor(1.2483)
Iteration 370000: total loss 0.0295, losses: [0.014796960167586803, 0.014739260077476501], learning rate: 0.0095099005
Val loss  tensor(1.1331)
Iteration 65000: total loss 0.0636, losses: [0.03212782368063927, 0.03149278089404106], learning rate: 0.0099000000
Val loss  tensor(1.1979)
Iteration 70000: total loss 0.0956, losses: [0.047231581062078476, 0.04840680956840515], learning rate: 0.0099000000
Val loss  tensor(1.1757)
Iteration 375000: total loss 0.0314, losses: [0.01551466342061758, 0.01585068367421627], learning rate: 0.0095099005
Val loss  tensor(1.2641)
Iteration 380000: total loss 0.0300, losses: [0.014715171419084072, 0.015284008346498013], learning rate: 0.0095099005
Val loss  tensor(0.9726)
Iteration 75000: total loss 0.0826, losses: [0.04069599509239197, 0.04194362834095955], learning rate: 0.0099000000
Val loss  tensor(1.1113)
Iteration 80000: total loss 0.0987, losses: [0.04905388876795769, 0.04966636002063751], learning rate: 0.0099000000
Val loss  tensor(0.8765)
Iteration 385000: total loss 0.0539, losses: [0.027032608166337013, 0.026823529973626137], learning rate: 0.0095099005
Val loss  tensor(1.1099)
Iteration 390000: total loss 0.0248, losses: [0.012329943478107452, 0.01245822198688984], learning rate: 0.0095099005
Val loss  tensor(0.8289)
Iteration 85000: total loss 0.1102, losses: [0.05413930490612984, 0.05605076625943184], learning rate: 0.0099000000
Val loss  tensor(1.2215)
Iteration 90000: total loss 0.0903, losses: [0.04538600146770477, 0.044943492859601974], learning rate: 0.0099000000
Val loss  tensor(0.7190)
Iteration 395000: total loss 0.0362, losses: [0.017900196835398674, 0.018252693116664886], learning rate: 0.0095099005
Val loss  tensor(1.0936)
Iteration 400000: total loss 0.0415, losses: [0.02061193622648716, 0.020932335406541824], learning rate: 0.0095099005
Val loss  tensor(1.1139)
Iteration 95000: total loss 0.1553, losses: [0.07816644757986069, 0.07715632021427155], learning rate: 0.0099000000
Val loss  tensor(1.1708)
Iteration 100000: total loss 0.0655, losses: [0.03219377622008324, 0.03333517536520958], learning rate: 0.0099000000
Val loss  tensor(0.9205)
Iteration 405000: total loss 0.0308, losses: [0.015428622253239155, 0.015352833084762096], learning rate: 0.0095099005
Val loss  tensor(1.2006)
Iteration 410000: total loss 0.0219, losses: [0.010875995270907879, 0.011009047739207745], learning rate: 0.0095099005
Val loss  tensor(0.9690)
Iteration 105000: total loss 0.0864, losses: [0.04247269779443741, 0.04388866946101189], learning rate: 0.0099000000
Val loss  tensor(1.2280)
Iteration 110000: total loss 0.0657, losses: [0.03252217173576355, 0.03315142169594765], learning rate: 0.0099000000
Val loss  tensor(0.8907)
Iteration 415000: total loss 0.0284, losses: [0.0140130165964365, 0.014398850500583649], learning rate: 0.0094148015
Val loss  tensor(1.1173)
Iteration 420000: total loss 0.0253, losses: [0.012532561086118221, 0.01275124866515398], learning rate: 0.0094148015
Val loss  tensor(0.9703)
Iteration 115000: total loss 0.0783, losses: [0.03892189636826515, 0.039406973868608475], learning rate: 0.0099000000
Val loss  tensor(1.1823)
Iteration 120000: total loss 0.1250, losses: [0.0620291493833065, 0.06297599524259567], learning rate: 0.0099000000
Val loss  tensor(0.8492)
Iteration 425000: total loss 0.0496, losses: [0.024680720642209053, 0.024952875450253487], learning rate: 0.0094148015
Val loss  tensor(1.1127)
Iteration 430000: total loss 0.0260, losses: [0.012716549448668957, 0.013287561945617199], learning rate: 0.0094148015
Val loss  tensor(1.0741)
Iteration 125000: total loss 0.0825, losses: [0.040984343737363815, 0.041509952396154404], learning rate: 0.0099000000
Val loss  tensor(1.1833)
Iteration 130000: total loss 0.0795, losses: [0.03992319479584694, 0.039563070982694626], learning rate: 0.0099000000
Val loss  tensor(0.7664)
Iteration 435000: total loss 0.0295, losses: [0.014429229311645031, 0.015061169862747192], learning rate: 0.0094148015
Val loss  tensor(1.1386)
Iteration 440000: total loss 0.0284, losses: [0.014326408505439758, 0.014062217436730862], learning rate: 0.0094148015
Val loss  tensor(0.9589)
Iteration 135000: total loss 0.0632, losses: [0.0318744033575058, 0.03130941092967987], learning rate: 0.0099000000
Val loss  tensor(1.1890)
Iteration 140000: total loss 0.0561, losses: [0.027585281059145927, 0.02848203293979168], learning rate: 0.0099000000
Val loss  tensor(0.8625)
Iteration 445000: total loss 0.0316, losses: [0.015616590157151222, 0.01603107340633869], learning rate: 0.0094148015
Val loss  tensor(1.2172)
Iteration 450000: total loss 0.0222, losses: [0.01100986823439598, 0.011169684119522572], learning rate: 0.0094148015
Val loss  tensor(0.9853)
Iteration 145000: total loss 0.0637, losses: [0.03212038427591324, 0.031580716371536255], learning rate: 0.0098010000
Val loss  tensor(1.1782)
Iteration 150000: total loss 0.1169, losses: [0.059728849679231644, 0.05721638724207878], learning rate: 0.0098010000
Iteration 455000: total loss 0.0313, losses: [0.015271583572030067, 0.015987396240234375], learning rate: 0.0094148015
Val loss  tensor(1.0377)
Val loss  tensor(1.2511)
Iteration 460000: total loss 0.0330, losses: [0.0163866113871336, 0.016645070165395737], learning rate: 0.0094148015
Val loss  tensor(1.1585)
Iteration 155000: total loss 0.0508, losses: [0.025535842403769493, 0.025258157402276993], learning rate: 0.0098010000
Val loss  tensor(1.1975)
Iteration 160000: total loss 0.0645, losses: [0.03173307329416275, 0.032798875123262405], learning rate: 0.0098010000
Iteration 465000: total loss 0.0281, losses: [0.013844531960785389, 0.01423249114304781], learning rate: 0.0093206535
Val loss  tensor(0.9897)
Val loss  tensor(1.1345)
Iteration 470000: total loss 0.0256, losses: [0.012452262453734875, 0.013191798701882362], learning rate: 0.0093206535
Val loss  tensor(0.6541)
Val loss  tensor(1.1492)
Iteration 165000: total loss 0.0708, losses: [0.0354919396340847, 0.035291220992803574], learning rate: 0.0098010000
Iteration 475000: total loss 0.0360, losses: [0.017948834225535393, 0.01806558482348919], learning rate: 0.0093206535
Iteration 170000: total loss 0.0596, losses: [0.029716862365603447, 0.02984837256371975], learning rate: 0.0098010000
Val loss  tensor(0.6895)
Val loss  tensor(1.1505)
Iteration 480000: total loss 0.0309, losses: [0.01552542019635439, 0.015395593829452991], learning rate: 0.0093206535
Val loss  tensor(0.9149)
Val loss  tensor(1.1507)
Iteration 175000: total loss 0.0674, losses: [0.03342648968100548, 0.03401687368750572], learning rate: 0.0098010000
Iteration 485000: total loss 0.0242, losses: [0.011742617003619671, 0.012503541074693203], learning rate: 0.0093206535
Iteration 180000: total loss 0.1312, losses: [0.06474276632070541, 0.06642693281173706], learning rate: 0.0098010000
Val loss  tensor(1.0023)
Val loss  tensor(1.1953)
Iteration 490000: total loss 0.0263, losses: [0.012916033156216145, 0.013348576612770557], learning rate: 0.0093206535
Val loss  tensor(0.9779)
Val loss  tensor(1.1841)
Iteration 185000: total loss 0.0739, losses: [0.03675100579857826, 0.037107523530721664], learning rate: 0.0098010000
Iteration 495000: total loss 0.0424, losses: [0.021039798855781555, 0.02135252021253109], learning rate: 0.0093206535
Iteration 190000: total loss 0.0683, losses: [0.03422623500227928, 0.0340825580060482], learning rate: 0.0098010000
Val loss  tensor(0.8992)
Val loss  tensor(1.1685)
Iteration 195000: total loss 0.0528, losses: [0.025889068841934204, 0.026866262778639793], learning rate: 0.0098010000
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 200000: total loss 0.0687, losses: [0.03433529660105705, 0.03437040373682976], learning rate: 0.0098010000
Val loss  tensor(0.7184)
Val loss  tensor(1.1543)
Iteration 205000: total loss 0.0869, losses: [0.043044280260801315, 0.04387606307864189], learning rate: 0.0098010000
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5835, losses: [1.2974878549575806, 1.2859973907470703], learning rate: 0.0100000000
Val loss  tensor(1.4163)
Val loss  tensor(1.3615)
Iteration 210000: total loss 0.0512, losses: [0.025524480268359184, 0.025627871975302696], learning rate: 0.0098010000
Val loss  tensor(0.7286)
Val loss  tensor(1.1573)
Iteration 5000: total loss 0.1639, losses: [0.08141510933637619, 0.08249497413635254], learning rate: 0.0100000000
Iteration 215000: total loss 0.0664, losses: [0.03354773297905922, 0.032821666449308395], learning rate: 0.0098010000
Iteration 10000: total loss 0.1273, losses: [0.06375274807214737, 0.06349845975637436], learning rate: 0.0100000000
Val loss  tensor(1.0045)
Val loss  tensor(1.2044)
Iteration 220000: total loss 0.0843, losses: [0.04171380400657654, 0.042572684586048126], learning rate: 0.0098010000
Val loss  tensor(0.7304)
Val loss  tensor(1.1541)
Iteration 15000: total loss 0.1511, losses: [0.07613060623407364, 0.07492514699697495], learning rate: 0.0100000000
Iteration 225000: total loss 0.0510, losses: [0.025070911273360252, 0.02595101110637188], learning rate: 0.0097029900
Iteration 20000: total loss 0.0807, losses: [0.03966422751545906, 0.04107792302966118], learning rate: 0.0100000000
Val loss  tensor(0.9398)
Val loss  tensor(1.1537)
Iteration 230000: total loss 0.0811, losses: [0.04052009433507919, 0.04053342342376709], learning rate: 0.0097029900
Val loss  tensor(1.1131)
Val loss  tensor(1.1975)
Iteration 25000: total loss 0.0888, losses: [0.0437900573015213, 0.044960107654333115], learning rate: 0.0100000000
Iteration 235000: total loss 0.1397, losses: [0.0693604052066803, 0.07030776888132095], learning rate: 0.0097029900
Iteration 30000: total loss 0.0749, losses: [0.03691491112112999, 0.03798624500632286], learning rate: 0.0100000000
Val loss  tensor(1.0095)
Val loss  tensor(1.1376)
Iteration 240000: total loss 0.1279, losses: [0.06426019221544266, 0.06363042443990707], learning rate: 0.0097029900
Val loss  tensor(5.6481)
Val loss  tensor(1.2621)
Iteration 35000: total loss 0.0940, losses: [0.04653343930840492, 0.047511011362075806], learning rate: 0.0100000000
Iteration 245000: total loss 0.1249, losses: [0.06269209831953049, 0.062253911048173904], learning rate: 0.0097029900
Iteration 40000: total loss 0.0703, losses: [0.0353887639939785, 0.034869398921728134], learning rate: 0.0100000000
Val loss  tensor(1.0521)
Val loss  tensor(1.1195)
Iteration 250000: total loss 0.1311, losses: [0.06599018722772598, 0.06509973108768463], learning rate: 0.0097029900
Val loss  tensor(1.0999)
Val loss  tensor(1.2288)
Iteration 45000: total loss 0.1037, losses: [0.05181030184030533, 0.051861464977264404], learning rate: 0.0100000000
Iteration 255000: total loss 0.1636, losses: [0.08232646435499191, 0.08129774779081345], learning rate: 0.0097029900
Iteration 50000: total loss 0.0918, losses: [0.04481826722621918, 0.046967364847660065], learning rate: 0.0100000000
Val loss  tensor(1.0587)
Val loss  tensor(1.1712)
Iteration 260000: total loss 0.1221, losses: [0.061164554208517075, 0.06098189949989319], learning rate: 0.0097029900
Val loss  tensor(0.6361)
Val loss  tensor(1.0703)
Iteration 55000: total loss 0.0768, losses: [0.03873229771852493, 0.03802640363574028], learning rate: 0.0100000000
Iteration 265000: total loss 0.1073, losses: [0.05341612920165062, 0.053841542452573776], learning rate: 0.0097029900
Iteration 60000: total loss 0.0708, losses: [0.03554288670420647, 0.035254064947366714], learning rate: 0.0100000000
Val loss  tensor(0.9865)
Val loss  tensor(1.0888)
Iteration 270000: total loss 0.1012, losses: [0.0505155511200428, 0.05064016580581665], learning rate: 0.0097029900
Val loss  tensor(1.1366)
Val loss  tensor(1.2276)
Iteration 65000: total loss 0.0585, losses: [0.028927749022841454, 0.029547644779086113], learning rate: 0.0100000000
Iteration 275000: total loss 0.1157, losses: [0.05836275964975357, 0.057348113507032394], learning rate: 0.0097029900
Iteration 70000: total loss 0.1389, losses: [0.06826706975698471, 0.07059777528047562], learning rate: 0.0100000000
Val loss  tensor(1.0169)
Val loss  tensor(1.1657)
Iteration 280000: total loss 0.1213, losses: [0.060537006705999374, 0.060795556753873825], learning rate: 0.0097029900
Val loss  tensor(0.8132)
Val loss  tensor(1.1464)
Iteration 75000: total loss 0.1520, losses: [0.0762883648276329, 0.07571250200271606], learning rate: 0.0099000000
Iteration 285000: total loss 0.1885, losses: [0.09615889191627502, 0.09235737472772598], learning rate: 0.0097029900
Iteration 80000: total loss 0.1110, losses: [0.05626491457223892, 0.05472518876194954], learning rate: 0.0099000000
Val loss  tensor(0.8483)
Val loss  tensor(1.0846)
Iteration 290000: total loss 0.1319, losses: [0.06676065921783447, 0.06514929980039597], learning rate: 0.0097029900
Val loss  tensor(0.6380)
Val loss  tensor(1.1625)
Iteration 85000: total loss 0.1301, losses: [0.06534790247678757, 0.06474431604146957], learning rate: 0.0099000000
Iteration 295000: total loss 0.1228, losses: [0.06178228184580803, 0.061033885926008224], learning rate: 0.0097029900
Iteration 90000: total loss 0.0966, losses: [0.04823660850524902, 0.048363301903009415], learning rate: 0.0099000000
Val loss  tensor(0.9331)
Val loss  tensor(1.0491)
Iteration 300000: total loss 0.1178, losses: [0.058756496757268906, 0.059041913598775864], learning rate: 0.0097029900
Val loss  tensor(1.0745)
Val loss  tensor(1.1848)
Iteration 95000: total loss 0.0799, losses: [0.040056414902210236, 0.03984932228922844], learning rate: 0.0099000000
Iteration 305000: total loss 0.1408, losses: [0.07005436718463898, 0.070748470723629], learning rate: 0.0097029900
Iteration 100000: total loss 0.0968, losses: [0.04786735400557518, 0.04894876107573509], learning rate: 0.0099000000
Val loss  tensor(0.9033)
Val loss  tensor(1.0761)
Iteration 310000: total loss 0.1284, losses: [0.06452822685241699, 0.06391841173171997], learning rate: 0.0097029900
Val loss  tensor(1.0362)
Val loss  tensor(1.1356)
Iteration 105000: total loss 0.0954, losses: [0.04822031408548355, 0.04713964834809303], learning rate: 0.0099000000
Iteration 315000: total loss 0.1146, losses: [0.05697350576519966, 0.057598453015089035], learning rate: 0.0096059601
Iteration 110000: total loss 0.1091, losses: [0.0544377937912941, 0.05467003211379051], learning rate: 0.0099000000
Val loss  tensor(0.9490)
Val loss  tensor(1.1220)
Iteration 320000: total loss 0.1193, losses: [0.06003452464938164, 0.059297043830156326], learning rate: 0.0096059601
Val loss  tensor(0.8753)
Val loss  tensor(1.1451)
Iteration 115000: total loss 0.0764, losses: [0.039075009524822235, 0.03734341263771057], learning rate: 0.0099000000
Iteration 325000: total loss 0.1468, losses: [0.07286162674427032, 0.07394879311323166], learning rate: 0.0096059601
Iteration 120000: total loss 0.0771, losses: [0.037861067801713943, 0.039250053465366364], learning rate: 0.0099000000
Val loss  tensor(0.9146)
Val loss  tensor(1.0894)
Iteration 330000: total loss 0.1230, losses: [0.06102855131030083, 0.0619366280734539], learning rate: 0.0096059601
Val loss  tensor(0.9169)
Val loss  tensor(1.1563)
Iteration 125000: total loss 0.1042, losses: [0.051719892770051956, 0.05251359939575195], learning rate: 0.0099000000
Iteration 335000: total loss 0.1420, losses: [0.07195904105901718, 0.07005181163549423], learning rate: 0.0096059601
Iteration 130000: total loss 0.1102, losses: [0.055576302111148834, 0.054585326462984085], learning rate: 0.0099000000
Val loss  tensor(1.2604)
Val loss  tensor(1.3183)
Iteration 340000: total loss 0.1280, losses: [0.06364911794662476, 0.06437847763299942], learning rate: 0.0096059601
Val loss  tensor(1.1416)
Val loss  tensor(1.2238)
Iteration 135000: total loss 0.0947, losses: [0.04733993113040924, 0.04734222963452339], learning rate: 0.0098010000
Iteration 345000: total loss 0.1048, losses: [0.05168278142809868, 0.053129423409700394], learning rate: 0.0096059601
Iteration 140000: total loss 0.1759, losses: [0.08931082487106323, 0.08659986406564713], learning rate: 0.0098010000
Val loss  tensor(1.0605)
Val loss  tensor(1.1311)
Iteration 350000: total loss 0.1056, losses: [0.05283131077885628, 0.05272122099995613], learning rate: 0.0096059601
Val loss  tensor(0.5940)
Val loss  tensor(1.1607)
Iteration 145000: total loss 0.0787, losses: [0.039046574383974075, 0.039681561291217804], learning rate: 0.0098010000
Iteration 355000: total loss 0.0982, losses: [0.049346357583999634, 0.04883354902267456], learning rate: 0.0096059601
Iteration 150000: total loss 0.0683, losses: [0.034318942576646805, 0.03402014449238777], learning rate: 0.0098010000
Val loss  tensor(0.9273)
Val loss  tensor(1.1160)
Iteration 360000: total loss 0.1302, losses: [0.06458523869514465, 0.06561493128538132], learning rate: 0.0096059601
Val loss  tensor(1.7591)
Val loss  tensor(1.1856)
Iteration 155000: total loss 0.1034, losses: [0.051473915576934814, 0.05188019201159477], learning rate: 0.0098010000
Iteration 365000: total loss 0.0891, losses: [0.04379981383681297, 0.045260991901159286], learning rate: 0.0096059601
Iteration 160000: total loss 0.1050, losses: [0.05212777853012085, 0.052897509187459946], learning rate: 0.0098010000
Val loss  tensor(0.7455)
Val loss  tensor(1.1305)
Iteration 370000: total loss 0.1073, losses: [0.05382341891527176, 0.053440339863300323], learning rate: 0.0096059601
Val loss  tensor(0.6152)
Val loss  tensor(1.3402)
Iteration 165000: total loss 0.1054, losses: [0.05183935910463333, 0.053528763353824615], learning rate: 0.0098010000
Iteration 375000: total loss 0.1211, losses: [0.06057502701878548, 0.06057320907711983], learning rate: 0.0096059601
Iteration 170000: total loss 0.0934, losses: [0.047389283776283264, 0.04601588100194931], learning rate: 0.0098010000
Val loss  tensor(0.8842)
Val loss  tensor(1.0899)
Iteration 380000: total loss 0.1908, losses: [0.09521456807851791, 0.0956154391169548], learning rate: 0.0096059601
Val loss  tensor(1.2220)
Val loss  tensor(1.2819)
Iteration 175000: total loss 0.1083, losses: [0.05430951341986656, 0.05397716164588928], learning rate: 0.0098010000
Iteration 385000: total loss 0.1916, losses: [0.09582846611738205, 0.09574863314628601], learning rate: 0.0096059601
Iteration 180000: total loss 0.1475, losses: [0.07329224050045013, 0.07424818724393845], learning rate: 0.0098010000
Val loss  tensor(0.9242)
Val loss  tensor(1.1772)
Iteration 390000: total loss 0.1425, losses: [0.07112039625644684, 0.07139480113983154], learning rate: 0.0096059601
Val loss  tensor(1.1376)
Val loss  tensor(1.1482)
Iteration 185000: total loss 0.1343, losses: [0.06773789972066879, 0.06655839830636978], learning rate: 0.0098010000
Iteration 395000: total loss 0.1674, losses: [0.08306583017110825, 0.08430274575948715], learning rate: 0.0096059601
Iteration 190000: total loss 0.1247, losses: [0.06268622726202011, 0.06198384240269661], learning rate: 0.0098010000
Val loss  tensor(1.0711)
Val loss  tensor(1.1518)
Iteration 400000: total loss 0.1109, losses: [0.054614245891571045, 0.056261785328388214], learning rate: 0.0096059601
Val loss  tensor(1.0246)
Val loss  tensor(1.2352)
Iteration 195000: total loss 0.0863, losses: [0.0424983948469162, 0.04383532702922821], learning rate: 0.0098010000
Iteration 405000: total loss 0.1135, losses: [0.0571906678378582, 0.05630607157945633], learning rate: 0.0095099005
Iteration 200000: total loss 0.1121, losses: [0.05624747276306152, 0.05585679039359093], learning rate: 0.0098010000
Val loss  tensor(0.6967)
Val loss  tensor(1.0814)
Iteration 410000: total loss 0.1752, losses: [0.08764510601758957, 0.08752378076314926], learning rate: 0.0095099005
Val loss  tensor(0.8812)
Val loss  tensor(1.1835)
Iteration 205000: total loss 0.0910, losses: [0.04528454691171646, 0.045665860176086426], learning rate: 0.0098010000
Iteration 415000: total loss 0.1804, losses: [0.08972344547510147, 0.09066233038902283], learning rate: 0.0095099005
Iteration 210000: total loss 0.1006, losses: [0.05005925893783569, 0.050559151917696], learning rate: 0.0098010000
Val loss  tensor(0.7475)
Val loss  tensor(1.1538)
Iteration 420000: total loss 0.1385, losses: [0.06898500770330429, 0.06950881332159042], learning rate: 0.0095099005
Val loss  tensor(0.8019)
Val loss  tensor(1.1435)
Iteration 215000: total loss 0.0678, losses: [0.03386741876602173, 0.03397383540868759], learning rate: 0.0098010000
Iteration 425000: total loss 0.1649, losses: [0.08247876912355423, 0.08239535987377167], learning rate: 0.0095099005
Iteration 220000: total loss 0.0882, losses: [0.04485035687685013, 0.043339893221855164], learning rate: 0.0098010000
Val loss  tensor(0.9209)
Val loss  tensor(1.4826)
Iteration 430000: total loss 0.1549, losses: [0.07712403684854507, 0.07779975980520248], learning rate: 0.0095099005
Val loss  tensor(0.7620)
Val loss  tensor(1.1205)
Iteration 225000: total loss 0.1031, losses: [0.052230894565582275, 0.0508706159889698], learning rate: 0.0098010000
Iteration 435000: total loss 0.1403, losses: [0.07006149739027023, 0.07022368907928467], learning rate: 0.0095099005
Iteration 230000: total loss 0.1329, losses: [0.06684904545545578, 0.06602101773023605], learning rate: 0.0098010000
Val loss  tensor(0.9981)
Val loss  tensor(1.0986)
Iteration 440000: total loss 0.1138, losses: [0.056736256927251816, 0.05710897967219353], learning rate: 0.0095099005
Val loss  tensor(0.9337)
Val loss  tensor(1.1156)
Iteration 235000: total loss 0.0667, losses: [0.03316303715109825, 0.033532314002513885], learning rate: 0.0098010000
Iteration 445000: total loss 0.1070, losses: [0.0538935624063015, 0.053135719150304794], learning rate: 0.0095099005
Iteration 240000: total loss 0.1301, losses: [0.0659538060426712, 0.06414364278316498], learning rate: 0.0098010000
Val loss  tensor(0.8610)
Val loss  tensor(1.0782)
Iteration 450000: total loss 0.1360, losses: [0.06821196526288986, 0.06780350208282471], learning rate: 0.0095099005
Val loss  tensor(1.0383)
Val loss  tensor(1.1400)
Iteration 245000: total loss 0.1058, losses: [0.05260699987411499, 0.053221024572849274], learning rate: 0.0098010000
Iteration 455000: total loss 0.1278, losses: [0.06347234547138214, 0.06431993097066879], learning rate: 0.0095099005
Iteration 250000: total loss 0.1109, losses: [0.05586527660489082, 0.05500461533665657], learning rate: 0.0098010000
Val loss  tensor(0.8708)
Val loss  tensor(1.0686)
Iteration 460000: total loss 0.1158, losses: [0.057026755064725876, 0.058727193623781204], learning rate: 0.0095099005
Val loss  tensor(1.0592)
Val loss  tensor(1.1848)
Iteration 255000: total loss 0.1526, losses: [0.0759509727358818, 0.07667521387338638], learning rate: 0.0097029900
Iteration 465000: total loss 0.0774, losses: [0.03814757615327835, 0.039233945310115814], learning rate: 0.0094148015
Iteration 260000: total loss 0.0662, losses: [0.03301732614636421, 0.033151037991046906], learning rate: 0.0097029900
Val loss  tensor(0.9372)
Val loss  tensor(1.0536)
Iteration 470000: total loss 0.1563, losses: [0.07819801568984985, 0.07812971621751785], learning rate: 0.0094148015
Val loss  tensor(0.7753)
Val loss  tensor(1.2426)
Iteration 265000: total loss 0.1040, losses: [0.05211671069264412, 0.051852207630872726], learning rate: 0.0097029900
Iteration 475000: total loss 0.1554, losses: [0.07716356962919235, 0.07826413959264755], learning rate: 0.0094148015
Iteration 270000: total loss 0.0692, losses: [0.03419163450598717, 0.035017963498830795], learning rate: 0.0097029900
Val loss  tensor(0.9422)
Val loss  tensor(1.1405)
Iteration 480000: total loss 0.1987, losses: [0.09756224602460861, 0.10109279304742813], learning rate: 0.0094148015
Val loss  tensor(0.7675)
Val loss  tensor(1.2370)
Iteration 275000: total loss 0.1238, losses: [0.06142399460077286, 0.0623631589114666], learning rate: 0.0097029900
Iteration 485000: total loss 0.1161, losses: [0.05822709947824478, 0.057893332093954086], learning rate: 0.0094148015
Iteration 280000: total loss 0.0846, losses: [0.04096376523375511, 0.04363063722848892], learning rate: 0.0097029900
Val loss  tensor(1.0516)
Val loss  tensor(1.1237)
Iteration 490000: total loss 0.1437, losses: [0.07144603878259659, 0.07224070280790329], learning rate: 0.0094148015
Val loss  tensor(1.1789)
Val loss  tensor(1.2231)
Iteration 285000: total loss 0.0995, losses: [0.04919326677918434, 0.05031865835189819], learning rate: 0.0097029900
Iteration 495000: total loss 0.0959, losses: [0.048372603952884674, 0.0475609265267849], learning rate: 0.0094148015
Iteration 290000: total loss 0.0713, losses: [0.034744154661893845, 0.036549437791109085], learning rate: 0.0097029900
Val loss  tensor(0.8271)
Val loss  tensor(1.0649)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): Tanh()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 295000: total loss 0.0949, losses: [0.04772535711526871, 0.047148678451776505], learning rate: 0.0097029900
Iteration 300000: total loss 0.0837, losses: [0.04187965393066406, 0.04185624420642853], learning rate: 0.0097029900
Val loss  tensor(0.5912)
Val loss  tensor(1.1454)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.6234, losses: [1.3197256326675415, 1.3037195205688477], learning rate: 0.0100000000
Val loss  tensor(1.8166)
Val loss  tensor(1.7286)
Iteration 305000: total loss 0.1257, losses: [0.06309869140386581, 0.06262540817260742], learning rate: 0.0097029900
Iteration 5000: total loss 0.0890, losses: [0.04319245368242264, 0.04581538960337639], learning rate: 0.0100000000
Iteration 310000: total loss 0.0860, losses: [0.04307371377944946, 0.042891524732112885], learning rate: 0.0097029900
Val loss  tensor(0.7466)
Val loss  tensor(1.2216)
Iteration 10000: total loss 0.0810, losses: [0.03942713886499405, 0.04159250110387802], learning rate: 0.0100000000
Val loss  tensor(1.1744)
Val loss  tensor(1.1812)
Iteration 315000: total loss 0.1182, losses: [0.0608646385371685, 0.05737745389342308], learning rate: 0.0097029900
Iteration 320000: total loss 0.0844, losses: [0.04215186461806297, 0.04220688343048096], learning rate: 0.0097029900
Val loss  tensor(0.9490)
Iteration 15000: total loss 0.0722, losses: [0.035887788981199265, 0.03629198670387268], learning rate: 0.0100000000
Val loss  tensor(1.1338)
Iteration 325000: total loss 0.0757, losses: [0.0377717986702919, 0.03790146857500076], learning rate: 0.0097029900
Iteration 20000: total loss 0.0989, losses: [0.04927922412753105, 0.04964696615934372], learning rate: 0.0100000000
Val loss  tensor(1.0573)
Val loss  tensor(1.1643)
Iteration 330000: total loss 0.1032, losses: [0.05140436813235283, 0.0518047921359539], learning rate: 0.0097029900
Val loss  tensor(0.6026)
Val loss  tensor(1.1350)
Iteration 25000: total loss 0.0463, losses: [0.022691428661346436, 0.023605480790138245], learning rate: 0.0100000000
Iteration 335000: total loss 0.1056, losses: [0.051852550357580185, 0.05375530198216438], learning rate: 0.0097029900
Iteration 30000: total loss 0.0400, losses: [0.019620632752776146, 0.02036386728286743], learning rate: 0.0100000000
Val loss  tensor(1.0995)
Val loss  tensor(1.1634)
Iteration 340000: total loss 0.0857, losses: [0.04195792227983475, 0.04374602064490318], learning rate: 0.0097029900
Val loss  tensor(0.8266)
Val loss  tensor(1.1076)
Iteration 35000: total loss 0.0401, losses: [0.020041977986693382, 0.02006390132009983], learning rate: 0.0100000000
Iteration 345000: total loss 0.1653, losses: [0.08451271057128906, 0.08082005381584167], learning rate: 0.0097029900
Iteration 40000: total loss 0.0434, losses: [0.02070663310587406, 0.02270772121846676], learning rate: 0.0100000000
Val loss  tensor(1.0844)
Val loss  tensor(1.1495)
Iteration 350000: total loss 0.0912, losses: [0.0458451509475708, 0.04533223807811737], learning rate: 0.0097029900
Val loss  tensor(1.0433)
Val loss  tensor(1.1470)
Iteration 45000: total loss 0.0453, losses: [0.02148536406457424, 0.02381098084151745], learning rate: 0.0100000000
Iteration 355000: total loss 0.2566, losses: [0.1280979961156845, 0.12847115099430084], learning rate: 0.0096059601
Iteration 50000: total loss 0.0360, losses: [0.017347099259495735, 0.018637312576174736], learning rate: 0.0100000000
Val loss  tensor(1.0916)
Val loss  tensor(1.1607)
Iteration 360000: total loss 0.1696, losses: [0.08480577915906906, 0.08484061062335968], learning rate: 0.0096059601
Val loss  tensor(0.7880)
Val loss  tensor(1.1461)
Iteration 55000: total loss 0.0624, losses: [0.030710583552718163, 0.03169788792729378], learning rate: 0.0100000000
Iteration 365000: total loss 0.0705, losses: [0.035084571689367294, 0.03537137061357498], learning rate: 0.0096059601
Iteration 60000: total loss 0.0559, losses: [0.02758377231657505, 0.02836233377456665], learning rate: 0.0100000000
Val loss  tensor(1.0838)
Val loss  tensor(1.1494)
Iteration 370000: total loss 0.1107, losses: [0.05499277263879776, 0.05575636029243469], learning rate: 0.0096059601
Val loss  tensor(0.9772)
Val loss  tensor(1.1722)
Iteration 65000: total loss 0.0612, losses: [0.030467811971902847, 0.03073134459555149], learning rate: 0.0100000000
Iteration 375000: total loss 0.1288, losses: [0.06400765478610992, 0.06476768106222153], learning rate: 0.0096059601
Iteration 70000: total loss 0.0353, losses: [0.017197681590914726, 0.01810591295361519], learning rate: 0.0100000000
Val loss  tensor(1.0893)
Val loss  tensor(1.1576)
Iteration 380000: total loss 0.0984, losses: [0.04941133037209511, 0.04897795990109444], learning rate: 0.0096059601
Val loss  tensor(0.9598)
Val loss  tensor(1.1714)
Iteration 75000: total loss 0.0538, losses: [0.026319975033402443, 0.02749120071530342], learning rate: 0.0099000000
Iteration 385000: total loss 0.1061, losses: [0.05376452952623367, 0.05233269929885864], learning rate: 0.0096059601
Iteration 80000: total loss 0.0346, losses: [0.017307667061686516, 0.017254916951060295], learning rate: 0.0099000000
Val loss  tensor(1.0966)
Iteration 390000: total loss 0.0668, losses: [0.03356681391596794, 0.03328164294362068], learning rate: 0.0096059601
Val loss  tensor(0.9588)
Val loss  tensor(1.1433)
Val loss  tensor(1.1544)
Iteration 395000: total loss 0.1021, losses: [0.051164258271455765, 0.050974246114492416], learning rate: 0.0096059601
Iteration 85000: total loss 0.0470, losses: [0.023160723969340324, 0.023853134363889694], learning rate: 0.0099000000
Iteration 400000: total loss 0.1354, losses: [0.0671289935708046, 0.06830068677663803], learning rate: 0.0096059601
Val loss  tensor(0.7390)
Val loss  tensor(1.1340)
Iteration 90000: total loss 0.0545, losses: [0.026936328038573265, 0.02752183936536312], learning rate: 0.0099000000
Val loss  tensor(1.0056)
Val loss  tensor(1.1423)
Iteration 405000: total loss 0.1018, losses: [0.051049795001745224, 0.05078240856528282], learning rate: 0.0096059601
Iteration 95000: total loss 0.0337, losses: [0.016842445358633995, 0.01684705726802349], learning rate: 0.0099000000
Iteration 410000: total loss 0.0655, losses: [0.03255587816238403, 0.03289955481886864], learning rate: 0.0096059601
Val loss  tensor(0.7691)
Val loss  tensor(1.1166)
Iteration 100000: total loss 0.0560, losses: [0.027685120701789856, 0.0283012967556715], learning rate: 0.0099000000
Val loss  tensor(1.0767)
Val loss  tensor(1.1455)
Iteration 415000: total loss 0.0797, losses: [0.038781095296144485, 0.040880754590034485], learning rate: 0.0095099005
Iteration 105000: total loss 0.0435, losses: [0.021748041734099388, 0.02176966704428196], learning rate: 0.0099000000
Iteration 420000: total loss 0.0579, losses: [0.028646141290664673, 0.029249507933855057], learning rate: 0.0095099005
Val loss  tensor(0.6825)
Val loss  tensor(1.1348)
Iteration 110000: total loss 0.0273, losses: [0.013278926722705364, 0.014027753844857216], learning rate: 0.0099000000
Val loss  tensor(1.0947)
Val loss  tensor(1.1541)
Iteration 425000: total loss 0.0984, losses: [0.05063709616661072, 0.04778033122420311], learning rate: 0.0095099005
Iteration 115000: total loss 0.0324, losses: [0.015737272799015045, 0.016674894839525223], learning rate: 0.0099000000
Iteration 430000: total loss 0.1118, losses: [0.05573230981826782, 0.0560901053249836], learning rate: 0.0095099005
Val loss  tensor(0.8491)
Val loss  tensor(1.1376)
Iteration 120000: total loss 0.0565, losses: [0.027568507939577103, 0.02897842600941658], learning rate: 0.0099000000
Val loss  tensor(0.9946)
Val loss  tensor(1.1336)
Iteration 435000: total loss 0.0941, losses: [0.04707525297999382, 0.047003816813230515], learning rate: 0.0095099005
Iteration 125000: total loss 0.0308, losses: [0.015297102741897106, 0.015520155429840088], learning rate: 0.0099000000
Iteration 440000: total loss 0.0959, losses: [0.048616230487823486, 0.047291405498981476], learning rate: 0.0095099005
Val loss  tensor(0.7497)
Val loss  tensor(1.1009)
Iteration 130000: total loss 0.0271, losses: [0.012912263162434101, 0.014153820462524891], learning rate: 0.0099000000
Val loss  tensor(1.1268)
Val loss  tensor(1.1702)
Iteration 445000: total loss 0.1580, losses: [0.07747353613376617, 0.0804811641573906], learning rate: 0.0095099005
Iteration 135000: total loss 0.0319, losses: [0.015518316999077797, 0.016415802761912346], learning rate: 0.0099000000
Iteration 450000: total loss 0.1195, losses: [0.059576328843832016, 0.05992956832051277], learning rate: 0.0095099005
Val loss  tensor(1.0870)
Val loss  tensor(1.1320)
Iteration 140000: total loss 0.0555, losses: [0.027323052287101746, 0.028139833360910416], learning rate: 0.0099000000
Val loss  tensor(1.1148)
Val loss  tensor(1.1850)
Iteration 455000: total loss 0.1213, losses: [0.06050453335046768, 0.060769882053136826], learning rate: 0.0095099005
Iteration 145000: total loss 0.0526, losses: [0.02636776678264141, 0.026212770491838455], learning rate: 0.0099000000
Iteration 460000: total loss 0.1396, losses: [0.06978978961706161, 0.06979071348905563], learning rate: 0.0095099005
Val loss  tensor(1.0998)
Val loss  tensor(1.1455)
Iteration 150000: total loss 0.0281, losses: [0.013464768417179585, 0.014644148759543896], learning rate: 0.0099000000
Val loss  tensor(1.0864)
Iteration 465000: total loss 0.1032, losses: [0.05296735838055611, 0.05025072768330574], learning rate: 0.0094148015
Val loss  tensor(1.1573)
Iteration 470000: total loss 0.1089, losses: [0.05444979667663574, 0.0544854998588562], learning rate: 0.0094148015
Val loss  tensor(0.7814)
Val loss  tensor(1.1762)
Iteration 155000: total loss 0.0464, losses: [0.022836513817310333, 0.023600881919264793], learning rate: 0.0099000000
Iteration 475000: total loss 0.0981, losses: [0.04912969470024109, 0.049019407480955124], learning rate: 0.0094148015
Iteration 160000: total loss 0.0334, losses: [0.016433292999863625, 0.01691742055118084], learning rate: 0.0099000000
Val loss  tensor(1.1787)
Val loss  tensor(1.1970)
Iteration 480000: total loss 0.0901, losses: [0.044701363891363144, 0.045384157449007034], learning rate: 0.0094148015
Val loss  tensor(0.6495)
Val loss  tensor(1.1255)
Iteration 165000: total loss 0.0269, losses: [0.01306850928813219, 0.01387654896825552], learning rate: 0.0099000000
Iteration 485000: total loss 0.1073, losses: [0.05328560993075371, 0.05404150113463402], learning rate: 0.0094148015
Iteration 170000: total loss 0.0297, losses: [0.014598267152905464, 0.015147867612540722], learning rate: 0.0099000000
Val loss  tensor(1.1790)
Val loss  tensor(1.2079)
Iteration 490000: total loss 0.0717, losses: [0.036524463444948196, 0.035219691693782806], learning rate: 0.0094148015
Val loss  tensor(0.8334)
Val loss  tensor(1.1286)
Iteration 175000: total loss 0.0434, losses: [0.02142188511788845, 0.021945390850305557], learning rate: 0.0098010000
Iteration 495000: total loss 0.0995, losses: [0.04994283244013786, 0.049601960927248], learning rate: 0.0094148015
Iteration 180000: total loss 0.0263, losses: [0.012767629697918892, 0.01351282000541687], learning rate: 0.0098010000
Val loss  tensor(1.0683)
Val loss  tensor(1.1622)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 185000: total loss 0.0323, losses: [0.01594998501241207, 0.016325419768691063], learning rate: 0.0098010000
Iteration 190000: total loss 0.0268, losses: [0.013012821786105633, 0.013830686919391155], learning rate: 0.0098010000
Val loss  tensor(1.1526)
Val loss  tensor(1.1865)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5527, losses: [1.287528395652771, 1.265207290649414], learning rate: 0.0100000000
Val loss  tensor(1.0815)
Val loss  tensor(0.9813)
Iteration 195000: total loss 0.0300, losses: [0.014788503758609295, 0.015164843760430813], learning rate: 0.0098010000
Iteration 5000: total loss 0.1202, losses: [0.05985025689005852, 0.060323119163513184], learning rate: 0.0100000000
Iteration 200000: total loss 0.0249, losses: [0.012355244718492031, 0.012581385672092438], learning rate: 0.0098010000
Val loss  tensor(0.9686)
Val loss  tensor(1.1956)
Iteration 10000: total loss 0.1036, losses: [0.05087440088391304, 0.05270358547568321], learning rate: 0.0100000000
Val loss  tensor(1.0787)
Val loss  tensor(1.4487)
Iteration 205000: total loss 0.0896, losses: [0.04447753354907036, 0.045093800872564316], learning rate: 0.0098010000
Iteration 15000: total loss 0.0458, losses: [0.021565882489085197, 0.02420768141746521], learning rate: 0.0100000000
Iteration 210000: total loss 0.0247, losses: [0.011834812350571156, 0.012854263186454773], learning rate: 0.0098010000
Val loss  tensor(0.9258)
Val loss  tensor(1.1612)
Iteration 20000: total loss 0.0591, losses: [0.029071485623717308, 0.0300351083278656], learning rate: 0.0100000000
Val loss  tensor(1.0101)
Val loss  tensor(1.1656)
Iteration 215000: total loss 0.0571, losses: [0.028166918084025383, 0.028938904404640198], learning rate: 0.0098010000
Iteration 25000: total loss 0.0579, losses: [0.028679439797997475, 0.02926703356206417], learning rate: 0.0100000000
Iteration 220000: total loss 0.0258, losses: [0.012541360221803188, 0.013232091441750526], learning rate: 0.0098010000
Val loss  tensor(0.9409)
Val loss  tensor(1.1411)
Iteration 30000: total loss 0.0663, losses: [0.03220938518643379, 0.034050703048706055], learning rate: 0.0100000000
Val loss  tensor(0.8807)
Val loss  tensor(1.1133)
Iteration 225000: total loss 0.0416, losses: [0.020942196249961853, 0.02068934589624405], learning rate: 0.0098010000
Iteration 35000: total loss 0.0532, losses: [0.026418525725603104, 0.026822729036211967], learning rate: 0.0100000000
Iteration 230000: total loss 0.0252, losses: [0.01212709303945303, 0.013025750406086445], learning rate: 0.0098010000
Val loss  tensor(1.1037)
Val loss  tensor(1.1630)
Iteration 40000: total loss 0.0312, losses: [0.015370693989098072, 0.015800202265381813], learning rate: 0.0100000000
Val loss  tensor(0.8629)
Val loss  tensor(1.1445)
Iteration 235000: total loss 0.0380, losses: [0.018490049988031387, 0.01951463893055916], learning rate: 0.0098010000
Iteration 45000: total loss 0.0400, losses: [0.019887754693627357, 0.02011355198919773], learning rate: 0.0100000000
Iteration 240000: total loss 0.0372, losses: [0.017986033111810684, 0.019198274239897728], learning rate: 0.0098010000
Val loss  tensor(1.1199)
Iteration 50000: total loss 0.0485, losses: [0.02382860891520977, 0.024637846276164055], learning rate: 0.0100000000
Val loss  tensor(0.9673)
Val loss  tensor(1.1991)
Val loss  tensor(1.0578)
Iteration 55000: total loss 0.0568, losses: [0.02814146690070629, 0.028630530461668968], learning rate: 0.0100000000
Iteration 245000: total loss 0.0363, losses: [0.018101302906870842, 0.018204841762781143], learning rate: 0.0098010000
Iteration 60000: total loss 0.0420, losses: [0.020445270463824272, 0.02157503366470337], learning rate: 0.0100000000
Val loss  tensor(0.8064)
Val loss  tensor(1.0446)
Iteration 250000: total loss 0.2417, losses: [0.11879328638315201, 0.12286382913589478], learning rate: 0.0098010000
Val loss  tensor(1.1228)
Val loss  tensor(1.1961)
Iteration 65000: total loss 0.0311, losses: [0.015415499918162823, 0.015710724517703056], learning rate: 0.0100000000
Iteration 255000: total loss 0.0269, losses: [0.013315764255821705, 0.013538728468120098], learning rate: 0.0098010000
Iteration 70000: total loss 0.0384, losses: [0.01895330287516117, 0.019481796771287918], learning rate: 0.0100000000
Val loss  tensor(0.8107)
Val loss  tensor(1.0702)
Iteration 260000: total loss 0.0268, losses: [0.012632359750568867, 0.014174972660839558], learning rate: 0.0098010000
Val loss  tensor(1.0903)
Val loss  tensor(1.1785)
Iteration 75000: total loss 0.0272, losses: [0.013206538744270802, 0.01398508157581091], learning rate: 0.0100000000
Iteration 265000: total loss 0.0493, losses: [0.024797027930617332, 0.02454499900341034], learning rate: 0.0097029900
Iteration 80000: total loss 0.0626, losses: [0.03068261779844761, 0.031964004039764404], learning rate: 0.0100000000
Val loss  tensor(0.9680)
Val loss  tensor(1.0792)
Iteration 270000: total loss 0.0340, losses: [0.016589438542723656, 0.017420867457985878], learning rate: 0.0097029900
Val loss  tensor(1.0828)
Val loss  tensor(1.1865)
Iteration 85000: total loss 0.0557, losses: [0.027392501011490822, 0.02830592356622219], learning rate: 0.0100000000
Iteration 275000: total loss 0.0240, losses: [0.011579567566514015, 0.012388098053634167], learning rate: 0.0097029900
Iteration 90000: total loss 0.0269, losses: [0.013338129036128521, 0.01358693279325962], learning rate: 0.0100000000
Val loss  tensor(1.1912)
Val loss  tensor(1.1959)
Iteration 280000: total loss 0.0237, losses: [0.01144934818148613, 0.012264116667211056], learning rate: 0.0097029900
Val loss  tensor(1.0533)
Val loss  tensor(1.1723)
Iteration 95000: total loss 0.0246, losses: [0.012239225208759308, 0.01239769533276558], learning rate: 0.0100000000
Iteration 285000: total loss 0.0683, losses: [0.03396984934806824, 0.03436612710356712], learning rate: 0.0097029900
Iteration 100000: total loss 0.0345, losses: [0.01707003451883793, 0.0174099151045084], learning rate: 0.0100000000
Val loss  tensor(0.9976)
Val loss  tensor(1.1719)
Iteration 290000: total loss 0.0266, losses: [0.013271793723106384, 0.01327968668192625], learning rate: 0.0097029900
Val loss  tensor(1.1099)
Val loss  tensor(1.1788)
Iteration 105000: total loss 0.0878, losses: [0.043097008019685745, 0.044666025787591934], learning rate: 0.0100000000
Iteration 295000: total loss 0.0407, losses: [0.01972871646285057, 0.0210031159222126], learning rate: 0.0097029900
Iteration 110000: total loss 0.0376, losses: [0.018211955204606056, 0.01941606216132641], learning rate: 0.0100000000
Val loss  tensor(1.0806)
Val loss  tensor(1.1373)
Iteration 300000: total loss 0.0249, losses: [0.011781292036175728, 0.013068732805550098], learning rate: 0.0097029900
Val loss  tensor(1.2054)
Val loss  tensor(1.2076)
Iteration 115000: total loss 0.0375, losses: [0.018499046564102173, 0.01897953636944294], learning rate: 0.0099000000
Iteration 305000: total loss 0.0224, losses: [0.010914522223174572, 0.011469805613160133], learning rate: 0.0097029900
Iteration 120000: total loss 0.0255, losses: [0.012361845932900906, 0.013168999925255775], learning rate: 0.0099000000
Val loss  tensor(0.7391)
Val loss  tensor(1.0683)
Iteration 310000: total loss 0.0589, losses: [0.029176045209169388, 0.029689347371459007], learning rate: 0.0097029900
Val loss  tensor(1.1768)
Iteration 125000: total loss 0.0310, losses: [0.01528101321309805, 0.015759779140353203], learning rate: 0.0099000000
Val loss  tensor(1.1950)
Iteration 130000: total loss 0.0440, losses: [0.02153937704861164, 0.022424574941396713], learning rate: 0.0099000000
Val loss  tensor(0.7183)
Val loss  tensor(1.0693)
Iteration 315000: total loss 0.0260, losses: [0.012756255455315113, 0.01328729372471571], learning rate: 0.0097029900
Iteration 135000: total loss 0.0269, losses: [0.013405079953372478, 0.013488947413861752], learning rate: 0.0099000000
Iteration 320000: total loss 0.0244, losses: [0.011921430937945843, 0.012453590519726276], learning rate: 0.0097029900
Val loss  tensor(1.1642)
Val loss  tensor(1.1856)
Iteration 140000: total loss 0.0415, losses: [0.020338883623480797, 0.021151293069124222], learning rate: 0.0099000000
Val loss  tensor(0.7437)
Val loss  tensor(1.0706)
Iteration 325000: total loss 0.0363, losses: [0.017467526718974113, 0.018861548975110054], learning rate: 0.0096059601
Iteration 145000: total loss 0.0222, losses: [0.011046774685382843, 0.011114358901977539], learning rate: 0.0099000000
Iteration 330000: total loss 0.0231, losses: [0.011225267313420773, 0.011875765398144722], learning rate: 0.0096059601
Val loss  tensor(1.0981)
Val loss  tensor(1.1465)
Iteration 150000: total loss 0.0389, losses: [0.019146544858813286, 0.01978939399123192], learning rate: 0.0099000000
Val loss  tensor(0.7344)
Val loss  tensor(1.1475)
Iteration 335000: total loss 0.0224, losses: [0.011177434585988522, 0.011250164359807968], learning rate: 0.0096059601
Iteration 155000: total loss 0.0333, losses: [0.016575386747717857, 0.016753150150179863], learning rate: 0.0099000000
Iteration 340000: total loss 0.0222, losses: [0.010673867538571358, 0.0115740355104208], learning rate: 0.0096059601
Val loss  tensor(1.1421)
Val loss  tensor(1.2044)
Iteration 160000: total loss 0.0458, losses: [0.022482439875602722, 0.023309694603085518], learning rate: 0.0099000000
Val loss  tensor(0.9237)
Val loss  tensor(1.0889)
Iteration 345000: total loss 0.0262, losses: [0.013168958015739918, 0.013047091662883759], learning rate: 0.0096059601
Iteration 165000: total loss 0.0310, losses: [0.015274696052074432, 0.01573268510401249], learning rate: 0.0099000000
Iteration 350000: total loss 0.0289, losses: [0.014215159229934216, 0.01463803369551897], learning rate: 0.0096059601
Val loss  tensor(1.0031)
Val loss  tensor(1.1510)
Iteration 170000: total loss 0.0419, losses: [0.020431311801075935, 0.02141926996409893], learning rate: 0.0099000000
Val loss  tensor(0.7878)
Val loss  tensor(1.0965)
Iteration 355000: total loss 0.0225, losses: [0.011021404527127743, 0.011458824388682842], learning rate: 0.0096059601
Iteration 175000: total loss 0.0249, losses: [0.011958186514675617, 0.012948771007359028], learning rate: 0.0099000000
Iteration 360000: total loss 0.0273, losses: [0.013416138477623463, 0.013871550559997559], learning rate: 0.0096059601
Val loss  tensor(1.0864)
Val loss  tensor(1.1377)
Iteration 180000: total loss 0.0376, losses: [0.018684163689613342, 0.018885036930441856], learning rate: 0.0099000000
Val loss  tensor(1.1020)
Val loss  tensor(1.1941)
Iteration 365000: total loss 0.0377, losses: [0.01880042999982834, 0.01892794482409954], learning rate: 0.0096059601
Iteration 185000: total loss 0.0350, losses: [0.017266852781176567, 0.017751038074493408], learning rate: 0.0098010000
Iteration 370000: total loss 0.0411, losses: [0.020056841894984245, 0.021091509610414505], learning rate: 0.0096059601
Val loss  tensor(1.1204)
Iteration 190000: total loss 0.0469, losses: [0.022984763607382774, 0.02390924282371998], learning rate: 0.0098010000
Val loss  tensor(0.9182)
Val loss  tensor(1.2053)
Val loss  tensor(1.1071)
Iteration 375000: total loss 0.0338, losses: [0.016709981486201286, 0.017080416902899742], learning rate: 0.0095099005
Iteration 195000: total loss 0.0182, losses: [0.009018370881676674, 0.009211758151650429], learning rate: 0.0098010000
Iteration 200000: total loss 0.0268, losses: [0.013155422173440456, 0.013616046868264675], learning rate: 0.0098010000
Val loss  tensor(0.7879)
Iteration 380000: total loss 0.0328, losses: [0.015683932229876518, 0.017066216096282005], learning rate: 0.0095099005
Val loss  tensor(1.2084)
Val loss  tensor(1.1067)
Val loss  tensor(1.2146)
Iteration 205000: total loss 0.0240, losses: [0.011774773709475994, 0.01225978508591652], learning rate: 0.0098010000
Iteration 385000: total loss 0.0405, losses: [0.019962241873145103, 0.02057514898478985], learning rate: 0.0095099005
Iteration 210000: total loss 0.0274, losses: [0.013209070079028606, 0.01415950059890747], learning rate: 0.0098010000
Val loss  tensor(0.7679)
Val loss  tensor(1.1251)
Iteration 390000: total loss 0.0343, losses: [0.01687614433467388, 0.017406022176146507], learning rate: 0.0095099005
Val loss  tensor(1.0963)
Val loss  tensor(1.1990)
Iteration 215000: total loss 0.0203, losses: [0.009741405956447124, 0.01051368284970522], learning rate: 0.0098010000
Iteration 395000: total loss 0.0245, losses: [0.012318319641053677, 0.01221951749175787], learning rate: 0.0095099005
Iteration 220000: total loss 0.0197, losses: [0.009640770964324474, 0.01005276944488287], learning rate: 0.0098010000
Val loss  tensor(0.9014)
Val loss  tensor(1.1149)
Iteration 400000: total loss 0.0455, losses: [0.02272569201886654, 0.0227933581918478], learning rate: 0.0095099005
Val loss  tensor(1.0468)
Val loss  tensor(1.2201)
Iteration 225000: total loss 0.0344, losses: [0.01680896058678627, 0.017594585195183754], learning rate: 0.0098010000
Iteration 405000: total loss 0.0261, losses: [0.012959393672645092, 0.013190018944442272], learning rate: 0.0095099005
Iteration 230000: total loss 0.0277, losses: [0.01387106068432331, 0.013810825534164906], learning rate: 0.0098010000
Val loss  tensor(0.8748)
Val loss  tensor(1.1301)
Iteration 410000: total loss 0.0256, losses: [0.012424170039594173, 0.013217789120972157], learning rate: 0.0095099005
Val loss  tensor(1.0208)
Val loss  tensor(1.1295)
Iteration 235000: total loss 0.0351, losses: [0.017311228439211845, 0.017787402495741844], learning rate: 0.0098010000
Iteration 415000: total loss 0.0255, losses: [0.0127099035307765, 0.012797926552593708], learning rate: 0.0095099005
Iteration 240000: total loss 0.0232, losses: [0.01159969624131918, 0.01163506880402565], learning rate: 0.0098010000
Val loss  tensor(0.7844)
Val loss  tensor(1.1033)
Iteration 420000: total loss 0.0245, losses: [0.011963363736867905, 0.012527051381766796], learning rate: 0.0095099005
Val loss  tensor(1.0920)
Val loss  tensor(1.1646)
Iteration 245000: total loss 0.0350, losses: [0.01729464903473854, 0.017709434032440186], learning rate: 0.0097029900
Iteration 425000: total loss 0.0261, losses: [0.012783941812813282, 0.013279243372380733], learning rate: 0.0095099005
Iteration 250000: total loss 0.0300, losses: [0.01473468542098999, 0.015228393487632275], learning rate: 0.0097029900
Val loss  tensor(1.1006)
Val loss  tensor(1.1716)
Iteration 430000: total loss 0.0236, losses: [0.011744867078959942, 0.011899369768798351], learning rate: 0.0095099005
Val loss  tensor(1.0534)
Val loss  tensor(1.1474)
Iteration 255000: total loss 0.0253, losses: [0.01242043823003769, 0.012922832742333412], learning rate: 0.0097029900
Iteration 435000: total loss 0.0372, losses: [0.01849536970257759, 0.018662426620721817], learning rate: 0.0094148015
Iteration 260000: total loss 0.0283, losses: [0.014285901561379433, 0.013997090049088001], learning rate: 0.0097029900
Val loss  tensor(1.0601)
Val loss  tensor(1.1598)
Iteration 440000: total loss 0.0378, losses: [0.018547188490629196, 0.019203929230570793], learning rate: 0.0094148015
Val loss  tensor(0.9718)
Val loss  tensor(1.1309)
Iteration 265000: total loss 0.0172, losses: [0.008285355754196644, 0.008887807838618755], learning rate: 0.0097029900
Iteration 270000: total loss 0.0308, losses: [0.015233151614665985, 0.015577730722725391], learning rate: 0.0097029900
Val loss  tensor(0.6591)
Iteration 445000: total loss 0.0239, losses: [0.011737695895135403, 0.012180224061012268], learning rate: 0.0094148015
Val loss  tensor(1.0897)
Iteration 275000: total loss 0.0227, losses: [0.011151790618896484, 0.011511757038533688], learning rate: 0.0097029900
Iteration 450000: total loss 0.0481, losses: [0.023514023050665855, 0.02462186850607395], learning rate: 0.0094148015
Val loss  tensor(1.0824)
Val loss  tensor(1.1775)
Iteration 280000: total loss 0.0170, losses: [0.008563468232750893, 0.00842264574021101], learning rate: 0.0097029900
Val loss  tensor(0.7329)
Val loss  tensor(1.1173)
Iteration 455000: total loss 0.0297, losses: [0.01450100727379322, 0.015229055657982826], learning rate: 0.0094148015
Iteration 285000: total loss 0.0350, losses: [0.017230350524187088, 0.017742235213518143], learning rate: 0.0097029900
Iteration 460000: total loss 0.0323, losses: [0.015892500057816505, 0.016430089250206947], learning rate: 0.0094148015
Val loss  tensor(1.0747)
Val loss  tensor(1.1900)
Iteration 290000: total loss 0.0208, losses: [0.01012346986681223, 0.010668382048606873], learning rate: 0.0097029900
Val loss  tensor(1.0582)
Val loss  tensor(1.1544)
Iteration 465000: total loss 0.1879, losses: [0.09409569948911667, 0.09379895031452179], learning rate: 0.0094148015
Iteration 295000: total loss 0.0325, losses: [0.0159947220236063, 0.016495687887072563], learning rate: 0.0097029900
Iteration 470000: total loss 0.0227, losses: [0.011054718866944313, 0.011602289043366909], learning rate: 0.0094148015
Val loss  tensor(1.0834)
Val loss  tensor(1.1957)
Iteration 300000: total loss 0.0330, losses: [0.016661783680319786, 0.016308441758155823], learning rate: 0.0097029900
Val loss  tensor(0.8768)
Val loss  tensor(1.0971)
Iteration 475000: total loss 0.0223, losses: [0.010864920914173126, 0.011454475112259388], learning rate: 0.0094148015
Iteration 305000: total loss 0.0202, losses: [0.010050235316157341, 0.010186244733631611], learning rate: 0.0097029900
Iteration 480000: total loss 0.0263, losses: [0.013062146492302418, 0.013189489021897316], learning rate: 0.0094148015
Val loss  tensor(1.1645)
Val loss  tensor(1.1918)
Iteration 310000: total loss 0.0354, losses: [0.017503587529063225, 0.017866242676973343], learning rate: 0.0097029900
Val loss  tensor(0.8593)
Val loss  tensor(1.1018)
Iteration 485000: total loss 0.0410, losses: [0.01988600380718708, 0.021077005192637444], learning rate: 0.0093206535
Iteration 315000: total loss 0.0428, losses: [0.021274996921420097, 0.02152748964726925], learning rate: 0.0097029900
Iteration 490000: total loss 0.0305, losses: [0.015029971487820148, 0.01551015954464674], learning rate: 0.0093206535
Val loss  tensor(0.9875)
Val loss  tensor(1.1374)
Iteration 320000: total loss 0.0346, losses: [0.01725616492331028, 0.01732952520251274], learning rate: 0.0097029900
Val loss  tensor(1.1458)
Val loss  tensor(1.1959)
Iteration 495000: total loss 0.0389, losses: [0.01930142194032669, 0.019549844786524773], learning rate: 0.0093206535
Iteration 325000: total loss 0.0526, losses: [0.026079848408699036, 0.026566505432128906], learning rate: 0.0096059601
Iteration 330000: total loss 0.0263, losses: [0.013018502853810787, 0.01330498605966568], learning rate: 0.0096059601
Val loss  tensor(1.1739)
Val loss  tensor(1.2075)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=128, bias=True)
    (5): Tanh()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): Tanh()
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 335000: total loss 0.0383, losses: [0.018732771277427673, 0.019521018490195274], learning rate: 0.0096059601
Iteration 340000: total loss 0.0381, losses: [0.019121399149298668, 0.019005224108695984], learning rate: 0.0096059601
Val loss  tensor(1.0891)
Val loss  tensor(1.1814)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5143, losses: [1.2616417407989502, 1.2526943683624268], learning rate: 0.0100000000
Val loss  tensor(1.0202)
Val loss  tensor(1.0172)
Iteration 345000: total loss 0.0188, losses: [0.009325544349849224, 0.009466729126870632], learning rate: 0.0096059601
Iteration 5000: total loss 0.0896, losses: [0.044393111020326614, 0.045177750289440155], learning rate: 0.0100000000
Iteration 350000: total loss 0.0203, losses: [0.009904271923005581, 0.01040488202124834], learning rate: 0.0096059601
Val loss  tensor(1.0049)
Val loss  tensor(1.1718)
Iteration 10000: total loss 0.0431, losses: [0.02114204876124859, 0.021908406168222427], learning rate: 0.0100000000
Val loss  tensor(0.9669)
Val loss  tensor(1.1146)
Iteration 355000: total loss 0.0314, losses: [0.015562983229756355, 0.01587674207985401], learning rate: 0.0096059601
Iteration 15000: total loss 0.0451, losses: [0.022358441725373268, 0.02270352654159069], learning rate: 0.0100000000
Iteration 360000: total loss 0.0219, losses: [0.010595048777759075, 0.011269702576100826], learning rate: 0.0096059601
Val loss  tensor(0.8061)
Val loss  tensor(1.1152)
Iteration 20000: total loss 0.0323, losses: [0.015948625281453133, 0.016305331140756607], learning rate: 0.0100000000
Val loss  tensor(1.0897)
Val loss  tensor(1.1377)
Iteration 365000: total loss 0.0366, losses: [0.018503088504076004, 0.018114468082785606], learning rate: 0.0096059601
Iteration 25000: total loss 0.0525, losses: [0.025839651003479958, 0.0266252513974905], learning rate: 0.0100000000
Iteration 370000: total loss 0.0191, losses: [0.009400955401360989, 0.009656333364546299], learning rate: 0.0096059601
Val loss  tensor(0.6835)
Val loss  tensor(1.1056)
Iteration 30000: total loss 0.0352, losses: [0.017475171014666557, 0.01769282855093479], learning rate: 0.0100000000
Val loss  tensor(1.0845)
Val loss  tensor(1.0915)
Iteration 375000: total loss 0.0342, losses: [0.017153549939393997, 0.01705547235906124], learning rate: 0.0096059601
Iteration 35000: total loss 0.0398, losses: [0.020039571449160576, 0.01975572668015957], learning rate: 0.0100000000
Iteration 380000: total loss 0.0237, losses: [0.011754926294088364, 0.011973060667514801], learning rate: 0.0096059601
Val loss  tensor(0.8037)
Val loss  tensor(1.0971)
Iteration 40000: total loss 0.0526, losses: [0.026034260168671608, 0.026540512219071388], learning rate: 0.0100000000
Val loss  tensor(1.0809)
Val loss  tensor(1.1155)
Iteration 385000: total loss 0.0250, losses: [0.01235633622854948, 0.012666746973991394], learning rate: 0.0095099005
Iteration 45000: total loss 0.0326, losses: [0.016124103218317032, 0.016439467668533325], learning rate: 0.0100000000
Iteration 390000: total loss 0.0306, losses: [0.015261560678482056, 0.015323993749916553], learning rate: 0.0095099005
Val loss  tensor(0.8124)
Val loss  tensor(1.1165)
Iteration 50000: total loss 0.0554, losses: [0.027536114677786827, 0.027910877019166946], learning rate: 0.0100000000
Val loss  tensor(1.0499)
Val loss  tensor(1.0850)
Iteration 395000: total loss 0.0338, losses: [0.016464943066239357, 0.017317619174718857], learning rate: 0.0095099005
Iteration 55000: total loss 0.0358, losses: [0.01774521730840206, 0.018005911260843277], learning rate: 0.0100000000
Iteration 400000: total loss 0.0327, losses: [0.01594097539782524, 0.01673087850213051], learning rate: 0.0095099005
Val loss  tensor(0.8155)
Val loss  tensor(1.1435)
Iteration 60000: total loss 0.0546, losses: [0.02665201760828495, 0.02791597507894039], learning rate: 0.0100000000
Val loss  tensor(1.0044)
Val loss  tensor(1.0882)
Iteration 405000: total loss 0.0176, losses: [0.008594726212322712, 0.009043234400451183], learning rate: 0.0095099005
Iteration 65000: total loss 0.0229, losses: [0.011027239263057709, 0.01182771660387516], learning rate: 0.0099000000
Iteration 410000: total loss 0.0325, losses: [0.016102304682135582, 0.016408532857894897], learning rate: 0.0095099005
Val loss  tensor(0.9968)
Val loss  tensor(1.1361)
Iteration 70000: total loss 0.0405, losses: [0.02011142484843731, 0.020342038944363594], learning rate: 0.0099000000
Val loss  tensor(1.1520)
Val loss  tensor(1.1794)
Iteration 415000: total loss 0.0316, losses: [0.01569410227239132, 0.015879705548286438], learning rate: 0.0095099005
Iteration 75000: total loss 0.0311, losses: [0.015491947531700134, 0.015651142224669456], learning rate: 0.0099000000
Iteration 420000: total loss 0.0341, losses: [0.017005590721964836, 0.017067858949303627], learning rate: 0.0095099005
Val loss  tensor(1.0445)
Val loss  tensor(1.1761)
Iteration 80000: total loss 0.0390, losses: [0.019443226978182793, 0.01960122399032116], learning rate: 0.0099000000
Val loss  tensor(1.0328)
Val loss  tensor(1.0795)
Iteration 425000: total loss 0.0203, losses: [0.009685083292424679, 0.010642948560416698], learning rate: 0.0095099005
Iteration 85000: total loss 0.0276, losses: [0.013584914617240429, 0.014034295454621315], learning rate: 0.0099000000
Iteration 430000: total loss 0.0235, losses: [0.011546432971954346, 0.011921567842364311], learning rate: 0.0095099005
Val loss  tensor(0.6435)
Val loss  tensor(1.0863)
Iteration 90000: total loss 0.0357, losses: [0.0181052777916193, 0.017599258571863174], learning rate: 0.0099000000
Val loss  tensor(1.0965)
Val loss  tensor(1.1747)
Iteration 435000: total loss 0.0373, losses: [0.018021952360868454, 0.01929079368710518], learning rate: 0.0095099005
Iteration 95000: total loss 0.0256, losses: [0.012589325197041035, 0.013050766661763191], learning rate: 0.0099000000
Iteration 440000: total loss 0.0272, losses: [0.013546559028327465, 0.013699850998818874], learning rate: 0.0095099005
Val loss  tensor(1.1862)
Val loss  tensor(1.2247)
Iteration 100000: total loss 0.0368, losses: [0.01814189925789833, 0.018624184653162956], learning rate: 0.0099000000
Val loss  tensor(1.0002)
Val loss  tensor(1.1681)
Iteration 445000: total loss 0.0342, losses: [0.016912393271923065, 0.017293760553002357], learning rate: 0.0095099005
Iteration 105000: total loss 0.0288, losses: [0.01431760098785162, 0.014460675418376923], learning rate: 0.0099000000
Iteration 450000: total loss 0.0174, losses: [0.008465605787932873, 0.008895650506019592], learning rate: 0.0095099005
Val loss  tensor(1.1610)
Val loss  tensor(1.1983)
Iteration 110000: total loss 0.0287, losses: [0.014392643235623837, 0.01426269393414259], learning rate: 0.0099000000
Val loss  tensor(1.0800)
Val loss  tensor(1.1664)
Iteration 455000: total loss 0.0347, losses: [0.01722407154738903, 0.01743352599442005], learning rate: 0.0095099005
Iteration 115000: total loss 0.0317, losses: [0.015599370002746582, 0.01605129800736904], learning rate: 0.0099000000
Iteration 460000: total loss 0.0289, losses: [0.014467013068497181, 0.014454959891736507], learning rate: 0.0095099005
Val loss  tensor(0.8269)
Val loss  tensor(1.1049)
Iteration 120000: total loss 0.0229, losses: [0.011249679140746593, 0.011616564355790615], learning rate: 0.0099000000
Val loss  tensor(1.1465)
Val loss  tensor(1.2067)
Iteration 465000: total loss 0.0229, losses: [0.011123730801045895, 0.011813684366643429], learning rate: 0.0095099005
Iteration 125000: total loss 0.0207, losses: [0.010434829629957676, 0.010271039791405201], learning rate: 0.0098010000
Iteration 470000: total loss 0.0355, losses: [0.01733897067606449, 0.018116284161806107], learning rate: 0.0095099005
Val loss  tensor(0.8815)
Val loss  tensor(1.1640)
Iteration 130000: total loss 0.0402, losses: [0.020335473120212555, 0.01985812745988369], learning rate: 0.0098010000
Val loss  tensor(1.1402)
Val loss  tensor(1.2037)
Iteration 475000: total loss 0.0287, losses: [0.01397800724953413, 0.014673586003482342], learning rate: 0.0095099005
Iteration 135000: total loss 0.0262, losses: [0.013005460612475872, 0.013200740329921246], learning rate: 0.0098010000
Iteration 480000: total loss 0.0243, losses: [0.012009835802018642, 0.012253574095666409], learning rate: 0.0095099005
Val loss  tensor(1.1917)
Val loss  tensor(1.2179)
Iteration 140000: total loss 0.0272, losses: [0.013206550851464272, 0.014028496108949184], learning rate: 0.0098010000
Val loss  tensor(0.8846)
Val loss  tensor(1.1409)
Iteration 485000: total loss 0.0259, losses: [0.012833214364945889, 0.013025052845478058], learning rate: 0.0094148015
Iteration 145000: total loss 0.0406, losses: [0.020650994032621384, 0.01996217854321003], learning rate: 0.0098010000
Iteration 490000: total loss 0.0175, losses: [0.008521668612957, 0.008969674818217754], learning rate: 0.0094148015
Val loss  tensor(0.7672)
Val loss  tensor(1.0955)
Iteration 150000: total loss 0.0245, losses: [0.012204322032630444, 0.01234176754951477], learning rate: 0.0098010000
Val loss  tensor(1.0516)
Val loss  tensor(1.1731)
Iteration 495000: total loss 0.0327, losses: [0.01642288826406002, 0.016297457739710808], learning rate: 0.0094148015
Iteration 155000: total loss 0.0295, losses: [0.014341299422085285, 0.015193231403827667], learning rate: 0.0098010000
Iteration 160000: total loss 0.0247, losses: [0.011999196372926235, 0.012711325660347939], learning rate: 0.0098010000
Val loss  tensor(0.9491)
Val loss  tensor(1.1520)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): SiLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 165000: total loss 0.0223, losses: [0.011175385676324368, 0.011135389097034931], learning rate: 0.0098010000
Iteration 170000: total loss 0.0181, losses: [0.008904718793928623, 0.009198053739964962], learning rate: 0.0098010000
Val loss  tensor(0.5208)
Val loss  tensor(1.0999)
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.6531, losses: [1.3319047689437866, 1.3212432861328125], learning rate: 0.0100000000
Val loss  tensor(1.2499)
Val loss  tensor(1.2462)
Iteration 175000: total loss 0.0375, losses: [0.018667519092559814, 0.018783865496516228], learning rate: 0.0098010000
Iteration 5000: total loss 0.1165, losses: [0.057774852961301804, 0.05876762792468071], learning rate: 0.0100000000
Iteration 180000: total loss 0.0297, losses: [0.014567282982170582, 0.015181438997387886], learning rate: 0.0098010000
Val loss  tensor(1.1025)
Val loss  tensor(1.1968)
Iteration 10000: total loss 0.0552, losses: [0.027198975905776024, 0.028022555634379387], learning rate: 0.0100000000
Val loss  tensor(0.9778)
Val loss  tensor(1.1414)
Iteration 185000: total loss 0.0360, losses: [0.01793411187827587, 0.018029367551207542], learning rate: 0.0098010000
Iteration 15000: total loss 0.0538, losses: [0.026890773326158524, 0.026939881965517998], learning rate: 0.0100000000
Iteration 190000: total loss 0.0255, losses: [0.012627675198018551, 0.012852661311626434], learning rate: 0.0098010000
Val loss  tensor(0.8660)
Val loss  tensor(1.1865)
Iteration 20000: total loss 0.0466, losses: [0.02303888462483883, 0.023535531014204025], learning rate: 0.0100000000
Val loss  tensor(0.8718)
Val loss  tensor(1.1844)
Iteration 195000: total loss 0.0379, losses: [0.018596036359667778, 0.019258566200733185], learning rate: 0.0098010000
Iteration 25000: total loss 0.0715, losses: [0.03578507527709007, 0.03566950559616089], learning rate: 0.0100000000
Iteration 200000: total loss 0.0238, losses: [0.011898514814674854, 0.011890099383890629], learning rate: 0.0098010000
Val loss  tensor(0.5963)
Val loss  tensor(1.1695)
Iteration 30000: total loss 0.0451, losses: [0.022582203149795532, 0.022482044994831085], learning rate: 0.0100000000
Val loss  tensor(0.9814)
Val loss  tensor(1.1155)
Iteration 205000: total loss 0.0259, losses: [0.0127742113545537, 0.013094305992126465], learning rate: 0.0098010000
Iteration 35000: total loss 0.0460, losses: [0.02251940779387951, 0.02349662594497204], learning rate: 0.0100000000
Iteration 210000: total loss 0.0352, losses: [0.01742582395672798, 0.01774602197110653], learning rate: 0.0098010000
Val loss  tensor(0.8372)
Val loss  tensor(1.1333)
Iteration 40000: total loss 0.0442, losses: [0.02233835868537426, 0.021903429180383682], learning rate: 0.0100000000
Val loss  tensor(0.8823)
Val loss  tensor(1.1069)
Iteration 215000: total loss 0.0262, losses: [0.013120735064148903, 0.013117763213813305], learning rate: 0.0098010000
Iteration 45000: total loss 0.0293, losses: [0.014859303832054138, 0.014432340860366821], learning rate: 0.0100000000
Iteration 220000: total loss 0.0206, losses: [0.010055694729089737, 0.010539181530475616], learning rate: 0.0098010000
Val loss  tensor(0.7747)
Val loss  tensor(1.1583)
Iteration 50000: total loss 0.0672, losses: [0.03278147801756859, 0.03438530117273331], learning rate: 0.0100000000
Val loss  tensor(1.2048)
Val loss  tensor(1.2815)
Iteration 225000: total loss 0.0313, losses: [0.015445679426193237, 0.015878736972808838], learning rate: 0.0097029900
Iteration 55000: total loss 0.0361, losses: [0.017943520098924637, 0.018108893185853958], learning rate: 0.0100000000
Iteration 230000: total loss 0.0267, losses: [0.01320000272244215, 0.013473543338477612], learning rate: 0.0097029900
Val loss  tensor(1.1873)
Val loss  tensor(1.2058)
Iteration 60000: total loss 0.0483, losses: [0.02441359870135784, 0.02386411838233471], learning rate: 0.0100000000
Val loss  tensor(0.9042)
Val loss  tensor(1.0873)
Iteration 235000: total loss 0.0182, losses: [0.008987515233457088, 0.009233280085027218], learning rate: 0.0097029900
Iteration 65000: total loss 0.0398, losses: [0.019721761345863342, 0.02011076547205448], learning rate: 0.0100000000
Iteration 240000: total loss 0.0156, losses: [0.007746000774204731, 0.0078062936663627625], learning rate: 0.0097029900
Val loss  tensor(1.0559)
Val loss  tensor(1.1819)
Iteration 70000: total loss 0.0256, losses: [0.012645581737160683, 0.01300260704010725], learning rate: 0.0100000000
Val loss  tensor(0.8356)
Val loss  tensor(1.0918)
Iteration 245000: total loss 0.0313, losses: [0.015571541152894497, 0.015768814831972122], learning rate: 0.0097029900
Iteration 75000: total loss 0.0432, losses: [0.021931281313300133, 0.02129608951508999], learning rate: 0.0100000000
Iteration 250000: total loss 0.0165, losses: [0.008287963457405567, 0.008170775137841702], learning rate: 0.0097029900
Val loss  tensor(0.9895)
Val loss  tensor(1.1791)
Iteration 80000: total loss 0.0653, losses: [0.032200369983911514, 0.033146824687719345], learning rate: 0.0100000000
Val loss  tensor(1.0003)
Val loss  tensor(1.1259)
Iteration 255000: total loss 0.0185, losses: [0.009143656119704247, 0.009364785626530647], learning rate: 0.0097029900
Iteration 85000: total loss 0.0287, losses: [0.014041918329894543, 0.014617768116295338], learning rate: 0.0100000000
Iteration 260000: total loss 0.0352, losses: [0.017519410699605942, 0.01770239882171154], learning rate: 0.0097029900
Val loss  tensor(1.2208)
Val loss  tensor(1.2388)
Iteration 90000: total loss 0.0361, losses: [0.01772032491862774, 0.01840013638138771], learning rate: 0.0100000000
Val loss  tensor(0.9792)
Val loss  tensor(1.1068)
Iteration 265000: total loss 0.0196, losses: [0.009760739281773567, 0.00988133903592825], learning rate: 0.0097029900
Iteration 95000: total loss 0.0261, losses: [0.012636187486350536, 0.013434785418212414], learning rate: 0.0100000000
Iteration 270000: total loss 0.0325, losses: [0.01624983549118042, 0.016255658119916916], learning rate: 0.0097029900
Val loss  tensor(0.6177)
Val loss  tensor(1.1161)
Iteration 100000: total loss 0.0272, losses: [0.013420823030173779, 0.013784280978143215], learning rate: 0.0100000000
Val loss  tensor(0.8939)
Val loss  tensor(1.0824)
Iteration 275000: total loss 0.0176, losses: [0.00851230975240469, 0.009080417454242706], learning rate: 0.0097029900
Iteration 105000: total loss 0.0267, losses: [0.013236591592431068, 0.01346424501389265], learning rate: 0.0100000000
Iteration 280000: total loss 0.0175, losses: [0.008809152990579605, 0.008653579279780388], learning rate: 0.0097029900
Val loss  tensor(0.9003)
Val loss  tensor(1.1218)
Iteration 110000: total loss 0.0435, losses: [0.021990496665239334, 0.021534210070967674], learning rate: 0.0100000000
Val loss  tensor(1.0458)
Val loss  tensor(1.1173)
Iteration 285000: total loss 0.0296, losses: [0.014909875579178333, 0.0146608492359519], learning rate: 0.0096059601
Iteration 115000: total loss 0.0501, losses: [0.024474456906318665, 0.025592220947146416], learning rate: 0.0100000000
Iteration 290000: total loss 0.0279, losses: [0.013793349266052246, 0.014059257693588734], learning rate: 0.0096059601
Val loss  tensor(1.0183)
Val loss  tensor(1.1582)
Iteration 120000: total loss 0.0336, losses: [0.016322491690516472, 0.017240287736058235], learning rate: 0.0100000000
Val loss  tensor(1.0516)
Val loss  tensor(1.1477)
Iteration 295000: total loss 0.0320, losses: [0.01562594063580036, 0.016344230622053146], learning rate: 0.0096059601
Iteration 125000: total loss 0.0425, losses: [0.020939374342560768, 0.02159491740167141], learning rate: 0.0099000000
Iteration 300000: total loss 0.0276, losses: [0.01378489751368761, 0.013812759891152382], learning rate: 0.0096059601
Val loss  tensor(0.8367)
Val loss  tensor(1.0896)
Iteration 130000: total loss 0.0238, losses: [0.011491178534924984, 0.012325302697718143], learning rate: 0.0099000000
Val loss  tensor(0.9743)
Val loss  tensor(1.0864)
Iteration 305000: total loss 0.0333, losses: [0.016510767862200737, 0.016742218285799026], learning rate: 0.0096059601
Iteration 135000: total loss 0.0301, losses: [0.014742943458259106, 0.015320384874939919], learning rate: 0.0099000000
Iteration 310000: total loss 0.0260, losses: [0.012808374129235744, 0.01319048274308443], learning rate: 0.0096059601
Val loss  tensor(0.9944)
Val loss  tensor(1.1318)
Iteration 140000: total loss 0.0323, losses: [0.016058890148997307, 0.016288109123706818], learning rate: 0.0099000000
Val loss  tensor(0.7755)
Val loss  tensor(1.0800)
Iteration 315000: total loss 0.0383, losses: [0.018789248540997505, 0.019475502893328667], learning rate: 0.0096059601
Iteration 145000: total loss 0.0241, losses: [0.011727117002010345, 0.012370769865810871], learning rate: 0.0099000000
Iteration 320000: total loss 0.0302, losses: [0.014904148876667023, 0.01531568169593811], learning rate: 0.0096059601
Val loss  tensor(1.0568)
Val loss  tensor(1.1824)
Iteration 150000: total loss 0.0337, losses: [0.016555849462747574, 0.017128419131040573], learning rate: 0.0099000000
Val loss  tensor(0.6876)
Val loss  tensor(1.0879)
Iteration 325000: total loss 0.0345, losses: [0.01708858273923397, 0.017412981018424034], learning rate: 0.0096059601
Iteration 155000: total loss 0.0406, losses: [0.020189089700579643, 0.020417621359229088], learning rate: 0.0099000000
Iteration 330000: total loss 0.0287, losses: [0.014242458157241344, 0.014466479420661926], learning rate: 0.0096059601
Val loss  tensor(0.8956)
Val loss  tensor(1.0788)
Iteration 160000: total loss 0.0347, losses: [0.01720397174358368, 0.01752186007797718], learning rate: 0.0099000000
Val loss  tensor(0.8232)
Val loss  tensor(1.1357)
Iteration 335000: total loss 0.0261, losses: [0.013016200624406338, 0.013044971041381359], learning rate: 0.0095099005
Iteration 165000: total loss 0.0334, losses: [0.016643444076180458, 0.01672714576125145], learning rate: 0.0099000000
Iteration 340000: total loss 0.0299, losses: [0.014890638180077076, 0.015055310912430286], learning rate: 0.0095099005
Val loss  tensor(0.9263)
Val loss  tensor(1.1616)
Iteration 170000: total loss 0.0293, losses: [0.014343044720590115, 0.01491845678538084], learning rate: 0.0099000000
Val loss  tensor(0.9904)
Val loss  tensor(1.1289)
Iteration 345000: total loss 0.0191, losses: [0.009418107569217682, 0.00964654516428709], learning rate: 0.0095099005
Iteration 175000: total loss 0.0325, losses: [0.016310151666402817, 0.016164680942893028], learning rate: 0.0099000000
Iteration 350000: total loss 0.0260, losses: [0.012819346971809864, 0.0131332753226161], learning rate: 0.0095099005
Val loss  tensor(0.9361)
Val loss  tensor(1.1542)
Iteration 180000: total loss 0.0348, losses: [0.017347387969493866, 0.017431173473596573], learning rate: 0.0099000000
Val loss  tensor(1.1300)
Val loss  tensor(1.1655)
Iteration 355000: total loss 0.0332, losses: [0.01677205227315426, 0.01643366925418377], learning rate: 0.0095099005
Iteration 185000: total loss 0.0297, losses: [0.014682997949421406, 0.015019389800727367], learning rate: 0.0099000000
Iteration 360000: total loss 0.0238, losses: [0.011941122822463512, 0.011866527609527111], learning rate: 0.0095099005
Val loss  tensor(0.6907)
Val loss  tensor(1.0776)
Iteration 190000: total loss 0.0394, losses: [0.019574662670493126, 0.01986265927553177], learning rate: 0.0099000000
Val loss  tensor(0.9151)
Val loss  tensor(1.1260)
Iteration 365000: total loss 0.0151, losses: [0.007510086987167597, 0.007617366965860128], learning rate: 0.0095099005
Iteration 195000: total loss 0.0306, losses: [0.014935287646949291, 0.015626564621925354], learning rate: 0.0099000000
Iteration 370000: total loss 0.0173, losses: [0.008463713340461254, 0.008809189312160015], learning rate: 0.0095099005
Val loss  tensor(0.5782)
Val loss  tensor(1.1083)
Iteration 200000: total loss 0.0211, losses: [0.010669062845408916, 0.010404173284769058], learning rate: 0.0099000000
Val loss  tensor(1.0047)
Val loss  tensor(1.1138)
Iteration 375000: total loss 0.0214, losses: [0.01068801712244749, 0.010721380822360516], learning rate: 0.0095099005
Iteration 205000: total loss 0.0377, losses: [0.018640123307704926, 0.01904168538749218], learning rate: 0.0098010000
Iteration 380000: total loss 0.0256, losses: [0.012903827242553234, 0.012658419087529182], learning rate: 0.0095099005
Val loss  tensor(0.9876)
Val loss  tensor(1.1713)
Iteration 210000: total loss 0.0417, losses: [0.02091073803603649, 0.020791780203580856], learning rate: 0.0098010000
Val loss  tensor(0.9262)
Val loss  tensor(1.0952)
Iteration 385000: total loss 0.0258, losses: [0.012881000526249409, 0.01294657588005066], learning rate: 0.0095099005
Iteration 215000: total loss 0.0344, losses: [0.016881028190255165, 0.017523398622870445], learning rate: 0.0098010000
Iteration 390000: total loss 0.0246, losses: [0.01194601971656084, 0.012677982449531555], learning rate: 0.0095099005
Val loss  tensor(0.9279)
Val loss  tensor(1.1152)
Iteration 220000: total loss 0.0346, losses: [0.01726974919438362, 0.017300350591540337], learning rate: 0.0098010000
Val loss  tensor(0.7173)
Val loss  tensor(1.1120)
Iteration 395000: total loss 0.0295, losses: [0.014578469097614288, 0.014884981326758862], learning rate: 0.0094148015
Iteration 225000: total loss 0.0352, losses: [0.017750371247529984, 0.017487308010458946], learning rate: 0.0098010000
Iteration 400000: total loss 0.0224, losses: [0.011184385977685452, 0.011264395900070667], learning rate: 0.0094148015
Val loss  tensor(0.8556)
Val loss  tensor(1.0823)
Iteration 230000: total loss 0.0261, losses: [0.01306734699755907, 0.013077306561172009], learning rate: 0.0098010000
Val loss  tensor(0.9881)
Val loss  tensor(1.1393)
Iteration 405000: total loss 0.0212, losses: [0.010818522423505783, 0.010426453314721584], learning rate: 0.0094148015
Iteration 235000: total loss 0.0376, losses: [0.01833147369325161, 0.019310234114527702], learning rate: 0.0098010000
Iteration 410000: total loss 0.0308, losses: [0.015271115116775036, 0.015514150261878967], learning rate: 0.0094148015
Val loss  tensor(0.9523)
Val loss  tensor(1.1741)
Iteration 240000: total loss 0.0190, losses: [0.009380551055073738, 0.009616508148610592], learning rate: 0.0098010000
Val loss  tensor(0.6350)
Val loss  tensor(1.1216)
Iteration 415000: total loss 0.0253, losses: [0.012510371394455433, 0.01280283648520708], learning rate: 0.0094148015
Iteration 245000: total loss 0.0329, losses: [0.01619984768331051, 0.01665680669248104], learning rate: 0.0098010000
Iteration 420000: total loss 0.0162, losses: [0.00792868621647358, 0.008229026570916176], learning rate: 0.0094148015
Val loss  tensor(1.1250)
Val loss  tensor(1.2257)
Iteration 250000: total loss 0.0342, losses: [0.01699693500995636, 0.017200857400894165], learning rate: 0.0098010000
Val loss  tensor(0.8600)
Val loss  tensor(1.1501)
Iteration 425000: total loss 0.0151, losses: [0.007396818138659, 0.00770854065194726], learning rate: 0.0094148015
Iteration 255000: total loss 0.0293, losses: [0.014523020945489407, 0.014800178818404675], learning rate: 0.0098010000
Iteration 430000: total loss 0.0242, losses: [0.01219113077968359, 0.01196055393666029], learning rate: 0.0094148015
Val loss  tensor(1.0896)
Val loss  tensor(1.2135)
Iteration 260000: total loss 0.0423, losses: [0.020717963576316833, 0.021577943116426468], learning rate: 0.0098010000
Val loss  tensor(0.8064)
Val loss  tensor(1.1317)
Iteration 435000: total loss 0.0148, losses: [0.007411949336528778, 0.007346452679485083], learning rate: 0.0094148015
Iteration 265000: total loss 0.0313, losses: [0.015750950202345848, 0.01553528755903244], learning rate: 0.0098010000
Iteration 440000: total loss 0.0151, losses: [0.007226733025163412, 0.0078957574442029], learning rate: 0.0094148015
Val loss  tensor(0.8775)
Val loss  tensor(1.1556)
Iteration 270000: total loss 0.0394, losses: [0.019739389419555664, 0.019677912816405296], learning rate: 0.0098010000
Val loss  tensor(0.7274)
Val loss  tensor(1.0840)
Iteration 445000: total loss 0.0200, losses: [0.009691322222352028, 0.010265596210956573], learning rate: 0.0093206535
Iteration 275000: total loss 0.0347, losses: [0.01737057976424694, 0.017373040318489075], learning rate: 0.0098010000
Iteration 450000: total loss 0.0271, losses: [0.013563034124672413, 0.013580865226686], learning rate: 0.0093206535
Val loss  tensor(1.0669)
Val loss  tensor(1.1874)
Iteration 280000: total loss 0.0267, losses: [0.013142178766429424, 0.013530428521335125], learning rate: 0.0098010000
Val loss  tensor(0.8484)
Val loss  tensor(1.1092)
Iteration 455000: total loss 0.0175, losses: [0.00888961274176836, 0.008618496358394623], learning rate: 0.0093206535
Iteration 285000: total loss 0.0338, losses: [0.016992392018437386, 0.016813648864626884], learning rate: 0.0098010000
Iteration 460000: total loss 0.0141, losses: [0.006955582182854414, 0.007187578361481428], learning rate: 0.0093206535
Val loss  tensor(0.6966)
Val loss  tensor(1.1403)
Iteration 290000: total loss 0.2635, losses: [0.13379733264446259, 0.12968702614307404], learning rate: 0.0098010000
Val loss  tensor(3.5475)
Val loss  tensor(3.9384)
Iteration 465000: total loss 0.0173, losses: [0.008584978058934212, 0.008758007548749447], learning rate: 0.0093206535
Iteration 295000: total loss 0.0282, losses: [0.013790622353553772, 0.014424866996705532], learning rate: 0.0097029900
Iteration 470000: total loss 0.0131, losses: [0.0066004106774926186, 0.006532347295433283], learning rate: 0.0093206535
Val loss  tensor(0.7163)
Val loss  tensor(1.1583)
Iteration 300000: total loss 0.0362, losses: [0.017954392358660698, 0.018287722021341324], learning rate: 0.0097029900
Val loss  tensor(0.9024)
Val loss  tensor(1.1223)
Iteration 475000: total loss 0.0151, losses: [0.007436296436935663, 0.007690039929002523], learning rate: 0.0093206535
Iteration 305000: total loss 0.0299, losses: [0.015041196718811989, 0.014868724159896374], learning rate: 0.0097029900
Iteration 480000: total loss 0.0129, losses: [0.006082264240831137, 0.006838233210146427], learning rate: 0.0093206535
Val loss  tensor(1.2093)
Val loss  tensor(1.2248)
Iteration 310000: total loss 0.0307, losses: [0.015432538464665413, 0.015286914072930813], learning rate: 0.0097029900
Val loss  tensor(0.7687)
Val loss  tensor(1.1157)
Iteration 485000: total loss 0.0202, losses: [0.010058640502393246, 0.010148732922971249], learning rate: 0.0093206535
Iteration 315000: total loss 0.0235, losses: [0.011469922959804535, 0.012075475417077541], learning rate: 0.0097029900
Iteration 490000: total loss 0.0325, losses: [0.01587667129933834, 0.016665687784552574], learning rate: 0.0093206535
Val loss  tensor(1.0089)
Val loss  tensor(1.1588)
Iteration 320000: total loss 0.0242, losses: [0.0120546855032444, 0.01214550994336605], learning rate: 0.0097029900
Val loss  tensor(0.9183)
Val loss  tensor(1.0909)
Iteration 495000: total loss 0.0202, losses: [0.009936006739735603, 0.010305128060281277], learning rate: 0.0093206535
Iteration 325000: total loss 0.0362, losses: [0.018239891156554222, 0.01794365607202053], learning rate: 0.0097029900
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Iteration 330000: total loss 0.0249, losses: [0.012685929425060749, 0.01226118765771389], learning rate: 0.0097029900
Val loss  tensor(1.0188)
Val loss  tensor(1.1238)
Iteration 335000: total loss 0.0167, losses: [0.00819889921694994, 0.008462297730147839], learning rate: 0.0097029900
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.5513, losses: [1.2795727252960205, 1.2716933488845825], learning rate: 0.0100000000
Val loss  tensor(3.1082)
Iteration 340000: total loss 0.0320, losses: [0.015543937683105469, 0.016465459018945694], learning rate: 0.0097029900
Val loss  tensor(0.8846)
Val loss  tensor(1.1187)
Val loss  tensor(2.9772)
Iteration 345000: total loss 0.0248, losses: [0.012117743492126465, 0.012661839835345745], learning rate: 0.0097029900
Iteration 5000: total loss 0.0837, losses: [0.040381986647844315, 0.04336424544453621], learning rate: 0.0100000000
Iteration 350000: total loss 0.0270, losses: [0.013389441184699535, 0.013641643337905407], learning rate: 0.0097029900
Val loss  tensor(0.8737)
Val loss  tensor(1.1066)
Iteration 10000: total loss 0.0812, losses: [0.03867688775062561, 0.042521990835666656], learning rate: 0.0100000000
Val loss  tensor(1.1765)
Val loss  tensor(1.1973)
Iteration 355000: total loss 0.0278, losses: [0.014203192666172981, 0.013561869971454144], learning rate: 0.0096059601
Iteration 360000: total loss 0.0281, losses: [0.013837556354701519, 0.014306812547147274], learning rate: 0.0096059601
Val loss  tensor(0.9514)
Iteration 15000: total loss 0.1098, losses: [0.05225427821278572, 0.05758190155029297], learning rate: 0.0100000000
Val loss  tensor(1.1058)
Iteration 365000: total loss 0.0205, losses: [0.010169575922191143, 0.010293620638549328], learning rate: 0.0096059601
Iteration 20000: total loss 0.0843, losses: [0.04102778062224388, 0.04327232390642166], learning rate: 0.0100000000
Val loss  tensor(1.1236)
Val loss  tensor(1.1960)
Iteration 370000: total loss 0.0243, losses: [0.012227865867316723, 0.012092046439647675], learning rate: 0.0096059601
Val loss  tensor(0.6903)
Val loss  tensor(1.1189)
Iteration 25000: total loss 0.0633, losses: [0.03144579753279686, 0.031834449619054794], learning rate: 0.0100000000
Iteration 375000: total loss 0.0237, losses: [0.011657938361167908, 0.012017703615128994], learning rate: 0.0096059601
Iteration 380000: total loss 0.0244, losses: [0.012111387215554714, 0.012325502000749111], learning rate: 0.0096059601
Val loss  tensor(0.8376)
Iteration 30000: total loss 0.0548, losses: [0.026376236230134964, 0.028401225805282593], learning rate: 0.0100000000
Val loss  tensor(1.0984)
Val loss  tensor(1.1076)
Val loss  tensor(1.2093)
Iteration 385000: total loss 0.0231, losses: [0.011374776251614094, 0.0117572620511055], learning rate: 0.0096059601
Iteration 35000: total loss 0.1097, losses: [0.05379733070731163, 0.05587254837155342], learning rate: 0.0100000000
Iteration 390000: total loss 0.0193, losses: [0.009488644078373909, 0.009763351641595364], learning rate: 0.0096059601
Val loss  tensor(0.7876)
Val loss  tensor(1.1335)
Iteration 40000: total loss 0.0608, losses: [0.02862735465168953, 0.032144542783498764], learning rate: 0.0100000000
Val loss  tensor(1.0931)
Val loss  tensor(1.2062)
Iteration 395000: total loss 0.0149, losses: [0.007424124516546726, 0.007504254579544067], learning rate: 0.0096059601
Iteration 400000: total loss 0.0175, losses: [0.008837947621941566, 0.00862935371696949], learning rate: 0.0096059601
Val loss  tensor(0.6563)
Val loss  tensor(1.1198)
Iteration 45000: total loss 0.0601, losses: [0.029730921611189842, 0.03035452961921692], learning rate: 0.0100000000
Iteration 405000: total loss 0.0187, losses: [0.009319772943854332, 0.009336039423942566], learning rate: 0.0095099005
Iteration 50000: total loss 0.0798, losses: [0.03947111591696739, 0.04037052392959595], learning rate: 0.0100000000
Val loss  tensor(1.1343)
Val loss  tensor(1.1946)
Iteration 410000: total loss 0.0210, losses: [0.010126731358468533, 0.01090872474014759], learning rate: 0.0095099005
Val loss  tensor(0.8007)
Val loss  tensor(1.1187)
Iteration 55000: total loss 0.0439, losses: [0.02158363349735737, 0.0223112553358078], learning rate: 0.0100000000
Iteration 415000: total loss 0.0270, losses: [0.013335645198822021, 0.013690577819943428], learning rate: 0.0095099005
Iteration 420000: total loss 0.0238, losses: [0.011975971050560474, 0.01185619831085205], learning rate: 0.0095099005
Val loss  tensor(0.9203)
Val loss  tensor(1.1285)
Iteration 60000: total loss 0.0507, losses: [0.024547461420297623, 0.02614513598382473], learning rate: 0.0100000000
Val loss  tensor(1.1310)
Val loss  tensor(1.1992)
Iteration 425000: total loss 0.0176, losses: [0.008481278084218502, 0.009092704392969608], learning rate: 0.0095099005
Iteration 65000: total loss 0.0418, losses: [0.02002890594303608, 0.02180466242134571], learning rate: 0.0100000000
Iteration 430000: total loss 0.0234, losses: [0.011906038969755173, 0.011541876941919327], learning rate: 0.0095099005
Val loss  tensor(0.5667)
Val loss  tensor(1.1335)
Iteration 70000: total loss 0.0776, losses: [0.03843225911259651, 0.039157282561063766], learning rate: 0.0100000000
Val loss  tensor(1.1722)
Iteration 435000: total loss 0.0174, losses: [0.008558548055589199, 0.008834857493638992], learning rate: 0.0095099005
Val loss  tensor(1.1944)
Iteration 440000: total loss 0.0194, losses: [0.009750455617904663, 0.009632407687604427], learning rate: 0.0095099005
Val loss  tensor(0.9156)
Val loss  tensor(1.1224)
Iteration 75000: total loss 0.0821, losses: [0.0403740294277668, 0.0417214035987854], learning rate: 0.0100000000
Iteration 445000: total loss 0.0229, losses: [0.011435364373028278, 0.01144217886030674], learning rate: 0.0095099005
Iteration 80000: total loss 0.0603, losses: [0.029498416930437088, 0.030825240537524223], learning rate: 0.0100000000
Val loss  tensor(1.0817)
Val loss  tensor(1.1611)
Iteration 450000: total loss 0.0163, losses: [0.007952781394124031, 0.008341369219124317], learning rate: 0.0095099005
Val loss  tensor(0.6012)
Val loss  tensor(1.1339)
Iteration 455000: total loss 0.0220, losses: [0.011025636456906796, 0.011016192846000195], learning rate: 0.0095099005
Iteration 85000: total loss 0.0799, losses: [0.038589250296354294, 0.041341908276081085], learning rate: 0.0100000000
Iteration 460000: total loss 0.0211, losses: [0.010600397363305092, 0.010540793649852276], learning rate: 0.0095099005
Val loss  tensor(0.7957)
Val loss  tensor(1.1238)
Iteration 90000: total loss 0.0552, losses: [0.027001312002539635, 0.028212839737534523], learning rate: 0.0100000000
Val loss  tensor(1.1754)
Val loss  tensor(1.1933)
Iteration 465000: total loss 0.0246, losses: [0.01221524365246296, 0.012413534335792065], learning rate: 0.0095099005
Iteration 95000: total loss 0.0462, losses: [0.02234838344156742, 0.023818230256438255], learning rate: 0.0100000000
Iteration 470000: total loss 0.0222, losses: [0.0108351930975914, 0.011374153196811676], learning rate: 0.0095099005
Val loss  tensor(0.7462)
Val loss  tensor(1.1402)
Iteration 100000: total loss 0.0467, losses: [0.022661367431282997, 0.023992231115698814], learning rate: 0.0100000000
Iteration 475000: total loss 0.0270, losses: [0.013548490591347218, 0.013409726321697235], learning rate: 0.0095099005
Val loss  tensor(1.0999)
Val loss  tensor(1.1750)
Iteration 480000: total loss 0.0148, losses: [0.007382494397461414, 0.007400337606668472], learning rate: 0.0095099005
Val loss  tensor(1.0724)
Val loss  tensor(1.1670)
Iteration 105000: total loss 0.0430, losses: [0.020904812961816788, 0.022073691710829735], learning rate: 0.0100000000
Iteration 485000: total loss 0.0275, losses: [0.013910307548940182, 0.013543285429477692], learning rate: 0.0094148015
Iteration 110000: total loss 0.0417, losses: [0.020214661955833435, 0.021472090855240822], learning rate: 0.0100000000
Val loss  tensor(1.1397)
Val loss  tensor(1.1936)
Iteration 490000: total loss 0.0256, losses: [0.012639576569199562, 0.012918203137814999], learning rate: 0.0094148015
Val loss  tensor(0.7671)
Val loss  tensor(1.1483)
Iteration 495000: total loss 0.0196, losses: [0.009817793034017086, 0.009742232970893383], learning rate: 0.0094148015
Iteration 115000: total loss 0.0376, losses: [0.018397867679595947, 0.01918848417699337], learning rate: 0.0100000000
Iteration 120000: total loss 0.0504, losses: [0.0245978981256485, 0.02578042820096016], learning rate: 0.0100000000
Val loss  tensor(1.1306)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=128, bias=True)
    (1): SiLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): SiLU()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): SiLU()
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
cuda:0
Folder already there
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Val loss  tensor(1.2074)
Iteration 125000: total loss 0.0402, losses: [0.018885040655732155, 0.02135070599615574], learning rate: 0.0100000000
(2505000,)
(2505000,)
(2505000,)
(2505000,)
(26109114,)
(26109114,)
(26109114,)
(26109114,)
Iteration 0: total loss 2.4949, losses: [1.2559659481048584, 1.2389177083969116], learning rate: 0.0100000000
Val loss  tensor(1.2087)
Val loss  tensor(1.1312)
Iteration 130000: total loss 0.0713, losses: [0.035125620663166046, 0.03613516315817833], learning rate: 0.0100000000
Val loss  tensor(1.1335)
Val loss  tensor(1.2044)
Iteration 5000: total loss 0.0978, losses: [0.047873880714178085, 0.04992426931858063], learning rate: 0.0100000000
Iteration 10000: total loss 0.0909, losses: [0.0448923259973526, 0.04603161662817001], learning rate: 0.0100000000
Val loss  tensor(1.0676)
Val loss  tensor(1.1211)
Iteration 135000: total loss 0.0446, losses: [0.020696241408586502, 0.023869087919592857], learning rate: 0.0099000000
Iteration 15000: total loss 0.0614, losses: [0.03070320188999176, 0.030718374997377396], learning rate: 0.0100000000
Iteration 140000: total loss 0.0410, losses: [0.02025887742638588, 0.02072151005268097], learning rate: 0.0099000000
Val loss  tensor(1.1471)
Val loss  tensor(1.2067)
Iteration 20000: total loss 0.0587, losses: [0.0288452859967947, 0.029874667525291443], learning rate: 0.0100000000
Val loss  tensor(1.0362)
Val loss  tensor(1.0982)
Iteration 145000: total loss 0.0385, losses: [0.01864154450595379, 0.019821632653474808], learning rate: 0.0099000000
Iteration 25000: total loss 0.0545, losses: [0.027091750875115395, 0.027365965768694878], learning rate: 0.0100000000
Iteration 30000: total loss 1.1326, losses: [0.5652481317520142, 0.567371666431427], learning rate: 0.0100000000
Val loss  tensor(1.0335)
Val loss  tensor(1.0105)
Iteration 150000: total loss 0.0411, losses: [0.01974809542298317, 0.02137323096394539], learning rate: 0.0099000000
Val loss  tensor(1.1150)
Val loss  tensor(1.2158)
Iteration 35000: total loss 1.1332, losses: [0.5661270618438721, 0.5670612454414368], learning rate: 0.0100000000
Iteration 155000: total loss 0.0804, losses: [0.039161838591098785, 0.041231896728277206], learning rate: 0.0099000000
Iteration 40000: total loss 1.1362, losses: [0.5681249499320984, 0.568081796169281], learning rate: 0.0100000000
Val loss  tensor(1.0713)
Val loss  tensor(1.0484)
Iteration 160000: total loss 0.0759, losses: [0.03704533725976944, 0.03886466100811958], learning rate: 0.0099000000
Val loss  tensor(1.1288)
Iteration 45000: total loss 1.1343, losses: [0.5661689043045044, 0.5680943727493286], learning rate: 0.0100000000
Val loss  tensor(1.1899)
Iteration 50000: total loss 1.1356, losses: [0.5669645667076111, 0.5686202645301819], learning rate: 0.0100000000
Val loss  tensor(0.9441)
Val loss  tensor(0.9212)
Iteration 165000: total loss 0.0421, losses: [0.020534122362732887, 0.0215456560254097], learning rate: 0.0099000000
Iteration 55000: total loss 1.1339, losses: [0.5637014508247375, 0.5701590776443481], learning rate: 0.0100000000
Iteration 170000: total loss 0.0361, losses: [0.017121464014053345, 0.01895184814929962], learning rate: 0.0099000000
Val loss  tensor(1.1742)
Val loss  tensor(1.2143)
Iteration 60000: total loss 1.1374, losses: [0.5685672760009766, 0.5688058733940125], learning rate: 0.0100000000
Val loss  tensor(0.9773)
Val loss  tensor(0.9544)
Iteration 65000: total loss 1.1297, losses: [0.5639054179191589, 0.565796971321106], learning rate: 0.0100000000
Iteration 175000: total loss 0.0386, losses: [0.018922170624136925, 0.01969318650662899], learning rate: 0.0099000000
Iteration 70000: total loss 1.1300, losses: [0.5650316476821899, 0.5649621486663818], learning rate: 0.0100000000
Val loss  tensor(1.0368)
Val loss  tensor(1.0139)
Iteration 180000: total loss 0.0348, losses: [0.01662312261760235, 0.018186448141932487], learning rate: 0.0099000000
Val loss  tensor(1.1806)
Val loss  tensor(1.2121)
Iteration 75000: total loss 1.1273, losses: [0.5640343427658081, 0.5632918477058411], learning rate: 0.0100000000
Iteration 185000: total loss 0.0866, losses: [0.042823608964681625, 0.04372831806540489], learning rate: 0.0099000000
Iteration 80000: total loss 1.1335, losses: [0.5679009556770325, 0.5655543804168701], learning rate: 0.0100000000
Val loss  tensor(1.0392)
Val loss  tensor(1.0163)
Iteration 85000: total loss 1.1322, losses: [0.5632038116455078, 0.5690028071403503], learning rate: 0.0100000000
Iteration 190000: total loss 0.0660, losses: [0.03143885359168053, 0.03460611030459404], learning rate: 0.0099000000
Val loss  tensor(1.1132)
Val loss  tensor(1.1963)
Iteration 90000: total loss 1.1331, losses: [0.5642735362052917, 0.5688463449478149], learning rate: 0.0100000000
Val loss  tensor(1.0239)
Val loss  tensor(1.0010)
Iteration 195000: total loss 0.0445, losses: [0.022405732423067093, 0.022078637033700943], learning rate: 0.0098010000
Iteration 95000: total loss 1.1319, losses: [0.566535472869873, 0.5653994083404541], learning rate: 0.0100000000
Iteration 100000: total loss 1.1314, losses: [0.5666435360908508, 0.5647401809692383], learning rate: 0.0100000000
Iteration 200000: total loss 0.0370, losses: [0.01732502691447735, 0.019676530733704567], learning rate: 0.0098010000
Val loss  tensor(1.0728)
Val loss  tensor(1.1565)
Val loss  tensor(1.0499)
Val loss  tensor(1.2168)
Iteration 105000: total loss 1.1303, losses: [0.5643144845962524, 0.5659577250480652], learning rate: 0.0100000000
Iteration 205000: total loss 0.0368, losses: [0.01761452853679657, 0.019142119213938713], learning rate: 0.0098010000
Iteration 110000: total loss 1.1340, losses: [0.5664328932762146, 0.5675298571586609], learning rate: 0.0100000000
Val loss  tensor(1.0113)
Val loss  tensor(0.9884)
Iteration 210000: total loss 0.0379, losses: [0.017870699986815453, 0.02002723701298237], learning rate: 0.0098010000
Val loss  tensor(1.0676)
Val loss  tensor(1.2127)
Iteration 115000: total loss 1.1316, losses: [0.5652704238891602, 0.566323459148407], learning rate: 0.0099000000
Iteration 120000: total loss 1.1289, losses: [0.5632104873657227, 0.565700352191925], learning rate: 0.0099000000
Val loss  tensor(1.0596)
Val loss  tensor(1.0367)
Iteration 215000: total loss 0.0454, losses: [0.021922560408711433, 0.023526465520262718], learning rate: 0.0098010000
Iteration 125000: total loss 1.1338, losses: [0.5669751763343811, 0.566804826259613], learning rate: 0.0099000000
Iteration 220000: total loss 0.0396, losses: [0.019022120162844658, 0.02056986838579178], learning rate: 0.0098010000
Val loss  tensor(1.1558)
Val loss  tensor(1.2029)
Iteration 130000: total loss 1.1367, losses: [0.5680838823318481, 0.568580150604248], learning rate: 0.0099000000
Val loss  tensor(1.0171)
Val loss  tensor(0.9942)
Iteration 225000: total loss 0.0552, losses: [0.02767302095890045, 0.027489501982927322], learning rate: 0.0098010000
Iteration 135000: total loss 1.1349, losses: [0.5691928267478943, 0.565707802772522], learning rate: 0.0099000000
Iteration 140000: total loss 1.1303, losses: [0.5664972066879272, 0.5637612342834473], learning rate: 0.0099000000
Val loss  tensor(1.0954)
Val loss  tensor(1.0725)
Iteration 230000: total loss 0.0358, losses: [0.017776092514395714, 0.017997032031416893], learning rate: 0.0098010000
Val loss  tensor(1.1047)
Val loss  tensor(1.2152)
Iteration 145000: total loss 1.1320, losses: [0.5660980343818665, 0.5658589601516724], learning rate: 0.0099000000
Iteration 235000: total loss 0.0421, losses: [0.020325785502791405, 0.02179202251136303], learning rate: 0.0098010000
Iteration 150000: total loss 1.1571, losses: [0.5779165029525757, 0.5792299509048462], learning rate: 0.0099000000
Val loss  tensor(1.0101)
Val loss  tensor(0.9871)
Iteration 155000: total loss 1.1333, losses: [0.5670105218887329, 0.5663313865661621], learning rate: 0.0099000000
Iteration 240000: total loss 0.0338, losses: [0.016225123777985573, 0.017562294378876686], learning rate: 0.0098010000
Val loss  tensor(1.1669)
Val loss  tensor(1.2090)
Iteration 160000: total loss 1.1347, losses: [0.5679505467414856, 0.5667001008987427], learning rate: 0.0099000000
Val loss  tensor(1.0366)
Val loss  tensor(1.0137)
Iteration 245000: total loss 0.0308, losses: [0.014644152484834194, 0.016137611120939255], learning rate: 0.0098010000
Iteration 165000: total loss 1.1348, losses: [0.5653815865516663, 0.5694593191146851], learning rate: 0.0098010000
Iteration 250000: total loss 0.0381, losses: [0.018037091940641403, 0.020067980512976646], learning rate: 0.0098010000
Val loss  tensor(1.0760)
Iteration 170000: total loss 1.1380, losses: [0.5681642889976501, 0.5698110461235046], learning rate: 0.0098010000
Val loss  tensor(0.9677)
Val loss  tensor(1.2003)
Val loss  tensor(0.9448)
Iteration 175000: total loss 1.1385, losses: [0.5701544880867004, 0.5683621764183044], learning rate: 0.0098010000
Iteration 255000: total loss 0.0351, losses: [0.016965430229902267, 0.018115337938070297], learning rate: 0.0098010000
Iteration 180000: total loss 1.1308, losses: [0.5657290816307068, 0.5650279521942139], learning rate: 0.0098010000
Val loss  tensor(1.0846)
Val loss  tensor(1.0617)
Iteration 260000: total loss 0.0646, losses: [0.03130824491381645, 0.033291179686784744], learning rate: 0.0098010000
Val loss  tensor(1.1843)
Val loss  tensor(1.2694)
Iteration 185000: total loss 1.1359, losses: [0.5685121417045593, 0.5673850774765015], learning rate: 0.0098010000
Iteration 190000: total loss 1.1506, losses: [0.5726113319396973, 0.5779581069946289], learning rate: 0.0098010000
Val loss  tensor(1.0822)
Val loss  tensor(1.0593)
Iteration 265000: total loss 0.0404, losses: [0.019782623276114464, 0.02058189921081066], learning rate: 0.0097029900
Iteration 195000: total loss 1.1343, losses: [0.5667771100997925, 0.5675663352012634], learning rate: 0.0098010000
Iteration 270000: total loss 0.0341, losses: [0.016693612560629845, 0.017400166019797325], learning rate: 0.0097029900
Val loss  tensor(1.0576)
Val loss  tensor(1.2164)
Iteration 200000: total loss 1.1314, losses: [0.5661869049072266, 0.5651922225952148], learning rate: 0.0098010000
Val loss  tensor(1.0441)
Val loss  tensor(1.0212)
Iteration 275000: total loss 0.0315, losses: [0.015092131681740284, 0.01642596535384655], learning rate: 0.0097029900
Iteration 205000: total loss 1.1326, losses: [0.5659652352333069, 0.5666160583496094], learning rate: 0.0098010000
Iteration 210000: total loss 1.1402, losses: [0.5706931352615356, 0.5694576501846313], learning rate: 0.0098010000
Val loss  tensor(0.9854)
Val loss  tensor(0.9625)
Iteration 280000: total loss 0.0318, losses: [0.015348863787949085, 0.016457555815577507], learning rate: 0.0097029900
Val loss  tensor(1.1370)
Val loss  tensor(1.2164)
Iteration 215000: total loss 1.1317, losses: [0.5656875967979431, 0.5659687519073486], learning rate: 0.0098010000
Iteration 285000: total loss 0.0352, losses: [0.01709086075425148, 0.018152721226215363], learning rate: 0.0097029900
Iteration 220000: total loss 1.1315, losses: [0.5637602210044861, 0.5677509307861328], learning rate: 0.0098010000
Val loss  tensor(1.0245)
Val loss  tensor(1.0016)
Iteration 225000: total loss 1.1310, losses: [0.5656059384346008, 0.5653795599937439], learning rate: 0.0097029900
Iteration 290000: total loss 0.0518, losses: [0.025365455076098442, 0.026451772078871727], learning rate: 0.0097029900
Val loss  tensor(1.1257)
Val loss  tensor(1.2039)
Iteration 230000: total loss 1.1397, losses: [0.5700080394744873, 0.5697011351585388], learning rate: 0.0097029900
Val loss  tensor(0.9976)
Val loss  tensor(0.9747)
Iteration 295000: total loss 0.0471, losses: [0.022949079051613808, 0.02411888912320137], learning rate: 0.0097029900
Iteration 235000: total loss 1.1326, losses: [0.564945638179779, 0.5676841139793396], learning rate: 0.0097029900
Iteration 300000: total loss 0.0329, losses: [0.01627344824373722, 0.01662411354482174], learning rate: 0.0097029900
Val loss  tensor(1.1425)
Iteration 240000: total loss 1.1329, losses: [0.5652639865875244, 0.5675904154777527], learning rate: 0.0097029900
Val loss  tensor(0.9912)
Val loss  tensor(1.2119)
Val loss  tensor(0.9683)
Iteration 245000: total loss 1.1368, losses: [0.5670987963676453, 0.5696842074394226], learning rate: 0.0097029900
Iteration 305000: total loss 0.0530, losses: [0.026081645861268044, 0.026909995824098587], learning rate: 0.0097029900
Iteration 250000: total loss 1.1339, losses: [0.5662686228752136, 0.567672073841095], learning rate: 0.0097029900
Val loss  tensor(1.0214)
Val loss  tensor(0.9985)
Iteration 310000: total loss 0.0388, losses: [0.01909291185438633, 0.019670337438583374], learning rate: 0.0097029900
Val loss  tensor(1.1311)
Val loss  tensor(1.2185)
Iteration 255000: total loss 1.1280, losses: [0.5637916922569275, 0.5642339587211609], learning rate: 0.0097029900
Iteration 260000: total loss 1.1313, losses: [0.5657873749732971, 0.5655544400215149], learning rate: 0.0097029900
Val loss  tensor(1.0625)
Val loss  tensor(1.0396)
Iteration 315000: total loss 0.0331, losses: [0.01620800979435444, 0.016865884885191917], learning rate: 0.0097029900
Iteration 265000: total loss 1.1311, losses: [0.5650144815444946, 0.5660931468009949], learning rate: 0.0097029900
Iteration 320000: total loss 0.0752, losses: [0.036930035799741745, 0.03824431449174881], learning rate: 0.0097029900
Val loss  tensor(1.0323)
Val loss  tensor(1.2063)
Iteration 270000: total loss 1.1312, losses: [0.5652490258216858, 0.5659286379814148], learning rate: 0.0097029900
Val loss  tensor(1.0307)
Val loss  tensor(1.0078)
Iteration 325000: total loss 0.0455, losses: [0.02166038192808628, 0.023881180211901665], learning rate: 0.0097029900
Iteration 275000: total loss 1.1369, losses: [0.5686725974082947, 0.5682734847068787], learning rate: 0.0096059601
Iteration 280000: total loss 1.1325, losses: [0.5658662915229797, 0.5666128993034363], learning rate: 0.0096059601
Val loss  tensor(1.1020)
Val loss  tensor(1.0791)
Iteration 330000: total loss 0.0543, losses: [0.026513967663049698, 0.027803463861346245], learning rate: 0.0097029900
Val loss  tensor(1.1447)
Val loss  tensor(1.2082)
Iteration 285000: total loss 1.1325, losses: [0.5651307106018066, 0.5673486590385437], learning rate: 0.0096059601
Iteration 335000: total loss 0.0441, losses: [0.02097642980515957, 0.023121604695916176], learning rate: 0.0097029900
Iteration 290000: total loss 1.1316, losses: [0.5642004013061523, 0.5674416422843933], learning rate: 0.0096059601
Val loss  tensor(1.0015)
Val loss  tensor(0.9786)
Iteration 340000: total loss 0.0403, losses: [0.019664494320750237, 0.020595921203494072], learning rate: 0.0097029900
Val loss  tensor(1.0837)
Iteration 295000: total loss 1.1386, losses: [0.5683126449584961, 0.5702599883079529], learning rate: 0.0096059601
Val loss  tensor(1.2169)
Iteration 300000: total loss 1.1305, losses: [0.5618042349815369, 0.5686859488487244], learning rate: 0.0096059601
Val loss  tensor(0.9973)
Val loss  tensor(0.9744)
Iteration 345000: total loss 0.0651, losses: [0.032203178852796555, 0.032906267791986465], learning rate: 0.0097029900
Iteration 305000: total loss 1.1481, losses: [0.5746399164199829, 0.5734947323799133], learning rate: 0.0096059601
Iteration 350000: total loss 0.0500, losses: [0.024387245997786522, 0.025615988299250603], learning rate: 0.0097029900
Val loss  tensor(1.1065)
Val loss  tensor(1.2038)
Iteration 310000: total loss 1.1322, losses: [0.5633261203765869, 0.5688937306404114], learning rate: 0.0096059601
Val loss  tensor(1.0724)
Val loss  tensor(1.0495)
Iteration 315000: total loss 1.1343, losses: [0.5654522776603699, 0.5688961744308472], learning rate: 0.0096059601
Iteration 355000: total loss 0.0494, losses: [0.023742664605379105, 0.025655997917056084], learning rate: 0.0097029900
Iteration 320000: total loss 1.1322, losses: [0.5665435791015625, 0.5656251907348633], learning rate: 0.0096059601
Val loss  tensor(1.0477)
Val loss  tensor(1.0248)
Iteration 360000: total loss 0.0332, losses: [0.015944665297865868, 0.01721946708858013], learning rate: 0.0097029900
Val loss  tensor(1.0921)
Val loss  tensor(1.2250)
Iteration 325000: total loss 1.1298, losses: [0.5640428066253662, 0.5657913684844971], learning rate: 0.0096059601
Iteration 365000: total loss 0.0521, losses: [0.02563895285129547, 0.026454782113432884], learning rate: 0.0097029900
Iteration 330000: total loss 1.1271, losses: [0.5642531514167786, 0.562894344329834], learning rate: 0.0096059601
Val loss  tensor(1.0456)
Val loss  tensor(1.0227)
Iteration 335000: total loss 1.1309, losses: [0.5654885172843933, 0.5653961300849915], learning rate: 0.0095099005
Iteration 370000: total loss 0.0618, losses: [0.031078368425369263, 0.030769122764468193], learning rate: 0.0097029900
Val loss  tensor(1.2094)
Val loss  tensor(1.3501)
Iteration 340000: total loss 1.1339, losses: [0.5644542574882507, 0.5694102644920349], learning rate: 0.0095099005
Val loss  tensor(1.0016)
Val loss  tensor(0.9787)
Iteration 375000: total loss 0.0338, losses: [0.016071924939751625, 0.017714906483888626], learning rate: 0.0096059601
Iteration 345000: total loss 1.1342, losses: [0.5676409006118774, 0.5665803551673889], learning rate: 0.0095099005
Iteration 350000: total loss 1.1274, losses: [0.564551830291748, 0.5628037452697754], learning rate: 0.0095099005
Val loss  tensor(1.0253)
Iteration 380000: total loss 0.0688, losses: [0.03342344984412193, 0.03540072590112686], learning rate: 0.0096059601
Val loss  tensor(1.0601)
Val loss  tensor(1.0024)
Val loss  tensor(1.2132)
Iteration 355000: total loss 1.1289, losses: [0.5645095705986023, 0.5643755197525024], learning rate: 0.0095099005
Iteration 385000: total loss 0.0371, losses: [0.0179020743817091, 0.019199304282665253], learning rate: 0.0096059601
Iteration 360000: total loss 1.1326, losses: [0.5660213828086853, 0.5665444731712341], learning rate: 0.0095099005
Val loss  tensor(1.0830)
Val loss  tensor(1.0601)
Iteration 390000: total loss 0.0356, losses: [0.016402458772063255, 0.019243592396378517], learning rate: 0.0096059601
Val loss  tensor(1.0932)
Val loss  tensor(1.2227)
Iteration 365000: total loss 1.1341, losses: [0.5660490989685059, 0.5680402517318726], learning rate: 0.0095099005
Iteration 370000: total loss 1.1349, losses: [0.5675284266471863, 0.5673723220825195], learning rate: 0.0095099005
Val loss  tensor(1.0733)
Val loss  tensor(1.0504)
Iteration 395000: total loss 0.0349, losses: [0.016585329547524452, 0.018278373405337334], learning rate: 0.0096059601
Iteration 375000: total loss 1.1300, losses: [0.564941942691803, 0.5650133490562439], learning rate: 0.0095099005
Iteration 400000: total loss 0.0591, losses: [0.028173858299851418, 0.03088686428964138], learning rate: 0.0096059601
Val loss  tensor(1.1856)
Val loss  tensor(1.3361)
Iteration 380000: total loss 1.1355, losses: [0.5686820149421692, 0.5668536424636841], learning rate: 0.0095099005
Val loss  tensor(1.0689)
Val loss  tensor(1.0460)
Iteration 405000: total loss 0.0292, losses: [0.013880888000130653, 0.015341763384640217], learning rate: 0.0096059601
Iteration 385000: total loss 1.1341, losses: [0.5632300972938538, 0.5708652138710022], learning rate: 0.0094148015
Iteration 390000: total loss 1.1321, losses: [0.5668846368789673, 0.5651751160621643], learning rate: 0.0094148015
Val loss  tensor(1.0308)
Val loss  tensor(1.0078)
Iteration 410000: total loss 0.0300, losses: [0.014270642772316933, 0.01567959226667881], learning rate: 0.0096059601
Val loss  tensor(1.1134)
Val loss  tensor(1.2203)
Iteration 395000: total loss 1.1284, losses: [0.5642094016075134, 0.5641511082649231], learning rate: 0.0094148015
Iteration 415000: total loss 0.0309, losses: [0.014733118005096912, 0.016143156215548515], learning rate: 0.0096059601
Iteration 400000: total loss 1.1422, losses: [0.5715357065200806, 0.570675790309906], learning rate: 0.0094148015
Val loss  tensor(1.0300)
Val loss  tensor(1.0071)
Iteration 405000: total loss 1.1427, losses: [0.5720283389091492, 0.5706784129142761], learning rate: 0.0094148015
Iteration 420000: total loss 0.0449, losses: [0.02199023962020874, 0.022861365228891373], learning rate: 0.0096059601
Val loss  tensor(1.1031)
Val loss  tensor(1.2191)
Iteration 410000: total loss 1.1307, losses: [0.5651397705078125, 0.5655367970466614], learning rate: 0.0094148015
Val loss  tensor(1.0682)
Val loss  tensor(1.0452)
Iteration 425000: total loss 0.0435, losses: [0.02101915143430233, 0.022459130734205246], learning rate: 0.0096059601
Iteration 415000: total loss 1.1332, losses: [0.5673583149909973, 0.5658886432647705], learning rate: 0.0094148015
Iteration 430000: total loss 0.0293, losses: [0.014101321808993816, 0.015235400758683681], learning rate: 0.0096059601
Val loss  tensor(1.0974)
Val loss  tensor(1.2180)
Iteration 420000: total loss 1.1321, losses: [0.5640614628791809, 0.5680748224258423], learning rate: 0.0094148015
Val loss  tensor(1.0107)
Val loss  tensor(0.9878)
Iteration 425000: total loss 1.1376, losses: [0.5675370097160339, 0.5700823068618774], learning rate: 0.0094148015
Iteration 435000: total loss 0.0313, losses: [0.015077317133545876, 0.016237353906035423], learning rate: 0.0095099005
Iteration 430000: total loss 1.1359, losses: [0.5675501227378845, 0.5683670043945312], learning rate: 0.0094148015
Val loss  tensor(1.0602)
Val loss  tensor(1.0373)
Iteration 440000: total loss 0.0441, losses: [0.021726125851273537, 0.022386984899640083], learning rate: 0.0095099005
Val loss  tensor(1.1044)
Val loss  tensor(1.2104)
Iteration 435000: total loss 1.1351, losses: [0.566956639289856, 0.5681334733963013], learning rate: 0.0094148015
Iteration 440000: total loss 1.1311, losses: [0.5652414560317993, 0.5658273696899414], learning rate: 0.0094148015
Val loss  tensor(1.0325)
Iteration 445000: total loss 0.0347, losses: [0.016420206055045128, 0.018292203545570374], learning rate: 0.0095099005
Val loss  tensor(1.0096)
Iteration 445000: total loss 1.1342, losses: [0.5653667449951172, 0.5688128471374512], learning rate: 0.0093206535
Iteration 450000: total loss 0.0456, losses: [0.02267669513821602, 0.022934328764677048], learning rate: 0.0095099005
Val loss  tensor(1.0603)
Val loss  tensor(1.1971)
Iteration 450000: total loss 1.1334, losses: [0.5653374195098877, 0.5680439472198486], learning rate: 0.0093206535
Val loss  tensor(1.0204)
Val loss  tensor(0.9975)
Iteration 455000: total loss 0.0421, losses: [0.020383687689900398, 0.021719802170991898], learning rate: 0.0095099005
Iteration 455000: total loss 1.1378, losses: [0.5693154335021973, 0.5684845447540283], learning rate: 0.0093206535
Iteration 460000: total loss 1.1318, losses: [0.5650212168693542, 0.5667402744293213], learning rate: 0.0093206535
Val loss  tensor(1.0302)
Iteration 460000: total loss 0.0419, losses: [0.020059600472450256, 0.021805444732308388], learning rate: 0.0095099005
Val loss  tensor(1.1958)
Val loss  tensor(1.0073)
Val loss  tensor(1.2441)
Iteration 465000: total loss 1.1356, losses: [0.5663197636604309, 0.5693264007568359], learning rate: 0.0093206535
Iteration 465000: total loss 0.0366, losses: [0.017749330028891563, 0.018888801336288452], learning rate: 0.0095099005
Iteration 470000: total loss 1.1341, losses: [0.5682703852653503, 0.565805196762085], learning rate: 0.0093206535
Val loss  tensor(0.9773)
Val loss  tensor(0.9544)
Iteration 470000: total loss 0.0330, losses: [0.015905506908893585, 0.017101213335990906], learning rate: 0.0095099005
Val loss  tensor(1.1502)
Val loss  tensor(1.2230)
Iteration 475000: total loss 1.1403, losses: [0.5684903860092163, 0.5717770457267761], learning rate: 0.0093206535
Iteration 480000: total loss 1.1337, losses: [0.5673452615737915, 0.5663204789161682], learning rate: 0.0093206535
Val loss  tensor(1.0617)
Val loss  tensor(1.0388)
Iteration 475000: total loss 0.0374, losses: [0.0180713701993227, 0.019325656816363335], learning rate: 0.0095099005
Iteration 485000: total loss 1.1341, losses: [0.565496027469635, 0.568570613861084], learning rate: 0.0093206535
Iteration 480000: total loss 0.0392, losses: [0.018875276669859886, 0.02030300535261631], learning rate: 0.0095099005
Val loss  tensor(1.1758)
Val loss  tensor(1.2185)
Iteration 490000: total loss 1.1421, losses: [0.5707880854606628, 0.5713164210319519], learning rate: 0.0093206535
Val loss  tensor(0.9675)
Val loss  tensor(0.9446)
Iteration 495000: total loss 1.1327, losses: [0.5660638213157654, 0.5666554570198059], learning rate: 0.0092274469
Iteration 485000: total loss 0.0564, losses: [0.02765061892569065, 0.02877402864396572], learning rate: 0.0094148015
Iteration 490000: total loss 0.0364, losses: [0.018094053491950035, 0.018338141962885857], learning rate: 0.0094148015
Val loss  tensor(1.1361)
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
Val loss  tensor(1.2221)
Iteration 495000: total loss 0.0422, losses: [0.020512361079454422, 0.02166646532714367], learning rate: 0.0094148015
FullyConnectedNetworkMod(
  (layers): ModuleList(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=128, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=128, out_features=2, bias=True)
  )
)
